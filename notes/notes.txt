Initial Installs  {{{
  NOTE: These notes assume you're installing a Debian based system. If you're rocking and rolling
  with something like Arch, you'll need to obviously use commands that apply to pacman, etal.

  Note on dotfile managers {{{
    UPDATE 20200519: I just read about something called `yadm` (yet another dotfiles manager). If
    you ever want to try something besides stow, you may want to consider giving this a shot. Sounds
    like a modified git setup made specifically for dotfiles and bringing a system up to speed from
    ground zero in a quick and painless way.
    UPDATE 20200823: Another you can check out is called chezmoi

  }}}

    Why Stow? tidbits {{{
      UPDATE 1: Something you've started looking into recently is using symlinks to better manage
      how your dotfiles are organized

      UPDATE 2: Went with Stow! Cool little app :) Dotfiles updated accordingly as well as using
      some sort of dotfiles symlink tool that allows you to essentially do a one-click/one-command
      full install of your dotfiles. So far, my research has led me to the following sites:

        http://blog.smalleycreative.com/tutorials/using-git-and-github-to-manage-your-dotfiles/
        https://dotfiles.github.io/
        https://github.com/anishathalye/dotbot
        https://codereview.stackexchange.com/questions/181855/script-to-create-symlinks-for-dotfiles-in-a-git-repository
        https://github.com/webpro/awesome-dotfiles
        https://github.com/cowboy/dotfiles

        It also turned me on to some cool tools that may come in handy later down the line:
          $ sudo apt install stow
          https://neomutt.org/about
          https://github.com/newsboat/newsboat
          https://github.com/nojhan/liquidprompt
          https://github.com/uzbl/uzbl/tree/master

      UPDATE 3: Now, before I started using Stow, I was setting up bare git repos and managing the
      dotfiles directly without any sort of middleware. Oddly enough, whenever I went back to find
      those setup notes, they didn't turn up anywhere... So, in the event you want to go back to
      a git only approach that doesn't rely on symlinks or anything, I'm going to go ahead and add
      those notes here. Now, I'm wanting to say there was a reason I moved away from the git only
      approach... If I recall correctly, I may have ran into some sort of issue where things were
      getting corrupted or that I ran into too many situations where the dotfiles themselves would
      need to have a bunch of conflicts merged... Either way, here's the lowdown

        # See Git Troubleshooting section for further comments on why you felt the need to
        # transition to something like Stow.
        Possible your git-foo has improved enough to where this will no longer be an issue?

        https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/

        $ git init --bare $HOME/.dotfiles
        $ alias dotfiles='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'
        $ dotfiles config --local status.showUntrackedFiles no
        echo "alias dotfiles='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'">> $HOME/.bashrc

        Then, now that you have the bare repo set up and ready to go, along with a corresponding
        alias, you can interact with this dotfiles directory/repo in this manner:
          $ dotfiles status
          $ dotfiles add .vimrc
          $ dotfiles commit -m "Add vimrc"
          $ dotfiles add .bashrc
          $ dotfiles commit -m "Add bashrc"
          $ dotfiles push

        #### Installing the above dotfiles onto a new system ####

        $ alias dotfiles='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'
        $ echo ".dotfiles" >> .gitignore
        $ git clone --bare <git-repo-url> $HOME/.dotfiles
        $ alias dotfiles='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'
        $ dotfiles checkout
          NOTE: If this fails with a message like, "Error, the following untracked working files
          would be overwritten by checkout: ..." that means those files already exist on your
          system. Simply backup the named files (ie rename to
          X_backup or something), then re-run the above command
        $ dotfiles config --local status.showUntrackedFiles no

        This process is saved at durdn's dotfile repo here: https://bitbucket.org/durdn/cfg.git He's
        taken this whole thing one step further and made a script out of his customized version of
        the above setup process. This way all he has to do is download the script from his github
        repo, run the script, and viola! He's good as new, rip roaring and ready to go.

    }}}

  Something else to keep in mind is that, if you're setting this up on a laptop or something without
  a dedicated graphics card, there's a good chance you'll also need to follow the nomodeset notes
  you have below for the bootloader. It helps with video card drivers.

  # Install these before you pull your dotfiles repo
    $ sudo apt install git curl stow

  Pull down dotfiles repo...
    What you're after now is a series of steps to do the following:
      1) Setup your machine to have access to your github repo. This generally comes in two steps

          1.1) set up bitbucket to recognize your machine by setting up an ssh key.
            Repo access on fresh hard drive {{{
              So when I got a new hard drive up and running, I had some problems getting git to
              allow me access to the repos. I ran the standard ssh key-gen commands, and I could
              verify they were created on the system AND uploaded to the bitbucket user SSH keys
              area... So what gives? Let's walk through the whole process...

                $ mkdir ~/.ssh
                $ cd ~/.ssh
                $ ssh-keygen
                  NOTE: Want to change your previously setup password for ssh?
                  $ ssh-keygen -p -f ~/.ssh/<id_rsa> # ie 'laptop' or 'virtual'

              Then, you're prompted to enter a filename, this saves as ~/.ssh/<filename>
              NOTE: In the past, you've generally used 'laptop' for your laptop and 'virtual' for
              your virtual

                $ cat ~/.ssh/<filename>.pub
                  this outputs the public key contents to the terminal. You then copy the output, go
                  to Bitbucket, User panel, SSH keys, Add key, give it a name, then paste that huge
                  string into the keybox, then save.

              With that done, you should be good to go for pulling down from git!

                $ cd
                $ ssh-add ~/.ssh/<filename>
                $ git clone git@bitbucket.org:BlakeLeBlanc/dotfiles.git

              If this doesn't work, see this section {{{

                $ ssh -T git@bitbucket.org # verifies your config and username

              Now, it turns out in this case, while the system had a ssh key generated, it was not
              yet formally associated with any particular id. To check, use:

                  $ ssh-add -l

              If a key is not listed, then you need to associate your system with the desired key
              with:

                  $ ssh-add ~/.ssh/<filename_used_above>

              If it complains that it "could not open a connection to your authentication agent" use
              this to "activate" ssh:

                $ eval $(ssh-agent)

              If it still complains "Permission denied" it may be because you had to run some of the
              initial ssh setup commands with sudo. Which it fine, it just means you need to grant
              access to the user you're currently logged in as. Assuming you're rocking and rolling
              under 'linux', it looks something like this:

                $ sudo chmod 700 ~/.ssh; chmod 600 ~/.ssh/*

                Once you've got that up and ready, you should then be able to run
                  $ ssh-add ~/.ssh/<filename> and be on your merry way

              https://unix.stackexchange.com/questions/90853/how-can-i-run-ssh-add-automatically-without-password-prompt
              Now, a bit more background on ssh-agent and how this works... Even though it can be
              annoying to have to enter your passphrase each and every time you need to use
              ssh-agent, it is strongly recommended that a passphrase be used. The next question,
              then, is how often do you feel you should have to do this? You can use various
              utilities such as seahorse, keychain, or ssh-ident. What both of these utilities do is
              manage your ssh login sessions. The only drawback, of course, is that what you trade
              for added convenience, you lose in security. So at the end of the day, it all comes
              down to how secure you want your logins to be. With something like keychain or
              ssh-ident, it's not uncommon for them to store the login info in memory. There are
              even settings that all it to be a one-and-done setup in that you simply enter the
              passphrase once and it will persist between reboots or user sessions.

              The middle ground seems to be to use one of two approaches. Currently using number 2,
              added to ~/.bashrc
                1) a utility yet set it up in such a way that it "forgets' the login between
                   sessions/reboots, this way you have to enter the passphrase each and every time
                   you start your computer.
                2) Set something up manually in your ~/.bashrc file

              }}}

            }}}

          1.2) Now that your machine is recognized, you need to setup git locally so that it too can
            get access to the repo. For that, take a gander at your notes on git's "initial setup"

        Once those things are squared away, and you've got the dotfiles repo cloned, it's time to
        progress further!

      2a) cd into ~/dotfiles
        $ stow bash

        If it complains of existing files, run the following
          $ rm -rf ~/.bash_profile ~/.bash_aliases ~/.bashrc

        $ exec $SHELL
          If that doesn't work, you can also try `$ exec bash` or the more full-blown `$ reset`

      2b) Make sure source code repositories are enabled

          $ enablesourcerepos # alias

            $ sudo sed -i '/deb-src/s/^# //' /etc/apt/sources.list
            $ sudo apt update

      3) $ installall
        NOTE: This is an alias. If you'd rather go through it manually, see below

          If you'd rather run through each one separately, though, here are their notes: {{{
            3a) $ installbase # alias containing the following, plus some extras
                  $ sudo apt install vim
                  $ sudo apt install tmux
                  $ pip3 install tmuxp
                  $ sudo apt install taskwarrior
                    NOTE: In pacman, this is called "task"
                  $ sudo apt install timewarrior
                    NOTE: In pacman, this is called "timew"
                  $ sudo apt install tasksh
            3b) $ installstow # alias for the following
              cd into ~/dotfiles
              $ stow git reference taskwarrior tmux tmuxp vim vimwiki rvm

              NOTE: Some of the files you'll be pulling in overwrite default program files. If this
              happens, simply delete the default file from your system then rerun the applicable
              stow command
            3c) $ installvim # alias
              For vim, there are some extra steps you need to go through before you launch it for
              the first time to prepare it to play nicely with the plugins.
            3d) $ installdwm # alias}}}

      4) Now that flash is no more, this step may not be necessary...
        $ sudo apt install adobe-flashplugin # If that doesn't work, then search package manager
         for "flashplugin"
         NOTE: In pacman, this looks to be called "flashplugin"

         NOTE: Other things I've read say that if neither of those work, you need to look for
         something called a Popper Flash Player, Pepper, PPAPI Flash, or maybe even Pipelight. This
         all stems from the fact that Adobe doesn't really support Linux as of 2014-ish so the
         community has made several work arounds. The other possibility is this may be something
         confined to the Vivaldi browser as Firefox seems to handle it okay.

    Next up, if you're looking to do some Ruby development on the system, you'll need to jump
    through a few more hoops. See the sections for the following:

      5)  $ setuplandapp
        NOTE: This is an alias, if you'd rather run through things manually, see below

        Postgres {{{
            NOTE: You need to setup PG first so its plumbing is in place for some of the
            Ruby gems you're using in land_app
          RVM
          nodejs (nvm, npm, pnpm, etc)
            UPDATE: This is now included as part of the installbase alias
          $ installwebp
            NOTE: You need to install webp ahead of libvips so libvips can "see" that it
            needs to be ready for webp
          $ gitinstalllibvips
          If you're going to be working with land_app, the str_metrics gem requires rust:
            $ installrust

          If you're going to be tinkering with land_app, there are some additional things you'll
          need to bring to the party
          NOTE: These next two relate to a capybara-webkit gem that you use in your land_app gem
          bundle

            $ sudo apt install qt5-default
            $ sudo apt --no-install-recommends install libqt*5-dev qt*5-dev qml-module-qtquick-* qt*5-doc-html

          Also, be sure you ran the abovereferenced gitinstalllibvips alias
          And you'll also want to 'stow' the logrotate file for a landapp *.log file cronjob with

            $ sudo stow logrotate -t /

        }}}

      5b) You also need to actually go through the process of setting up the postgres servers too
         |postgresql|

      6) If you'd like to be able to "share" your books directory onto the VM check out the
         |vbox-guest-additions| section

      7) If you're rocking a virtual machine, there's a good chance you'll need to take care of
         the |vbox-guest-additions| stuff so the resolution is displayed properly. Those
         notes are tucked away in here, just search, you'll find'em :)

      8) Now, onto the extras...

          Get the ly launch screen up and at'em
            `gitinstallly1` `gitinstallly2`

          Apply the dwm patches
            pertag
            cfacts
              # NOTE: Per recent attempt, some of the patches are not compatible with 6.3+, will
              # likely need to go in and add manually to the config.h

              $ sudo git apply <patch_file.diff>

            Then you need to re "make" the dwm stuff

              $ sudo make clean install

          (What else?)

      8) At this point, there may be some loose ends that need some touch-ups. And the only way
         you'll know that is to go ahead and try firing up some things. For example, when you first
         launch vim it should go through some auto setup stuff from your vimrc.

         BUT! There may be a chance the `stow vim` function wasn't able to run properly during the
         `installall` thing from earlier. If that's the case, go into ~/.dotfiles and run the `stow
         vim` command again. If it complains about the vim related dotfiles already existing in
         $HOME, then go in and delete the ones it complains about so the path is clear, then
         re-attempt the `stow vim` command.

         There may also be some plugin related follow-up that you need to do for, say, ddc and deno
         if you're still using those plugins.

         Little things like that. But it's all stuff that's pretty self explanatory, things you've
         had no problem in navigating and correcting in the past.

          UPDATE: When it came to getting everything set up on Bodhi 6.0, you ran into an issue
          wherein the go-to vim version would resort to the apt installed 8.1 rather than your newly
          compiled 8.2xx version. The "answer" to this was to completely wipe out the apt installed
          packages so that only the compiled vim version remained

            $ sudo apt --purge autoremove "vim*"

  And from here, you're pretty much done with your initial installs! Congrats :)

  }}}

Books and other resources, notes {{{
  NOTE: At some point, I'd really like to consolidate all of this with my book_notes file on google
  drive. Have one giant compendium that interweaves into itself. Vimwiki might be a good platform to
  accomplish this, though I don't know how readily some of the Excel stuff will transfer over.
  Especially in those instances where I use graphics or drawings or special Excel formatting tricks
  to get an idea across...

  https://devdocs.io
    Holy Batman! This is a giant EVERYTHING resource! Wow!

  https://roadmap.sh

  Coding challenges {{{
    https://www.codewars.com
    https://www.exercism.io
      Very interesting toolset, lets you 'download' the full work environment with a CLI interface.
      NOTE: Download them using `sudo`
    https://www.fullstack.cafe
    https://www.hackerrank.com
    https://www.leetcode.com
    https://www.pramp.com
    https://github.com/jimweirich/gilded_rose_kata
    https://github.com/emilybache/Racing-Car-Katas

  }}}

  https://www.codewithjason.com/refactorings-should-be-atomic/
  https://www.codewithjason.com/atomic-commits-testing/
  https://www.codewithjason.com/favorite-debugging-techniques/

  https://jsonplaceholder.typicode.com
    A 'free' RESTful API you can use to pretend like you have a full server up and running in the
    background. So you can practice get/post requests and test that they will go complete properly
    without having to set up your own server backend. Very cool!

  If you're using something like Codepen and want to be able to use your own images, rather than
  paying for the Pro upgrade, just use a site to convert your image files to base64 and include the
  raw code directly

    https://tobase64.dev

  Design related, for games, etc
    https://dribbble.com
    https://interfaceingame.com

  Caret notation
    https://en.wikipedia.org/wiki/Caret_notation

    When the ^ key is referenced in computer related stuff, that often refers to the <CTRL> key.
      ^T refers to holding down <CTRL> + T

    What's interesting, though, is it's customary for the proceeding character after the caret to be
    written in caps. But, this DOES NOT mean the capital version of that letter should be used. In
    the above example, the user is not required to holdown CTRL + Shift + t, but only CTRL-t

  Misc {{{
    Smart practice {{{
      Face the music
        Go directly to work on the difficult passages and work them over and over and over. Don't
        shy away from the hard parts by "resting in" playing the parts that you have down pat as
        a confidence booster. You must face the dragon. Use a metronome and slowly build up
        proficiency. Making a mistake? Go slower. Focus on technical mastery, play it well.

      Talk it out
        Talking engages a different part of your brain. Saying what you'd like to do in clear and
        precise language can help your brain focus and "figure out a way" to make it happen.

    }}}

    Connected to a WIFI but can't get the browser confirmation page to come up? {{{
      Try going to one of the default router addresses, like

        192.168.1.1
        127.1.1.1
        http://localhost

    }}}

    Gmail account aliases {{{
      put a `+` after the tail end of your email, and you can create multiple accounts that route to
      your one primary email

      So for example, I wanted to make another github account that uses a different username. But
      since github binds the account to the email address, whenever I'd put in my gmail, it would
      say, "Hey, that email is in use. Sorry, can't do that!"

      HOWEVER! Using this "trick", I could make an new account with

        <gmail-account>+<alias>@gmail.com

    }}}

  }}}

  Althoff, C Self-Taught Programmer {{{
    NOTE: This book is written with Python in mind, though many of the coding principles apply to
    virtually any object-oriented language

    Programming Paradigms {{{
      Imperative Programming {{{
        When you explicitly tell a program to "do this, then that". A sequence of steps moving
        toward a solution, with each step along the way changing the program's state. This last part
        about state changes is KEY! x = 2 y = 4 xy = x + y

      }}}

      Functional Programming {{{
        Writing functions that -- given the same input -- always return the same output. In
        functional programming, you only program with functions.

        "Functional code is characterized by one thing: the absence of side effects. It does not
        rely on data outside the current function [aside from any data that is taken in through
        parameters that get passed into the functions]and it does not change data that exists
        outside the current function."
          - Mary Rose Cook

        Here are a couple examples showcasing the difference between imperative and functional
        programming (in Python):

          Imperative a = 0 def increment() global a a += 1

          Functional
            def increment(a)
              return a + 1

        The first function is "unfunctional" because it relies on data outside of itself, and
        changes data outside of the current function by incrementing a global variable. The second
        function is functional because it does not rely on any data outside of itself, and it does
        not change any data outside of itself either. Functional programmers write functions this
        way to eliminate side effects-- the unintended consequences that happen when you are
        constantly changing the state of your program.

      }}}

    }}}

    How variables work

    Hexadecimals
      Base 16 number system
      Uses 0-9, then A-F for the remaining
      Each "place value" goes from 0-15.
      Prefaced with 0x by some programming languages (C, for example)

  }}}

  Bowler, J How to Study {{{
    NOTE: Not bad, though I gott say, a lot of things seems to be a regurgitation of things like
    Newport's Deep Work or Atomic Habits, etc.

    https://www.youtube.com/watch?v=EHJP-XOdJBE
    https://www.youtube.com/watch?v=tDkutigfg3Q

    SAAD {{{
      S paced repitition
          Revisit what you're learning, lengthening the time interval little by little.

          This is kind of where flash cards and things like Anke come into play
      A ctive recall
          Asking tough questions, challenging yourself to figure it out yourself rather than
          flipping back to what you've read, etc. Trying to find answers OUTSIDE of the literal
          material you're studying. Familiarity is not the same as recall.

          Blurting, write prompt words. Then scribble out or draw or whatever EVERYTHING you can
          think of on that topic/etc.
      A ssociations
          Linking new things you're learning with things you already known/understand well

          Also similar to the Sherlock's Mind Palace, of "walking through" a familiar
          place/scene/etc that you know stone cold and associating things to the objects, etc.
      D esirable difficulty
          Make sure you're challening yourself the right amount so you remain engaged

    }}}

    Time-tabling {{{
      Plan through your material. One of the insights you're trying to gain here is also a growing
      familiarity with how long certain topics/etc take you to run through or digest.

    }}}

    Interleaving {{{
      Interspersing a few different things into a "learning session" or whatever you want to call it
      can be effective. Rather than slogging it out on ONE THING for days and days on end. Giving
      yourselves a "break" to dive into another topic gives your mind some space to grind through
      the other things you're studying in your background/subconscious and create new
      associations/insights/etc.

    }}}

    Sacred spaces {{{
      Having a consistent place/area for certain activities. Can also be variations on a theme
      within one space (ie change lighting, change colors, change soundscape, etc)

      Closely linked with a sense of ritual, priming your mind

    }}}

    Ditch the aesthetics {{{
      Don't confuse fluffing and sprucing things up with actually getting things done.

    }}}

    Habit and routine {{{
      Do it. Stick to it. Over and over and over and over. Consistency is KEY

      Chris advised tackling the most undesirable thing first, get it out of the way. Often our
      minds put up false walls/barriers of resistance. Whereas once you jump right into it, you may
      come to realize it wasn't so bad after all and, wellwhaddayaknowwouldyalookatthat? I'm
      already halfway done!

      It makes moving on to your next items much more enjoyable because you're coming off of a "high
      note" so to say.

    }}}

    Fully accept that your best is good enough to get started {{{
      Try not get bogged down or stymied by perfectionism. Because you're often good enough to get
      started with where you're at right now. Reminds me of the adage, "You'll never know more about
      <xyz> than what you know now. So best get to it!" And that sort of active "sliding scale" is
      so true.

      WHAT DO YOU WANT TO ACCOMPLISH? Great! Now get after it!

      Because you'll always have to start with what you've got ("Free haircuts tomorrow" on a sign
      that always stays up on the wall), don't let that become a hidden crutch/excuse to
      procracstinate.

      Do your best and be confident in that effort. Sure, "your best" may not ultimately come out to
      being your absolute best of the best in an objective "if I could do it again, I would..." type
      of sense. It means that GIVEN THE CIRCUMSTANCES and what you were facing at the moment, within
      that context, you gave it your all.

      THAT is what is meant by being comfortable in doing your best.

    }}}

    Learn from failure {{{
      Take it, move on. Grow, learn. Keep it up.

    }}}

    Prepare your mindset {{{
      Be calm, be balanced. Don't let stress/anxiety rule the day. Your mind works best when you're
      in control, not when you're amped up and anxious.

      Visualization can be powerful here. See the ideal, see what you WANT to happen. And as your
      anxiety and stuff comes up, see yourself responding in a different way, how you WANT to
      respond.

    }}}

    Sleep{{{
      REM is how your brain organizes and synthesizes things together. DO NOT neglect it as
      a process to learning and digesting things. It is absolutely essential.

    }}}

    Organize early {{{
      Prepare in advance so you have the time and space to get in the groove. I think of setting my
      clothes out in advance, in allowing ample time to drive and arrive WAY early, etc etc

    }}}

    Guard your conversations {{{
      Avoid stressful topics, etc that get you amped up

    }}}

  }}}

  Fowler, M Refactoring Ruby Edition {{{

    When you find you have to add a feature to a program, and the program's code is not structured
    in a convenient way to add the feature, first refactor the program to make it easy to add the
    feature, then add the feature.

    Steps in refactoring
      1. Create tests. You MUST. HAVE. TESTS. before you can start anything. And these tests must be
         self-checking.

      Decompose long methods into smaller pieces. Pull things out into separate methods.

  }}}

  Gersting, J Mathematical Structures for Computer Science, 5th ed {{{
    Symbology {{{
      NOTE: I've included LaTex syntax where applicable

      Every compound statement is equivalent to a statement using only the connectives of
      conjunction and negation.

      /\ \land # conjunction
        and
        but
        also
        in addition
        moreover

      \/ \lor # disjunction
        inclusive or

      (+) \lxor \loplus
        exclusive or; the result is false when both components are true.

        "At the intersection, you should turn north or south."

      A -> B \rightarrow # implication
        If A, then B
        A implies B
        A, therefore B
        A only if B
        B follows from A
        B is a necessary condition for A
        A is a sufficient condition for B

      A <-> \leftrightarrow # equivalence
        A if and only if B
        A is necessary and sufficient for B

      A' # negation
        not A
        It is false that A...
        It is not true that A...

      A <=> B \Leftrightarrow # tautology; intrinsically true; truth value that is always true
        Commutative
          A \/ B <=> B \/ A
          A /\ B <=> B /\ A

        Associative
          (A \/ B) \/ C <=> A \/ (B \/ C)
          (A /\ B) /\ C <=> A /\ (B /\ C)

        Distributive
          A \/ (B /\ C) <=> (A \/ B) /\ (A \/ C)
          A /\ (B \/ C) <=> (A /\ B) \/ (A /\ C)

        Identity
          A \/ 0 <=> A
          A /\ 1 <=> A

        Complement
          A \/ A' <=> 1
          A /\ A' <=> 0

      A /\ A' # contradiction; intrisically false; false value that is always false

      Well-formed formulas are often reduced to more manageable chunks. So something like
        ((A \/ B) /\ C) -> (B \/ C')
          could be reduced down to its main connective
        P -> Q
          In this case P is equal to the first A B C portion and Q is equal to the B C' portion

      From here, you can go ahead and build out a truth table for each of the above portions

      Here are some examples for how statements are represented in logic notation
        "If prices go up, then housing will be plentiful and expensive; but if housing is not
        expensive, then it will still be plentiful."

        A = prices go up B = housing will be plentiful
        C = housing will be expensive

          [A -> (B /\ C)] /\ (C' -> B)

        "Either going to bed or going swimming is a sufficient condition for changing clothes;
        however, changing clothes does not mean going swimming"
          A = going to bed
          B = going swimming
          C = changing clothes

          [(A \/ B) -> C] /\ (C -> B)'
          [(A \lor B)] \rightarrow C] \land (C \rightarrow B)'

        "Either it will rain or it will snow, but not both."
          A = it will rain
          B = it will snow

          (A \/ B) /\ (A /\ B)'
          (A \lor B) \land (A \land B)'

    }}}

  }}}

  Harris, Andy - How to Think Like a Programmer {{{
    "Coding" is the easy part, thinking through what you're actually trying to accomplish (ie more
    on the algorithm side of things) that's where things start to get a bit difficult. But even
    then, always try to circle back to WHAT IS IT YOU'RE TRYING TO ACCOMPLISH. Then your questions
    become much more focused. "How do I output a line of text in Python?" etc Those are concepts you
    can readily fine the answer to and keep chugging along

    He jokes that there are roughly 6 or 7 or 8 things you need to know when it comes to
    programming. And that's it. Everything else flows through from those concepts.

    "I don't want computers, I want BRAINS! From people who think and explore and create things!"

    In this way, then, he's a big advocate for just "talking your way" through what you want to
    happen, in a string of comments, and then line by line, straight from the top, "converting" that
    stuff to code.

    Variable
      Name - what do we call this thing?
      Type - what type of data does it contain?
      Initial value - what is it's starting value?

    New variable algorithm
      Goes through and answers the above concerns, syntax per language

    Output
      message: text to write to user

    Input
      variable: where the answer from the user will be stored
      message: question being asked of the user

        Now if you think about this from a human conversation/request setting, any time you're
        wanting some input from another person, there are usually some dependencies involved. He
        gives an example of asking someone, "What's the answer?" (crowd is silent, a bit confused)
        Again, repeats, "What's the answer? (pause) Ahh, you see there? The answer to what, right?
        I never actually set up an actual question... The same holds true when we're thinking about
        input..."

        He also uses an example of asking someone to throw him a ball. But before he does that,
        maybe he should think about putting on a glove himself, right?

        The whole point here is that when we're talking about conversations or trading or whatever,
        there are usually some prerequisites, some "ground work" that must be established in
        advance. And it's the same way with programming.

    Input algorithm
      ask the user a message and store the answer in a variable

    Failure is wonderful.
      Debugging is simply part of the path. He's a HUGE proponent of getting AWAY from the computer
      coding stuff and just sticking with thinking through things. It's OKAY to feel like, "I don't
      know..." is your response. That's great!

      Usually all it comes down to is an algorithm problem. Something with syntax or
      whatever. There's usually something you're not quite understanding properly.

      Try to avoid the temptation of trying to solve a problem you don't fully understanding. DO NOT
      go right away to running for a solution. Check your assumptions, sit with it a while. Turn of
      your computer and go to a whiteboard. Like literally. Put down the computer, go back to your
      thoughts and assumptions. Your thinking and reasoning.

    Convert one type to another.
      From a string to an integer, etc etc

    For loop
      sentry - int var that will control the loop
      start - int val of sentry at the beginning
      finish - int val of sentry at the end
      change - int to add to sentry at each pass

    While loop
      initialize sentry then continue as long as cond is true. So it has the same moving parts as
      a for loop, it just implies a few things are present/provided right out of the gates during
      its initialization

  }}}

  Hernandez, M Database Design for Mere Mortals {{{
    The 'relational' phrase in relational database design is not refering to how tables can be
    'related' to one another, but instead, comes from mathematical set theory. The other branch of
    math it is based on is first-order predicate logic.

    Structured Query Language (SQL) is how you retreive and interact with a relational database. The
    three components are
      1) SELECT...FROM statement
      2) WHERE clause
      3) ORDER BY clause

    While the relational model has its roots in mathematics, the object-oriented database model does
    NOT have a similar theoretical foundation. OO design stuff is where things like classes,
    encapsulation, and inheritance came into play

    Data is what gets INPUT into a database, while information is what get OUTPUT from a database.

    Objectives of good database design
      * The database supports both required and ad hoc information retrieval.
      * The tables are constructed properly and efficiently. Each table in the database represents
      * a single object, composed
        of relatively distinct fields, keeps redundant data to an absolute minimum, and is
        identified throughout the database by a field with unique values.
      * Data integrity is imposed at the field, table, and relationship levels. This helps guarantee
      * that the data
        structures and their values will be valid and accurate at all times.
      * Supports business rules relevant to the organization, providing valid and accurate
      * information that is always
        meaningful to the business
      * Lends itself to future growth with a structure that is easy to modify or expand as the
      * information requirements of
        the business grow and change.

    A normal form is a specific set of rules that can be used to test a table structure and to
    ensure that it is sound and free of problems.

    In his design methodology, he pays homage to normal forms, but does not impose them directly.
    Instead, he creates several guidelines that, if consistently and rigorously followed to a T,
    will yield the results and benefits consistent with the application of normal forms. His primary
    reasoning for this is that the underlying vocabulary and theoretical sides of normal forms can
    be very confusing and off-putting to a layperson not already steeped in the jargon.

    Database Design Process
      He unequivocally states that you simply MUST go through the process of going through a series
      of questions and  {{{ refinements as you progress through building the database. As your
      needs and requirements change and grow with the app (ie, "Oh wait, business has been good but
      no we're realizing that we need xyz too!" Then go through each of the question phases to make
      sure that 'new' portion is properly implemented...) He strongly cautions that PARTIALLY going
      through the process is just as bad, if not worse, than not doing it at all because it gives
      you a false sense of comfort.

      On this point, he's also quick to point out that databases are not hard to design, it just
      takes a bit of time to design them properly. "Don't allow yourself to take shortcuts when it
      seems as if the design process is taking too long -- just be patient and stay the course!"

      PHASE 1 - Mission statement and mission objectives
        {{{
        This is all about providing yourself/team some sort of 'guiding light' as to the ultimate
        purpose you're trying to achieve

        The mission statement helps ensure that you develop and appropriate database structure and
        that you collect the data necessary to support the intended purpose of the database. Think
        of this as kind of like the high-level goals, something say a management team would espouse.

          A good mission statement is succinct and to the point. But again, it's purpose is to be
          a general statement, not overly bogged down with details or specifics. It should not
          address specific details or describe any specific tasks. But if those types of things do
          come up while you're discussing things, keep them aside for later, they'll likely prove
          important once you get the later phases when you're ready to drill down closer to home.

            A good rule of thumb is, if after you read the statement you're worried, "Uh oh... Are
            there any tasks we forgot to include?" then chances are it's not general enough. You
            likely got pulled into too specific and detailed of a statement/paragraph.

        The mission objectives, then, are statements that represent the general tasks your users can
        perform against the data. This might be thought of as, once you have the broader 'marching
        orders' from the higher-ups, you talk to the workers to get a feel for how THEY will
        interact/etc with the data, to get a clearer sense for how the wants of the top-brass might
        be practically fulfilled in a general sense.

          A good mission objective simply gives the WHAT without worrying about going into any
          detail on the HOW.

          Here are some examples of good mission objectives:
            "Maintain complete patient address information"
            "Keep track of customer sales"
            "Produce employee phone directories"

          Here is an example of a poorly worded mission objective along with tips on how it could be
          improved: "We need to keep track of the entertainers we represent and the type of
          entertainment they provide, as well as the engagements that we book for them."

            First off, it defines more than one general task.
            Secondly, it contains unnecessary detail. The phrase 'type of entertainment' either
            refers to a distinct characteristic of an entertainer, or represents a new task that
            should be declared as a separate mission obj. Here they are again, with these fixes put
            in place: "Maintain complete entertainer information" "Keep track of all the engagements
            we book"

      }}}

      PHASE 2 - Analysis of current database and workflow
        {{{
        If there is a current database of some sort in place, look at how the resources are
        currently being used to, again, give yourself a feel for how the data flows through the
        business. Why are they pulling report xyz every day? How do they find information? Who is
        this list sent to? What do they do with it once they receive it? Bossman, I see you're
        jotting down xyz after your meeting, why? What do you need that info for? Etc etc These are
        all things you need to become thoroughly familiar with.

        And then from that workflow, you'll start to build an initial list of fields, weeding out
        any calculated fields and stuff like that... Starting to build a top-to-bottom look at the
        lay of the land and what parts you have to work with.

        Some questions to drive toward:
          What types of data does the organization use?
          How does the organization use that data?
          How does the organization manage and mantain that data?

        Something crucial to keep in mind too is this:
          DO NOT adopt the current database structure as the basis for the new database structure.

          Instead, go through the full process and, after you've fully analyzed the current setup
          and everything, start fresh. Because unfortunately, if you were to "start" with the
          current setup and build-up from there, any glitches or oversights will directly carry over
          to the new database too. Better instead to start fresh since, after all, there is a REASON
          they came looking to you in the first place seeking out a new database.

        When it comes to building out a "case file" on the current system, he's a big fan of using
        screenshots. Get a shot of all the different slips of paper, organization cabinets, computer
        program screens, etc etc and jot your notes and impressions down with those files. Later,
        you can use these as a great reference as well as show them to others during follow-up
        interviews to clarify something, etc. He'll often hand a user a screenshot, ask a few
        questions, and then, if the user recommends any changes or something, have them write down
        their thoughts onto sticky notes that are then adhered directly to the image print-out. This
        way everything is kept nice and tidy for later review.

        Most common things you'll likely come across in this time are things like
          reports (paper, email, program generated, etc),
          screen presentations used throughout management, etc and
          web pages

          What you're after in looking at these is to get a handle on WHAT data is being stored,
          what information is being presented, how it is used, things like that...

        When you're talking with people, pay attention to what NOUNS they use. In your notes, he
        recommends double underlining nouns you feel are pertinent.

        Then take all of those underlined words and add them to, what he calls, a List of Subjects.
        You can then draw from this list as a basis for further questions, etc

        What you're after at this point, is to get a List of Characteristics surrounding each of the
        above mentioned subjects. These characteristics will later inform you of the fields needed
        for a given subject's database table, etc Recommends single-underlines for these items.

        Preliminary field list: Once you've gathered all the characteristics, add all of them to
        another list. Then go through all of the items listed and check for duplicates. Then you
        refine them as you come across them. For example, say you see "Name" listed several times.
        If each "Name" is a chracteristic of a separate subject, then consider renaming them
        appropriately (ie Employee Name, Client Name, etc rather than simply "Name").

        Value Lists or Enumerated Lists: These are "accepted ranges" for a particular characterstic,
        usually due to some sort of constraint that enforces a business rule. So for example, say
        you notice that a particular form a user fills out has a place where they select a Vendor.
        And they must select a vendor from a drop down list. That "value list" of a limited
        selection pool is what we're talking about here.

        Also, when you're reviewing your preliminary field list, you must remove every calculated
        field and place it on a separate list. Those will come in handy later.

      }}}

      PHASE 3 - Create the database structures
        {{{
        Define tables and fields, establish keys, define field specifications for every field.

        Each table should
          cover one and only one subject
          not contain any duplicate fields
          have fields that store only a single value (no multipart or multivalued fields)
          have fields that represent distinct characteristics of the table's subject
          have a properly defined primary key that uniquely identifies each record within a table

        After this creation process is done, then you need to establish field specifications for
        each field in the database

        He recommends initially working from the Preliminary Field List, NOT from your Subject List.
        Why? Because it will help give you a more unbiased perspective. And from that field list,
        you're goal is to think of what concepts or structures or whatever relates them together.
        So, for example, say you're looking at a list with things like Course Code Course Name
        Course Description Lab Fee ... These fields seem to suggest something like "Courses".

        Again, your goal here is to kind of reverse engineer a new subject list. To go through all
        of the fields and identify as many subjects as possible.

        The end result of this work will result in something that he calls the Preliminary Table
        List. Once you have that done, compare the Prelim Table List to the Prelim Subj List you
        created earlier, and merge them together into a "new" Prelim Subj List. Resolve duplicates
        When you see subjects and tables that have the same name, don't just take them as being the
        same thing. Try to determine whether they really are the same thing or if they're meant to
        represent different concepts or aspects of the mission/etc. If it turns out you feel they
        are different, then think of new name/s for them to make the distinction a bit clearer. The
        goal here is to cross out any duplicates from the Subj List so that everything gets
        consolidated onto the Prelim Table List

            For example, let's say you get done and see that you have "Equipment" listed on both
            your Subj List and Prelim Table List. Reviewing your interview notes and stuff, you
            start to understand that the Equip you have on your Subj List pertains to things like
            tools, appliances, and av equipment. However, the equip you have on your Prelim Table
            List encompasses additional things such as trucks, vans, and trailers. So you look into
            your notes a bit deeper and realize that vehicle rentals are treated differently from
            "regular" equipment rentals. Therefore, each occurence of equip actually does
            represent a different subject. You clarify this difference by letting equipment refer to
            'basic' stuff and add a new one for "Vehicles". So the Subj List would have both
            "Equipment" and "Rental Agreements" crossed out, and the Prelim Table List would list
            Equipment, Rental Agreements, and Vehicles (see how you "added" the new one to the
            Prelim Subj List?).

          Resolve items that represent the same subject
            Your goal here is to identify those items which have the same name, but after further
            consideration, actually refer to the same concept/etc. So what you're trying to do then,
            is make sure only ONE of those names actually gets put onto the Prel Table List. If you
            prefer the name on the PTL, then cross out the listing on the Subj List. If you prefer
            the name on the Subj List, then cross out the item on the PTL and add the item from the
            Subj List to the PTL

          Combine everything from the Subj List to the Prelim Table List
            This will create a sort of "second version" of the Preliminary Table List. At this
            point, you can get rid of the Subject List, mark it DONE/REVIEWED/whatever, because
            you'll now be working from the PTL

        Now that you have your PTL vers 2.0, it's important that you now look through your Mission
        Objectives list to see if there's anything you still feel may not be full represented by
        a table.

          Review each mission objective one at a time, underlining all of the relevant subjects
          mentioned in the statement.
          Then cross-check that the underlines are present in your new PTL. Ask yourself the same
          questions you did before:

            When you find a "match", do they both represent the same thing?
            If you feel they cover different subjects, then add whatever you feel is needed to the
            PTL.
            Otherwise, cross out the duplicate item in the mission objective.

            If a mission statement subj seems to be synonymouse with something on the PTL, select
            the name that best identifies the concept/subject and use it in the PTL.

            When something underlined in the mission objective represents a new subject, add it to
            the PTL.

            Rinse and repeat until you've made it through all of the mission objectives

        Now, it's time to take this thing to the next level and make up a Final Table List! * dun
        dun duhhh This includes the Name of the table, the Type of table, and a brief description of
        the table.

          See Figure 7.9 page 185 (230 of)

          Name
            most of what you have in your PTL should be okay, but now is the time when you need to
            take a moment to give a long hard look at the names you're using for the tables. There's
            a whole laundry list of guidelines at p 187

          Type
            NOTE: At this early stage of putting together the FTL, since you're working strictly
            from your PTL, essentially everything you list here should be a data table. The other
            table types will come into play later down the line in the process

            data table: primary foundation of the information that the database provides

            linking table: "through" table for many-to-many associations
            subset table: contains fields that are related to a particular data table and further
            describes the data table's subject in a very specific manner

            validation table: contains relatively static data, is a crucial component of data
            integrity

          Description
            Explicitly define the table AND state its importance

            Here are some things to watch out for...
              "Suppliers - the companies that supply us with ingredients and equipment"
              Okay... But what if the company starts receiving ingredients from local farmers? Those
              don't quite count as companies. Knowing that, a better descr would be to generalize
              the origination, so perhaps something like "The people and organizations from which we
              purchase ingredients and equipment"

              You also need to indicate WHY it is important. Something like this only focuses on the
              what: "We need the suppliers table to keep track of the names, addresses, phone
              numbers, and contact names of all of our suppliers." Better would be something like

                "Supplier information is vital to the bakery because it allows us to maintain
                a constant supply of ingredients and ensure that our equipment is always in working
                order."

              Do NOT include implementation-specific information in the table description

        Associate fields with each table
          He strongly recommends sticking to pen and paper or excel or something for this phase.
          Anything that allows you to make a list that does NOT involve creating a database in any
          way shape or form on a computer.

          Redundant data occurs in two ways
            1) Repeated value in a field as a result of a field's participation in relating two
               tables This is OKAY, that's how relational databases work
            2) Repeated value in a field as a result of a field or table anomaly This is NOT OKAY
               and must be resolved

          Duplicate field is a field that appears in two or more tables, for any of the following
          reasons:
            1) It is used to relate a set of tables together OKAY
            2) It indicates multiple occurences of a particular type of value NOT OKAY
            3) There is a perceived need for supplemental information NOT OKAY

          A common mistake is to think that you need to design the table so that it conforms to the
          reports you want/need to generate from it. This type of thinking is wrong, though, as it
          is likely to introduce things like duplicate fields in the form of reference fields.

            The example he uses here is one with two tables, Instruments and Manufacturers. The
            Instruments table contains fields related to individual instruments but also the
            ManufacturerPhone and WebSite. These two fields are already part of the Manufacturer
            table, however, and including them within the Instruments table in this way introduces
            them as reference fields, which is wrong. The fix for this is to simply remove the phone
            and website fields from the Instruments table altogether.

          Subset tables
            A table that contains a large number of blank values in its fields usually, but not
            always, represents more than one subject.

            A subset table represents a subordiante subject of a particular data table. The subset
            field contains fields that are germane dto the subordinate subject it represetnts, and
            it also includes a field (or fields) from the data table that serves to relate the data
            table to the subset table.

            A subset table does NOT contain fields that represent characteristics common to both it
            and the data table; these fields must remain in their respective data table.

            Once you've identified any needed subset tables, add them to the Final Table List with
            the Type as "Subset"

            As an example, he was going through the list of fields for the Products Table and he
            started to feel like Products was a bit too broad... Because there are certain offerings
            provided that have their own fields unique from the more generic products on offer. This
            lead him to create a Services table. Services are a bit different in that they have
            a type, materials charge, service charge, and service date. They are similar in that
            Products and Services both have a name, description, and category.

          Table-Level Integrity
            * There are no duplicate records in a table
            * The primary key exclusively identifies each record in a table
            * Every primary key value is unique
            * Primary key values are not null

        Field Specifications
          A field spec incorporates various elements that define every attribute of a field. All of
          the elements within the spec are categorized as general elements, physical elements, or
          logical elements

          General
            field name, parent table, label (think of this as any sort of shorthand representation
            of the field for how it might be abbreviated on forms or reports), spec type (unique,
            generic, replica), source spec, shared by (this comes into play whenever you have subset
            tables, you'd list out here the other tables that use this field as a reference),
            alias(es), description

            Now an interesting point worth noting on descriptions... It's easy to give dry and
            literal interpretations, but he advises it's much more beneficial to give it a greater
            sense of context. For example, rather than simply saying that CustName is "the last name
            of a customer", consider giving it a better scope with something like, "The surname of
            a customer, whether original or by marriage, that we use in all formal communications
            and correspondence with that customer."

          Physical
            data type, length, decimal places, character support (letters, numbers, keyboard,
            special), input mask, display format

          Logical
            key type, key structure (simple, composite), uniqueness, null support, values entered by
            (user, system), required value, default value, range of values, edit rule, comparisons
            allowed, operations allowed

            Thoughts on null... Null should never be used to represent a mere "blank". Null instead
            represents something very distinct, it represents a "missing" or "unknown" value. "Users
            commonly make the mistake of using a blank to represent a meaningful value, such as
            'none', 'not applicable', 'no response', and 'not wanted'. If these values for
            a particular field, then make sure you include them in the range of values element for
            the field. Above all, use nulls judiciously and DO NOT use blanks!"

            Operations allowed is one of those things that usually comes into play a bit later in
            the process, usually during or after the business rules are being established. The
            purpose of this is to clarify and give context to how certain fields are meant to be
            used. If it's a number, is this something you can or are meant to + - X % with itself,
            with other fields, or with other value expressions? If it's a text value, can or are you
            meant to concatenate? Things like this is what's at play here...

      }}}

      PHASE 4 - Establish table relationships
        {{{
        More interviews and feedback, then once you have your answers, you start implementing the
        logical connections between the tables with a primary key or linking table, etc

        He recommends making a matrix of all the tables you've identified and go through them one at
        a time, adding in the respective association

        Different types of associations

          Direct: just what it sounds like, STUDENTS table has a direct relationship to the CLASSES
          table since one or more students can attend a class

          Indirect: when a table is related to another table, but generally through another table.
          So for example, the CLASSES table has an indirect relationship to the STAFF table via the
          FACULTY table; it is a faculty member that teaches a class, not a staff member.

          Ownership-oriented questions include words or phrases such as own, has, is part of, and
          contain. For example, "Can a single order contain one or more products?"

          Action-oriented questions incorporate action verbs such as make, visit, place, teach, and
          attend. "Does a single flight instructor teach one or more types of classes?"

        Defining a deletion rule for each relationship

      }}}

      PHASE 5 - Define business rules
      {{{
        Identify limitations on various aspects of the database, establish business rules, and
        define and implement validation tables as needed. These could be uncovering certain
        limitations like the fact that a user working with order processing knows that a ship date
        must occur later than an order date, that a customer always needs a daytime phone number,
        things like that. Management, on the other hand, may not be privvy to the nitty gritty
        details of the day to day interactions, but will likely have its own set of nuances related
        to higher level aspects of the business.

        From these types of understandings, you'll then start to impose the various requirements and
        restrictions on the fields as to what can be entered and when, etc etc

      }}}

      PHASE 6 - Define views
      {{{
        This is when you start looking at the various ways in which the data will need to be pulled
        out of the database and displayed to its users. Think of views as being "virtual" tables in
        that the information doesn't necessarily exist in the database as it is shown in the report,
        but rather the report pulls from various tables and fields to cobble together a unique
        perspective on the information. A name from here, a product from there, etc etc

      }}}

      PHASE 7 - Review data integrity
      {{{
        Now that you've gone through all the above steps, you should have a pretty well tidied set
        of tables and fields and from all your ongoing interviews and feedback, they should be
        looking and flowing pretty well together! But now's the time to do one last pass to review
        what's been created.

        First, review each table to make sure it meets the criteria of a properly designed table and
        check the fields within each table for proper structure. Then check for table-level
        integrity.

        Second, check field specifications of each field. Then check for field-level integrity.

        Third, review validity of each relationship, confirm its type and the participation
        characteristics for each table within the relationship. Then review for relationship
        integrity.

        Lastly, review the business rules you identified earlier in the design process and confirm
        the constraints you've placed on various aspects of the database.

      }}}

      After all this, pen a paper, drawing, conceptual-ish stuff is done, it's time to start
      implementing everything with some sort of RDBMS program.

    }}}

    Interviews
    {{{
      Try to use open-ended questions as often as you can. Give the person/group room to speak
      openly without feeling pigeonholed in any way.

      Make your intentions known at the outset. What you want to discuss, why they were invited, etc
      etc. Be sure they KNOW KNOW KNOW that your discussions together are in no way a veiled
      performance review or anything weird like that. It is purely for invaluable feedback from
      those who will be using the system day in and day out. Openess and 100% honesty through and
      through, no shame or awkwardness, they can shine a light on anything and everything they feel
      is pertinent.

      Be genuinely sincere (you're already good at this).

      On subsequent interviews, display that you've actually done something with the info they gave
      you prior. Otherwise you run the risk of them feeling like the first interview was unheard and
      a waste of time.

      Also like you've done in the past, make sure you let everyone know that the buck stops with
      you. You are the official authority and arbitrator over any disputes that may arise. Stress
      that you've been entrusted with this authority and that you view yourself as a true
      third-party who only wants to identify the problems, flesh out disagreements, and arrive at
      the best solutions. As for inter-office politics, you don't have a dog in the fight. You're
      here to do a job.

      10 person limit for each interview. Any more than that and people feel outnumbered and drowned
      out. Keep it small, keep it personal. If you need to have a larger group, consider breaking
      people out into "clumps", each with its own designated leader. This way you can have say
      5 groups of 6 people each, but only be interacting directly with 5 people which is more
      than managable.

      Consider conducting separate interviews for users/workers and management. At least at the
      start. Then, if you feel like everyone meshes well, maybe you can have some mixed meetings so
      they can interact with one another during the discussion. IN MY MIND, I'd rather keep it
      separate. Part of what I'm being paid to do is be an intelligent filter of sorts conveying
      information in private to each. If I ever do go the mixed route though, I'd say keep it light
      and organized to where EVERYONE follows the same "toss the ball" rules or whatever so the
      atmosphere feels collegial and as much on a level playing field as possible.

      Prepare questions in advance.

      Take notes during or ask for permission to record. Or have someone come with you whose sole
      job is to take notes. I like large whiteboards and stuff for this so big points are easily
      seen by all, etc etc

      Give equal and undivided attention. Listen listen listen listen

      Keep the pace moving along, which means you need to maintain control of the flow of the
      session

    }}}

  }}}

  Lippman, S C++ Primer (5th ed, 2012) {{{


  }}}

  Metz, S 99 Bottles of OOP {{{
    Flocking, she borrows this concept from Fowler who wrote Refactoring. There's a Ruby version of
    that book too.
      1. Select things that are most alike
      2. Find smallest difference between them and focus your attention there
      3. Make the simplest change that will remove the difference, that will make them more and more
         identical, piece by piece
      4. Advised that you change one line at a time and re-run tests after each and every change.
         This is a crucial element to this approach. Sure, once you have a lot more experience under
         your belt, you can take some liberties, but be very careful for not getting too confident.
         You want to have a regular and consistent feedback loop. Taking care of seemingly "easy"
         fixes will add up in expected ways!

    Try to guard against only looking for the "hard" problems. Often improvement comes by a thousand
    small, boring changes. "It is common to find that hard problems are hard only because the easy
    ones have not yet been solved."

    DO NOT DISCOUNT THE VALUE OF SOLVING EASY PROBLEMS!

    Refactoring is a process that should be comfortingly predictable as far as thought process and
    implementation are concerned. Learning the art of transforming code one line at a time, while
    keeping the tests passing at every point, lets you undertake enormous refactorings piecemeal.

    Name methods after what they mean (the thought, the concept), name classes after what they are
    (physical reality). Abstract them one level above themselves. So if your object is a pewter
    hand-crafted bowl, you might choose to abstract it out to, say, "serving_dish". But abstracting
    it out to "food_receptacle" or "thing" is TOO abstract and too far removed from any sort of
    useful specificity.

    Excessive use of conditionals is a big red flag. In general, conditionals are one of those
    necessary evils that you should attempt to replace in your code as much as possible when we're
    talking about developing an OOP mindset.

    When refactoring... Re-run tests after. Every. Single. Change. Don't become lazy and wait for
    a "bundle" of changes to be in place before you re-test. Why? Because your first and foremost
    goal while refactoring is to keep the code functioning in such a way so that if your efforts
    were halted in some way midstream, the code will still work. If you make nine changes before
    re-testing and if your computer crashed on the start of the eighth change, could you say you
    KNOW the code works? No. But if you work in a piecemeal fashion, you can!

    Steps to remove parameters
      1. Alter the method definition to change the argument name, and provide a default.
      2. Change every sender of the message to remove the parameter.
      3. Delete the argument from the method definition

  }}}

  Object Oriented Programming and Patterns  {{{
    Objects DO things. The methods you call on them make them DO something.

    Methods/functions are intented to add funtionality to your program, to add actions and abilities.

    SOLID {{{
      https://rubygarage.org/blog/solid-principles-of-ood
      https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design

      Now, here's something interesting about Ruby... A lot of the books you've read so far deal
      with languages that have explicit interface-related structures. Ruby, however, does not have
      this. In Ruby, the closest it gets to some form of an interface is in how it structures its
      initialize blocks, where you give it an object and then set it to the '@' variable type. This
      was a HUGE aha moment for me and explains why I've been so confused by many of the books I've
      read thusfar on SOLID patterns. Big thanks to Jim Weirich for his talk here @ 27:45:

        https://www.youtube.com/watch?v=dKRbsE061u4

    }}}

    From Martin's Clean Code: "Objects expose behavior and hide data… Data structures expose data
    and have no significant behavior.... In any given system we will sometimes want the flexibility
    to add new data types, and so we prefer objects for that part of the system. Other times we will
    want the flexibility to add new behaviors, and so in that part of the system we prefer data
    types and procedures.

    From Olsen DPiR:
      "The total object orientation of Ruby has some implications for variables. Because everything
      in Ruby is an object, it is not really correct to say that the expression x = 44 assigns the
      value 44 to the variable x. Instead, what is really happening is that x receives a reference
      to an object that happens to represent the number after 43."

      On .each loops, `break` stops the loop when the condition is met, while `next` essentially
      "skips" the current position and advances to the next item in the array. So something like
      this would out put 'Next!' when it got to gar this would out put 'Next!' when it got to
      gary... names = ['george', 'mike', 'gary', 'diana'] names.each do |name| if name == 'gary'
      puts('Next!') next end puts(name) end

      When it comes to the use of quotes for strings, spreading things out on multiple lines can get
      a little awkward. To help with this, Ruby offers %Q{} for double quotes and %q{} for single
      quotes. Note here too that there are processing implications, single quotes receive minimal
      processing, though they also are not open for interpolation and things like that.

      Also important to keep in mind too that because of Ruby's OO foundation, strings are mutable
      and subject to change through recursive assignments. A = B, C = B, means if A is changed,
      C will change too since the reference pointer "trickles down". To offer some sort of immutable
      object, this is where symbols come into play with Ruby, they serve more or less as "immutable
      strings".

  }}}

  Note taking methods and tools {{{
    Zettelkesten
      https://en.wikipedia.org/wiki/Zettelkasten
      https://zettelkasten.de
      https://zettelkasten.de/posts/overview
      https://medium.com/@ethomasv/understanding-zettelkasten-d0ca5bb1f80e
      https://medium.com/@rebeccawilliams9941/the-zettelkasten-method-examples-to-help-you-get-started-8f8a44fa9ae6
      https://zenkit.com/en/blog/a-beginners-guide-to-the-zettelkasten-method/
      https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125?gi=df7e43f18cd1
        NOTE: Saved as PDF

    dendron
      https://dendron.so
        For VSCode

    dokuwiki
      https://www.dokuwiki.org/dokuwiki

    joplin
      open-source, but from what I can tell creating inter-note links is a bit cumbersome. It CAN be
      done though. So if you're interested in going this route, perhaps there are some plugins or
      something that make it a bit more user friendly?

    logseq
      https://logseq.com

    tiddlywiki
      https://github.com/Jermolene/TiddlyWiki5

  }}}

}}}

Animation, Drawing, Sound, Video {{{
  Drawing and Animation {{{
    NOTE: A lot of people seem to like opentoons. Been going on for a while and is VERY
    full-featured compared to some of the others. For example, Krita has only recently started
    supporting animations.

    k-3d
      http://www.k-3d.org/

    krita
      https://krita.org/en/

    opentoonz
      https://opentoonz.github.io/e/

    pencil2D
      https://www.pencil2d.org/

    synfig
      https://www.synfig.org/

    SVG related {{{
      $ sudo snap install inkscape

      $ sudo snap install gravit-designer

      $ sudo snap install vectr

    }}}

  }}}

  Video {{{
    Kdenlive
      https://kdenlive.org/en/

  }}}

}}}

Bootable USB file installers {{{
  NOTE: Ventoy is what I've used so far, worked really well!

  etcher
    https://github.com/balena-io/etcher

  unetbootin

  ventoy
    https://github.com/ventoy/Ventoy

  xboot

}}}

Bootstrap  {{{
  https://getbootstrap.com/docs/4.1/getting-started/introduction/

  NOTE: Just a heads up right out of the gates. Given that the official documentation offered by
  Bootstrap is friggin phenomenal, I don't see much use simply re-inventing the wheel here. So these
  notes will most likely remain pretty sparse -- if I don't end up abandoning these notes
  altogether!

  Container {{{
    The outer container element acts as a wrapper of sorts for the entire page. It basically adds
    a bit of padding to the edges by having a default width of 1200 pixels. This default width can
    be changed if you prefer.

    An alternative to this is to instead use
      container-fluid
    This will have it fill the entire width rather than add in the additional padding.
    Another difference is that while the standard container uses specific break-points for changing
    layouts, container-fluid incorporates a smooth transition without any breakpoints

  }}}

  Rows {{{
    The page can be seen as being a series of rows stacked on top of one another, each with its own
    elements and stuff packed within it.
    Every row contains 12 columns
    A row must have some sort of height to it (either through its content or in its styling) in
    order to be "seen".
    If you place a row inside a row, then that new row will itself contain its own 12 columns. So
    they have a sort of nesting
    quality to them

  }}}

  Columns {{{
    This is generally where the dynamic sizing/responsive web design stuff comes into play.
    Basically, there are different size presets that you can work with, each corresponding to
    a certain screen size. And for each desired screen-size that you want to customize, you can
    specify how many columns certain content for specific rows (or all rows, if you like) should
    contain.

    One guy recommended that you start you designs with the 'sm' size distinction and cutomize
    further from there as needed. If I understood him correctly, 'sm' can serve as a kind of
    catch-all in that unless you need to deal with the 'xs' size, then anything set for 'sm' will
    carry over to the larger sizes unless you decide to further customize those sizes later on (ie
    add a col-lg-## distinction). UPDATE: Yes. Basically, how it works is that the smallest size
    will also apply to all larger sizes unless or until those larger sizes themselves are further
    customized.

    The different sizes included in default bootstrap are:
      xs
      sm
      md
      lg
      xl

      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            ...
          </div>
        </div>
        <div class="row">
          <div class="col-sm-12">
            ...
          </div>
        </div>
      </div>

  }}}

  Offsets {{{
    These essentially serve as "empty" columns to the left that scoot the content over.
    So col-sm-offset-2 means that on a small screen or larger (if no parent sizes are included), the
    first two columns will have nothing in them, meaning that any content therein will effectively
    begin at column three.

  }}}

  Interesting bootstrap css-style classes to consider {{{
    "page-header"
    "lead"
    "text-left"
         -center
         -right
         -justify
         -nowrap

    <mark> # highlights
    <del> # cross-out

  }}}

}}}

Compiler resources {{{
  https://godbolt.org

}}}

Canvas {{{
  NOTE: These are a smattering of notes and thoughts after watching some videos. Eventually, these
  will likely need to be better organized, but for right now, I'm just putting them out there to act
  as stepping stones for what I'm wanting to do in with kimpy.

  https://youtu.be/o9sgjuh-CBM

  Canvas objects always rotate around the point of origin. By default, the point of origin is the
  top left corner. The point of origin is moved with the translate function.

  The general sequence of action is to FIRST Rotate, THEN draw.

  Save and Restore
    Think of these as ways of preserving a certain layout/setup and then returning to them later on.
    So let's say you made some adjustments and you want to go back to how things were previously.
    One way, would be to manually walk things back in reverse. So if you translated 300, 500, to go
    back to the initial location, you could move -300, -500. But as you can probably guess, after
    you've done a lot of adjustment, walking back through the motions may be quite cumbersome and
    confusing! And that's where save and restore come into play.

    So in the above example, if you first saved the canvas, then did your translate 300, 500, you
    can go back to the prior save state by simply using restore. And both of these commands act as
    a stack. So you can save save save save, say 4 different settings/configs on top of each other.
    And then each time you restore, you're going back one layer, pulling the most recent one off of
    the stack. Very similar to how push and pop work with arrays.

  When it comes to rotating things, you can also rotate the canvas element itself after things have
  been drawn:

    https://stackoverflow.com/questions/8517879/how-to-rotate-the-existing-content-of-html5-canvas

  Another way is to do it within the canvas's code itself, similar to what you've outlined before.
  Except to save some time, you can copy/clone the existing canvas to a new canvas, rotate the new
  canvas, then 're-draw' the old canvas at the rotated angle. This can save from having to literally
  redraw things again from scratch:

    https://stackoverflow.com/questions/30240768/rotate-existing-image-on-canvas


}}}

Chat clients (tele, disc, etc){{{
  NOTE: Okay so heads up muchacho! Evidently some users have been banned by using third-party
  Discord or Telegram clients. For whatever reason, it seems the devs frown upon this. So proceed
  with caution!

  UPDATE: Holy cow! Mattermost with Matterbridge or Matterhorn seems to be the mother of all
  clients! Implements just about everything!

    https://github.com/mattermost/
    https://hackage.haskell.org/package/matterhorn-50200.7.0
    https://github.com/42wim/matterbridge

      $ wget curl https://https://github.com/42wim/matterbridge/releases/download/v1.25.1/matterbridge-1.25.1-linux-64bit

  Bitlbee {{{
    https://gitub.com/bitlbee

  }}}

  Discord cli {{{
    https://github.com/ayntgl/discordo

    https://github.com/EionRobb/purple-discord
    https://github.com/fourjr/discord-cli
    https://github.com/sm00th/bitlbee-discord

  }}}

  IRC {{{
    https://irchighway.net/help/i-m-new-to-irc
      Great little write-up about what it is, how to set it up, etc

    libera.chat
      Up and coming alternative to freenode

    irssi {{{
      https://github.com/irssi/irssi

      Install
        $ sudo apt install irssi

      /network
      /help network

      /quit

      Add libera chat
        https://gist.github.com/xkr47/8c820ca500a1febdc24d44bb56e5f9d5
          $ /network add -nick <name> libera
          $ /server add -network libera -auto -ssl irc.libera.chat 6697
          $ /connect libera
          $ /query NickServ
          register <password> <email>
          <copypaste line from email>
          $ /network add -autosendcmd "" -sasl_username <user> -sasl_password <pass> -sasl_mechanism PLAIN libera
          $ /channel add -auto <channel> libera
          $ /join <channel>
          $ /save

    }}}

    mirc {{{
      I've mostly seen this one used for windows OS's

    }}}

    weechat {{{
      https://weechat.org/
      https://weechat.org/doc
      https://weechat.org/scripts/

      https://github.com/weechat/weechat

      $ sudo apt install weechat-curses weechat-plugins weechat-python

      /help
      /server add <server_name> <irc_address> -ssl
      /connect <server_name>
      /join #<name>

    }}}

  }}}

  Matrix {{{
    https://github.com/8go/matrix-commander
      $ pip3 install matrix-nio
      $ sudo apt install libolm-dev
      $ pip3 install matrix-commander

    https://matrix.org/docs/projects/client/weechat-matrix

  }}}

  Telegram {{{
    NOTE: Evidently Telegram has started tightening its API use and user tokens. To the point where
    I'm seeing people say their Telegram account got suspended or deactivated after trying to use one
    of these command line friendly versions. So proceed at your own risk, muchacho! Is it really worth
    it?

    https://github.com/ars3niy/tdlib-purple
    https://github.com/zevlg/telega.el

    telegram-cli {{{
      https://github.com/kenorb-contrib/tg
      $ sudo snap install telegram-cli

      https://github.com/vysheng/tg/wiki/Telegram-CLI-Commands

      $ telegram-cli -W -e "<command>"
      $ telegram-cli -h

      > help
      > contact_list
      > dialog_list
      > history <name>

    }}}

    tg {{{
      https://github.com/paul-nameless/tg

      Install {{{
        With pip3
          $ pip3 install tg

        From source
          $ cd /usr/share
          $ sudo git clone https://github.com/paul-nameless/tg.git
          $ cd tg
          $ pip3 install python-telegram
          $ pip install .
            NOTE: I tried using `pip3 install .`, but ran into issues. I had to use `pip`
          $ tg

            NOTE: It also complained about my not having the a python 3.8 or greater. So I installed
            3.8 then ran the `install .` command again

              $ sudo apt install python3.8

      }}}

    }}}

    telegramtui {{{
      DEPRECATED
      $ pip3 install telegramtui

    }}}

  }}}

}}}

Databases, Design, and Concepts  {{{
  Graph
    neo4j
    auradb
    cypher

    This is a very interesting way of modeling data, not as disparate tables linked by keys, but as
    a series of nodes and connections. This allows for VERY fast traversal and is VERY strong as
    a means for uncovering relationships, etc between data.

  UI Tools  {{{
    Kexi
      http://www.kexi-project.org

    OmniDB
      https://github.com/OmniDB/OmniDB

    pgAdmin
      https://www.pgadmin.org
      NOTE: See your setup notes in postgresql for how to get pgadmin up and running

    SQL Workbench
      http://www.sql-workbench.eu

  }}}

  Data Modelling  {{{
    Niels Henrik Juul Hansen  {{{
      https://youtu.be/tR_rOjPiEXc

      Data is 'raw' numbers and letters, the actual content that is saved itself.
      Information is data that is given a proper context. So now the number "123" is more than just
      those numbers, now we know it represents, say, a street address.

      Data Normalization, this comes in several different "levels", ie "normal forms", generally
      from 1 to 6. Helps make things consistent, searchable, and increases the quality and
      trustworthiness of the data. You can pretty much pick and choose which and how many levels you
      want to implement into the database.

        First Normal = One of Each Don't "mix" different types or sets of data within a single
        column. If you take an address, for example, if you throw the whole thing into one column
        "123 Street Drive City, State 123456", you'll see that there are more than one 'thing'
        present in the column. It might be better then, to separate each of those 'pieces' out into
        separate columns, like Street, City, State, Zip

        Second Normal = Remove redundancy/repetition If you see the same words over and over again
        in a single column, consider breaking those out into a seperate table/entity and then
        reference the values directly. So, say you have a giant table that lists the state
        "Oklahoma" over and over and over again in a single column, across multiple rows. All of
        that duplication is just begging for someone to mistype the word (ie Okalhoma). To prevent
        this, move all of the states into their own table and then reference each state from there.
        So, lets say in the States table, Oklahoma has an id number of 3. That 3 value would then be
        used in the main example table rather than the literal word "Oklahoma".

          When it comes to second normal, there is some room for interpretation. Because how much
          redundancy should you work to remove? For example, in his experience, he said he never
          breaks out street address values into separate tables. Sure, you may have the same street
          names used over and over again, but for him, he lets that slide.

        Third Normal = Only columns with direct dependency of the primary key are in the entity What
        this means it that, say you have the City and Zip as part of a table. Well, the City relates
        to the zip code being used. Which means you could have another table, say, zip_city, that
        lists out each of the zip codes along with the city they correspond to. This would then
        allow you to remove the city column altogether, since it will now be "looked up" by the
        included zip code in the zip_city table.

      Relations and Cardinality
        This are the various relationships between tables: one-to-one, one-to-many, many-to-many

      Datatypes
        These are things like INT, DECIMAL, DATETIME, VARCHAR, etc etc

      MySQL Workbench is a great UI enabled environment to create and relate database tables. Very
      cool!

    }}}

    Data Normalization Concepts
      http://www.agiledata.org/essays/dataNormalization.html
      https://beginnersbook.com/2015/05/normalization-in-dbms/
      https://www.phlonx.com/resources/nf3/
      https://tech.readersmanch.com/2018/09/30/how-to-describe-normalization/
      https://www.lifewire.com/database-normalization-basics-1019735

      Some say that you should normalize table fields/attributes, NOT necessarily their contents.
      The want to have some sort of value lookup table is a different consideration altogether. This
      seems to conflict with what Hansen said about second normal form... What am I possibly
      misunderstanding here then?

        I think they're both saying the same thing to a certain extent because in your case, if you
        track out WHAT you will be doing with, say, an address/location, you plan to use it in
        several different contexts. So rather than reinvent the wheel each and every time with
        something like States by having a separate State field duplicated across several tables,
        you've pulled it off into its own table so it is isolated to a specific "master" fieldset
        that can then be referenced in a relationship

    }}}

  Indexing {{{
    https://youtu.be/HubezKbFL7E

    It's tempting to see an index as being some sort of 'stripped down' version of the table at
    hand, when in reality, the process of creating an index is much more involved. Still blazingly
    fast, but there is some legwork that it's good to be aware of because there are times when you
    can take it too far and actually come out the other end with worse performance than you did
    before indexing.

    An index contains ONLY those values that were included in the index, along with a unique
    identifier generated by the database (these identifiers are NOT the same as a primary key, think
    of them as an internal address used behind the scenes by the database).

    These values are then typically structured into some form of a binary search pattern, usually
    a b-tree. the "b" in the case stands, not for binary, but for BALANCED. A balanced tree means
    that every "pathway" through the data results in the same depth. So the final "leaves" at the
    end of the tree are all at the same level. This ensures that every path equally fast to
    traverse.

    What this means in practice, is that there may be a bit more work "behind the scenes" than you
    might expect and this is why it takes some time to create and balance an index.

  }}}

}}}

Encryption tools {{{
  cryptomator
    https://github.com/cryptomator/cryptomator

  veracrypt
    https://www.veracrypt.fr/en/Home.html

}}}

file compression and conversion {{{
  https://pdfepub.com

  https://docdownloader.com/
  https://downscribd.com/

}}}

Git {{{
  Useful tools and resources {{{
    First off, if you're wanting a non-traditional git interface, gitui and lazygit are awesome!
    Though I will say, part of me is reluctant to dive into this headlong since there may be a lot
    of value in keeping the CLI interactions of git in working memory and in regular use, etc
      UPDATE: To be honest though, while they're kind of cool to look at and scroll through, I don't
      really see the value in them just yet. Maybe they'll really show their strengths if I'm ever
      managing or working with a bunch of other users?

    lazygit {{{
      NOTE: gitui seems to be even BETTER, but lazy is a LOT more popular

      https://github.com/jesseduffield/lazygit

        $ go install github.com/jesseduffield/lazygit@latest

      https://www.youtube.com/watch?v=CPLdltN7wgE
      https://www.youtube.com/watch?v=uXv4poPOdvM

    }}}

    gitui {{{
      NOTE: Holy moly! It's like lazygit but BETTER! Wow! The only major downside is that it's not
      near as popular as lazygit from a dev standpoint

      https://github.com/Extrawurst/gitui

        $ cargo install gitui

    }}}

    git-delta {{{
      https://github.com/dandavison/delta

      Install {{{
        $ cargo install git-delta

        ~/.gitconfig setup
          Edit git config file to contain something like:
            [core]
                pager = delta

            [interactive]
                diffFilter = delta --color-only --features=interactive

            [delta]
                features = decorations

            [delta "interactive"]
                keep-plus-minus-markers = false

            [delta "decorations"]
                commit-decoration-style = blue ol
                commit-style = raw
                file-style = omit
                hunk-header-decoration-style = blue box
                hunk-header-file-style = red
                hunk-header-line-number-style = "#067a00"
                hunk-header-style = file line-number syntax

      }}}

    }}}

    http://www.ndpsoftware.com/git-cheatsheet.html#loc=remote_repo;
    https://github.com/nvie/git-toolbelt#readme
    https://github.com/techgaun/active-forks
      If you find a project that doesn't seem to be well-maintained, but it has a lot of forks, use
      this site to find the most current fork
    https://ohmygit.org

    https://github.com/github/git-sizer/
    $ sudo apt install git-sizer
    $ git-sizer --verbose

    Now, when it comes to actually reducing the size of a repo, that is a bit more complicated. From
    what I can tell, even if you remove a file/directory from a repo and push the change up, it will
    likely still exist within the repo's history. In order to **really** get rid of the files,
    you've gotta use something like bfg-repo-cleaner:

      https://github.com/rtyley/bfg-repo-cleaner

    This is kind of like a wrapper for git's built-in git-filter-branch stuff, but it is also A LOT
    faster! So if you ever want or need to go down that road, definitely check'er out! The only
    downside I can see is that it is not officially supported or recommended. The git-filter-branch
    method may be more reliable in that way

  }}}

  Structuring a commit message {{{
    Subject line
      Capitalize subject line
      Limit to 50 chars
      No period at end
      Use imperative language: "Update link"
        Should finish the sentence, "If applied, this commit will <blah blah blah>"
    Body
      When more context is needed, use body to explain what and why, not how
      Separate body from subject with blank line
      Wrap body text at 72 chars

  }}}

  Initial setup and pulling down repos {{{
    NOTE: When it comes to pulling down other people's repos, I've found the HTTPS-style syntax
    works best. There may be a way to do it through SSH, but the most common approach I've seen is
    to use HTTPS. You can also add a directory name at the end of it to tell git to clone it INTO
    that directory (git will create the directory in the process)

    $ sudo apt install git
      the progit book recommends using sudo apt install git-all but that installs a ton of stuff
      that seems to be overkill and I haven't seen it mentioned in other tut sources.

    $ git config --global user.name "Your Name"
    $ git config --global user.email you.email@example.com
    $ git config --global push.default matching
    $ git init

    To undo git init, use:

        $ sudo rm -rf .git # This destroys the .git file that was created in this process

    NOTE: Hiding since I prefer SSH-style for the time being and don't want to get them mixed up
    ------ HTTPS-style --------{{{

    $ git remote add <remote name> <https url> # It seems the remote name is commonly used as
    a reference to a user name

      Typically use origin
      So an example is:

        $ git remote add origin https://BlakeLeBlanc@bitbucket.org/BlakeLeBlanc/<REPO NAME>.git

    $ git clone https://BlakeLeBlanc@bitbucket.org/BlakeLeBlanc/<REPO NAME>.git

    }}}

    ------ SSH-style -------
    NOTE: It is recommended you reset all of your SSH keys on an annual basis for security reasons
    and to make sure that there aren't any long gone users still hanging around with access to the
    repo, etc.

    $ git remote add <remote name> <ssh url>

      $ git remote add origin git@bitbucket.org:BlakeLeBlanc/<repo_name>.git

    NOTE: What's interesting here is that if you've already created a remote repository at
    bitbucket or wherever and you're wanting to UPLOAD your local stuff to that repository without
    cloning anything, you need to also use another command

      $ git remote set-url origin git@bitbucket.org:BlakeLeBlanc/<repo_name>.git

    If you run into any weird stuff, you can always check the setup with...

      $ git remote show origin

    ...and it should match up

    After this is done, you SHOULD be able to do your usual `git push -u origin master` stuff,
    BUT! if you let bitbucket generate a boilerplate gitignore file, it may complain that you
    first need to pull from the repo. And then, once you do that, you may face a merge conflict
    along with a notice of unrelated histories. Once you step through those hurdles, however, you
    should be able to then push to the repo.

      $ git clone git@bitbucket.org:BlakeLeBlanc/<REPO NAME>.git

    Now with cloned git repos, some things are different. For example, when you clone a repo that
    has multiple branches, if you run git branch, you'll find only the * master branch available.
    Run `$ git branch -a`, though, and you'll see the other remote branches listed. So how do you
    work with them? Switch to a remote branch like this:

      $ git checkout <remote branch name>

    You DO NOT include -b like you normally would with git checkout -b <branch name>. By leaving out
    the -b, git knows that you intend to enter into a remote branch and not simply make a new one.
    You then proceed to do the same "git add -A" "git commit -m" and "git push --all <remote name>"
    stuff like you're used to...

    NOTE: Something I encounter quite often is a message prompting me to set up tracking for
    a particular branch. Which has always confused me since the named branch often originated from
    the machine in question and that same machine is what sent the initial 'push' out to git. So
    what gives? Here's my understanding so far.. Turns out that in order to prevent this from
    happening, the first push you send out should always be as follows:

      $ git push -u origin <branch_name> (usually `git push -u origin master` for the initial setup)

    This links the branch to the origin tracker from the git go. From there on out, you should be
    safe to use the standard `git push --all` type stuff without issue. Now, in case that doesn't
    work and you still find yourself needing to type out the upstream command, here is an
    equivalent:

      $ git branch -u upstream/<branch_name> # matching local and remote branch names
      $ git branch -u upstream/<remote_branch> <local_branch>

    PS I may have those swapped, need to verify

    NOTE: Something I noticed when setting up my last repo is that after going through the above
    steps, whenever I'd attempt to run the initial push up to bitbucket, I received an error saying
    "src refspec master does not match any." Turns out I first needed to actually commit the files
    to git first! Duh! When I type it out it makes so much sense :)

    NOTE: Something interesting on remote names... From what I can tell, additional remotes are
    typically added to account for separate users. Origin is the default and is used to track
    a single user. SO! The big takeaway is that it's okay to have a single Origin remote named in
    multiple working directories/projects because they are all self-contained and will not conflict
    with one another. This means my initial reaction to add separate remote names relating to each
    project is not necessary :)

}}}

  Commonly used commands    {{{
    git add -A # stages all files
    git add -p # partial, only adds the changes rather than the full file
      NOTE: I've never used this one before, but it sounds like an awfully useful way to handle
      things
    git add -u # stages currently known files, ignores new files.
    git add . # stages new and modified, ignores deleted.
      UPDATE: I read somewhere that the `.` designation also does some interesting things with
      respect to the working file directory. Like it may not grab "all" the changes, just the ones
      within the given directory. For that reason, I've seen a few users recommend always using `-A`
      instead as it's "guaranteed" to find everything.

    git commit
      opens editor to input commit message
    git commit -a
      stage all modified and deleted files, but ignores newly created files that Git does not know
      about
    git commit -m "<message>"
    git commit -am "<message>"
      equivalent to doing an -a, then an -m, but in one fell swoop
    git push --all

    git clean -n
      removes untracked files currently staged for commit, the -n option merely lists the files
    git clean -f
      removes untracked files currently staged for commit, the -f option 'forces' it to remove the
      files
      NOTE: if you want to include directories also, use the -d option with the above clean commands

    git commit --amend
      Takes you to an editor based screen that makes it really intuitive. Just make your desired
      change to the message printed at the top then save'er up with the ole ':wqa' action sauce

      NOTE: If you've already pushed the commit, this is where things get a little weird with this
      command. After you amend the commit message, you're notified that your current branch has
      diverged from origin. So you've gotta `git pull`, which then merges things together, so you've
      gotta follow that up with a `git push`. What's weird about it though is that, while you
      updated the commit message, it doesn't actually change the previous commit. Instead, it seems
      to make a new commit alongside it and then merge both of those into the working/original
      branch. Rather than have ONE commit with an updated message, you end up with the old commit,
      your new commit with commit message, and then the merge commit bringing everything back under
      one roof. How weird...

    git ls-tree -r HEAD --name-only
      NOTE: aliased to gittracked and gittrackedforever
      Displays list of files being tracked by git

    git reflog
      Not quite sure what this'n is for... It returns a detailed look at the various head positions
      throughout the repo history, which from what I can gather, comes in handy when you need to
      revert or reset some commits.
    git reflog --show
    git reflog -g

    git log --grep=<pattern>
      search commit messages for a specific pattern. Will also work with your various log aliases

      $ git log --grep="drawOnto"

    git log --oneline | grep <pattern> # Similar to the above, but relies on a separate 'grep' tool
      $ git log --oneline | grep "drawOnto"

    git shortlog
      groups by user, shows just subject line

    git config --list
      displays the contents of your .gitconfig file
    git config core.autocrlf true
      This turns off the conversion messages about LR to CRLR
      NOTE: I'm not sure why this is in here... It has to do with converting different line endings
      that may be present. From what I read, it's better to leave this set to its default (false)

    git rm -r --cached <file> <file> ...
      removes named files or directories (use `-rf`) from version control
      NOTE: the -r option allows recursion
      NOTE: This should also be accompanied by them being named in the .gitignore file
      Also, if you find that git is still tracking changes to those files/directories, you can use

        $ git update-index --assume-unchanged <file>

    git push --all <remote name>
      This ensures all branches are pushed to the remote repo. Before, when I was simply using git
      push, I realized it would sometimes only push the master branch
    git pull --all
      This pulls all the files from the online repo into the local directory. Very cool!
    git pull --rebase origin master
      rebase will tack-on the commit after all the merge conflicts have been resolved. As you make
      merge changes, you can add each file with:

        git add <file>
        git rebase --continue
        git rebase --abort # Back out and return to how things were at start

    git stash
      Let's say you're rocking and rolling and for some reason, you'd like to revert back to
      a previous commit state WITHOUT losing the progress you've made so far. This is where stash
      comes in. You stash your unstaged changes, re-checkout the current branch which returns things
      to the prior commit, then scope things out. If you'd then like to jump ahead to where you were
      at the stash point, you use "git stash pop"
    git reset
      this should undo the most recent git additions you've made so you can rebuild the commit again
      from scratch
    git checkout <file>
      If there are some files that you've changed but you'd rather revert them back to their prior
      state, use this'n

    If you ever want to go back to how things were just after your last commit, try one of these out
    for size. Note, however, that if you've already pushed the commit online, then you're looking at
    a different situation. Here, you'll need to alter the commit history. Which, if there are other
    users operating on the project, you've gotta be REALLY careful here because edits to a live
    setting can really fubar things for other users down the line. So proceed with caution!!

      UPDATE: As alluded to above, some advocate for NOT using git reset at all because it is too
      destructive. There are better ways of going about things without running the risk of
      potentially losing work

        git add -A

      AVOID `git reset --hard`, see if there is a better way. The '--hard' part is what has the
      potential to get you in trouble. I know you used it before, but there have been too many
      instances where it's caused you a LOT of heartache

        git reset --hard

        git reset --hard <commit_that_you_want_to_return_to>
          same as above, but with specify commit position you want to return to, can jump
          across more than one commit

        git reset --hard HEAD~1
          this moves the head to the -1 position (one back from the active local commit) and
          removes anything after it. I've used this on my own personal projects when I either
          made a mistake I needed to correct, or -- more often -- encountered a ton of merge
          conflicts in a hairy file like ~/.viminfo that I'd rather not have a to slog
          through. So rather than step through the .viminfo edits one by one, I simply took
          one step backward to how things were before the .viminfo file was edited.

        On this note, there are a couple things you can try to recover the lost files, in case
        there were temporary copies saved on the system somewhere. Two popular ways of doing
        this are with

          $ git fsck --no-reflog
          $ git fsck --lost-found

          From there, you can see the contents of the blob or commit with `$ git show <SHA>`
          Or, you could checkout the file altogether to see if it has what you're looking for

      git revert <commit>
        This will essentially roll-back the named commit without totally destroying it from the
        history. If you need to roll back more than the last commit, check out the manual, because
        there's a certain way you string them together

      git checkout <commit> <file>
        This allows you go to "replace" the named file in the active branch with that file in the
        named commit. This came in handy when I accidentally fubar'd my reference file and had
        already commit and pushed the changes. To get back the good version I used

        $ git checkout HEAD~1 reference/.reference

    git checkout <commit>
      Allows you to check out a specific commit
    git checkout <commit> -- <file1> <file2> ...
      Checkouts out just the named files from a specific commit
      NOTE: These commands put the git head in a detached state. In order to reattached the head to
      your most recent commit, simply re-checkout the desired branch and you'll be good to go

    git checkout <branch_name> -- <file_path> <file_path> <...>
      allows you to pull single files from other branches, usually with the end-goal of merging them
      into the currently active branch

    NOTE: The use of the `--` syntax lets git know that everything after the dashes corresponds to
    file names. It will work without the dashes, but many users prefer to use them since it clears
    up any possible ambiguity.

    git branch -a
      view all branches, both local and remote

    git branch -m <new_branch_name>
      renames current branch
    git branch -m <old_name> <new_name>
      renames another branch that you're not currently on

    git show-branch
      this is a pretty interesting view... I'm not sure how to interpret it though. Kind of seems
      like a different layout to your `git lg` alias

    Search commit messages {{{
      $ git lg --grep="<word of phrase>"

    }}}

    Deleting {{{
      git branch -d <branch_name>
        deletes branch locally, checks merged status
      git branch -D <branch_name>
        deletes branch locally, does NOT check merged status
      git push <remote_name> --delete <branch_name>
        deletes branch on named remote. Typically the remote name is 'origin'
        $ git push origin --delete temp
      git fetch --prune
        This clears out all of the origin/remotes so it matches what's online. For instance, I wiped
        out a bunch of branches from bitbucket.org and ran the git branch -D <branchname> on all of
        the unneeded ones, but for some reason they kept showing back up whenever I'd run git
        branch. I think this is because those branch references were also being saved locally to the
        machine too... Now, why they weren't taken away when I ran the -D stuff, I don't know. But
        either way, doing fetch --prune did the trick
      git branch -vv
        this compares the local branches with those on remote. For those branches you have locally
        that are not or no longer on remote, you'll see that they're listed as "gone". In my case,
        I wanted a way to quickly match up the local branches to the remote branches without having
        to manually go through each and every one with `$ git branch --D <branch_name>`. To
        accomplish this, I included the following git alias, thanks to the good people on
        StackOverflow :)

          `$ git match`

          NOTE: If this command returns something like "fatal: branch name required" that means no
          mismatches were found

        UPDATE: So interesting... Here's what happened. On my virtual machine is where I performed
          the initial clean-up by doing a git push origin --delete <branch>, <branch>, ... etc After
          that, they were deleted from the remote, but I still had them locally on the VM. That's
          when I discovered the -vv stuff and ended up running the match-local-to-remote alias. At
          that point, all was well on the VM. Now, fast foward to when I was working on my
          laptop. On the laptop obviously things were not in sync. When I ran the
          match-local-to-remote, nothing happened. So I pulled up the -vv listing and sure enough,
          not a single branch talked about being ':gone'.  Odd... So I tried running git fetch
          --prune and that's what ended up adding the ':gone' distinction to the -vv
          command. Running match-local-to-remote at that point brought it all in line
        UPDATE 2: The `git fetch --prune` command is now also carried out with the `$ git match`
          alias command. Awesomesauce!

      }}}

    Merging and prepping for merge {{{
      git checkout <receiving_branch>
      git merge <branch name you want to have merged into currently active branch>

      git branch --merged
        lists branches merged into current branch
      git branch --merged <branch_name>
        lists branches merged into named branch
      git branch --no-merged
        lists branches which have not been merged

      git merge-base [options] <commit> <commit>
        find best common ancestor, useful for determine what to merge
      git merge-base [options] <branch> <branch>
        https://git-scm.com/docs/git-merge-base

      git branch -r | xargs -t -n 1 git branch -r --contains
        NOTE: aliased as 'contains'

      git fetch && git branch -r --merged | grep -q origin/master && echo Incorporates origin/master || echo Out of date from origin/master
        NOTE: aliased as 'incorporates'

    }}}

    Cherry-picking {{{
      NOTE: If you're wanting to move existing commits into a new branch, checkout the notes you
      made in the Troubleshooting and Notes section, because there's a MUCH easier way to handle
      this

      git cherry-pick <short or long-commit-hash-here>
        This allows you to merge only a specified commit into your currently checked out branch.
      git cherry-pick <older_commit>..<newer_commit>
        split range of commits into currently active branch, NOT inclusive
      git cherry-pick <older_commit>..^<newer_commit>
        same as above, the use of a ^ signifies the range IS inclusive
      git merge <commit>
        This merges everything up to and including the specified commit
        NOTE: If you've previously pushed the commits up to remote, when you get done modifying the
        local git history, you'll probably receive some sort of message notifying that your local
        commits are out of date and that you need to pull down from the remote. In this case,
        however, you'd like to push your changes to the remote. In order to do that, you "force" the
        push with the following:

            $ git push -f

    }}}

    Rebase-ing {{{
      NOTE: From what I've read, many say that cherry-picking can be the better approach when it
      comes to modifying branch histories and stuff. Rebase is powerful, especially with -i
      (interactive mode), but chances are, you may be trying to do something that cherry-picking was
      specifically designed for.

      $ git rebase <commit> -i
        NOTE: They key here is to use the commit just BEFORE where you want the break. Because the
        "move" will include the named commit

      You'll see in the new view a bunch of listed commits across the top, with a word at the far
      left (probably "pick"). In order to tell git what you'd like to 'cut-out' from the current
      branch, you replace the word "pick" with "drop", then :wqa

      At this point, if you do a `$ git lg` you'll see that, locally, things have been modified. Yet
      whenever you do a `$ git status` you'll likely be told that you're local commits are behind
      and that you need to pull down from remote. While that message is correct, you KNOW local is
      different and you'd like to push the changes through to remote anyway. In order to do this,
      you must "force" the push like so:

        $ git push -f

    }}}

    Resolving merge conflicts {{{
      git mergetool
        There have been some instances where opening conficted files through vim and resolving
        through :Gdiff simply doesn't work. Specifically, when trying to resolve conflicts related
        to .tmux-resurrect/last... In these instances, rather than try to fix them through vim, you
        can use `$ git/dotfiles mergetool` and you'll be asked whether you want to local or remote
        file to take precedence. Make your pick and BOOM! it's done! Granted, this method doesn't
        afford any sort of fine-tuning or line-by-line selectivity, but for handling .tmux-resurrect
        stuff, it's poifect! There are, evidently, ways to further customize this option, to even
        bring in vimdiff or even fugitive's Gdiff, but for now, I view mergetool as being
        a hassle-free alternative to those other GUI driven options...

        On usage, rather than manually copy and paste things from side to side, you can simply
        highlight one of the noted different 'sections' then use one of the following:

          :diffget or <leader>do
          :diffput or <leader>dp

        and vim will make the changes accordingly. Very cool!

        NOTE: I recently found that I was not able to "put" leftside changes to the rightside file
        during a merge session, because it complained about it not being a modifiable file. To get
        around this, I cursored over to the right side file and ran:

          :set ma

        This allowed it to be modifiable, and since I was on the right side, I could then use
        :diffget to pull over the desired content from the left side. Now, what's weird in all of
        this is that I'm relatively confident this is nothing but window dressing since IIRC the
        right side file is not actually representative of an actual file that you can "save to". It
        got me past this hiccup, but I'm still thinking there's a better way... Maybe it has
        something to do with the modified mergetool plugin thing I'm using from whitehinge or
        whatever his name is?

        :cq
          if you ever want to close out of the mergetool without recording any sort of change (ie
          still have vim return that merge conflicts exist), then use this command. It only works if
          you have the mergetool configured with `trustExitCode = true`, which you do if you're
          using .dotfiles .gitconfig file

        Here's a rundown of some of the merge tools that are available:
          # diffconflicts  {{{
            This vim plugin offers a modified version of the built-in vimdiff tool. It presents
            a two-way diff which is much easier to read and decipher than the normal three-way diff
            that's shown in vimdiff. Here are the notes you have copied from your .vimrc file:

              :DiffConflictsShowHistory " open additional tabs that include traditional three-way
              vimdiff. This command might come already pre-set to the following shortcut:

                <leader>D

              On use: The left hand side is the one that you'll end up 'keeping'. In his example
              video, he generally goes through the edits by hand, copy and pasting things over from
              the right side into the left side if they're that straightforward or making a mix and
              match of both worlds if there are elements he wants from both instances. The three-way
              view, he says, can be useful when you're trying to suss out the history or thinking
              behind some of the changes that are noted, but aside from that, he strongly recommends
              doing all edits and changes in the two-way view.
              https://www.youtube.com/watch?v=Pxgl3Wtf78Y

              And from there, the commands work the same as normal:

                ]c # jump to next confict
                [c # jump to previous conflict
                do # get change from other window, into current window. 'o' is for 'obtain'
                dp # put the changes from the current window into the other window

          }}}

          # vimdiff  {{{
            http://vimdoc.sourceforge.net/htmldoc/diff.html
            :h vimdiff # from inside vim

            Vim's own built in git diff tool. Can also be used "outside" of git to simply compare
            the contents of multiple files

              $ vimdiff <file1> <file2>

            NOTE: There's a really neat 'upgrade' to vimdiff that improves how the files are
            displayed https://github.com/whiteinge/dotfiles/blob/master/bin/diffconflicts

            $ git mergetool
              this launches vimdiff for the file
              This is very similar to :Gdiff. The top three windows show, in left to right order,
              are the
                * Local file
                * Base file (before any changes from either)
                * Remote file
              The bottom pane is the resulting Merged file.

              In order to tell it what changes to accept, move to the bottom Merged file pane, and
              move your cursor to a designated conflict.

                ]c # jump to next confict
                [c # jump to previous conflict

              Color codes:
                Blue-ish = Auto-resolved by git, with any additions highlighted in a slightly
                different color
                Red = deleted from file
                Green = added to file

              From there you then use one of the following commands:
                :diffg RE
                  accept Remote
                :diffg BA
                  accept Base
                :diffg LO
                  accept Local

                :diffu
                  run after each diffg command so that the vimdiff is "redrawn" to reflect latest
                  changes

              Sometimes it can be hard to see what the differences are between the files if all
              three variations (local/base/remote) are actively comparing against one another. To
              make things stand out a bit more, you can temporarily deactivate diff comparison's for
              the unneeded file

                :diffo
                  turn off diff highlighting for the active file
                :difft
                  turn on diff highlighting for the active file

                do
                  get change from other window, into current window. 'o' is for 'obtain'
                dp
                  put the changes from the current window into the other window

              Once you've moved through all of the conflicts and made your selections, :wqa to save
              the file then proceed with your normal $ git commit and $ git push

           }}}

          # kdiff3 {{{

          }}}

          # meld {{{
            http://meldmerge.org/

          }}}

          # tig {{{
            https://jonas.github.io/tig/

          }}}

    }}}

    Diffing {{{
      NOTE: I'm showing the 'modifed' difftool command below, though you can use the 'basic' `$ git
      diff` command instead
      UPDATE: So what the difftool does is simply restructure/reformat/redisplay the output from the
      git diff command. This is why when you get into the difftool layout, you're working with
      temporary files, not the "live" files themselves. In order to save to the "real" file, you
      specify the file as part of the :w command, like `:w <filename>`
      QUESTION: Is there any easy way to get to the file's location? Because right now, I'm having
      to manually type out the filepath, which means I usually have to pull it up in a separate vim
      session just to confirm where the file is located...
      UPDATE2: It sounds like vim-fugitive may introduce a way to save to the live file

      Here's the structure of the command:
        $ git difftool [<options>] [<commit> [<commit>]] [--] [<path>...]

      git difftool
        See what changes you've made to your files since your last commit
      git difftool <older_commit>^..<newer_commit>
        Shows all differences that occured between the two named commits. If you want to list the
        changes just between the two commits and not include any intervening stuff, then leave off
        the ^ afer the first commit name. You can save the output to a file with:

          git difftool <older_commit>^..<newer_commit> <file_name>
            UPDATE: See notes below on :TOhtml, that helps you save it to a cool format
          `$ git difftool a1234^..b7890 diff_file`

      git difftool <branch1> <branch2>
        shows all differences between two named branches
      git difftool <branch1> <branch2> <filename>
        shows all differences of a single file between two named branches
        NOTE: In practice, if you want to compare the active branch to the another branch, simply
        checkout one of the branches then use `$ git difftool <other_branch>` and it will show
        results as expected

      Some options you may want to throw in there from time to time:
        -d
          copies the modified files to a temporary location, then performs the diff on those files
        -y
          no prompt

      Similar to how you use the mergetool, rather than manually copy and paste things from side to
      side, you can simply highlight one of the noted different 'sections' then use one of the
      following

        :diffget
        :diffput

      Or, another way, while working in normal mode (ie, NOT highlighting anything, but cursoring
      the difference):
        <leader>do
          "diff obtain", same as :diffget
        <leader>dp
          "diff put", same as :diffput
      and vim will make the changes accordingly. Very cool!

      If you want to save the difftool output to a file, use the following:
        * Run the difftool command and have the resulting comparison files open and 'active' in vim.
        * In that vim session, run the following commands
        *   :TOhtml
        *     This will cause a few operations to run in the background, ultimately resulting in
        *     a new file being opened by vim. This is essentially the 'html' version of the file as
        *     you just requested be created. You may see some errors reported in the message window,
        *     but just <enter> through them, you should be fine
        *   :w! <filename>.html
        *     Save the output to the file as named
        *   :q
        *     close the window/pane

    }}}

      fixup and squash {{{
        https://fle.github.io/git-tip-keep-your-branch-clean-with-fixup-and-autosquash.html

        These two commands sound like a way to do some commit "housecleaning" in the sense that it
        can "blend" otherwise superfluous commits together. So all of those tiny, insignificant,
        maybe even horribly phrased commits can be rolled into something more appealing, etc.

      }}}

    blame {{{
      haven't used this yet, but evidently it can offer a very granular view for when each line what
      changed.

    }}}

    worktree {{{
      https://git-scm.com/docs/git-worktree
      https://opensource.com/article/21/4/git-worktree
      https://levelup.gitconnected.com/git-worktrees-the-best-git-feature-youve-never-heard-of-9cd21df67baf

      This is a way to pull down multiple branches saved in a completely isolated manner. You can
      still jump from one branch to the other, the difference is that rather than have to constantly
      checkout and compare from one single branch, having multiple branches readily at hand can make
      comparisons and merges much more intuitive and easy to navigate, etc.

      Evidently, you're able to float in and out of "worktree" mode as needed.

    }}}

    Git bisect, finding bugs {{{
      https://git-scm.com/docs/git-bisect
      https://stackoverflow.com/questions/4713088/how-to-use-git-bisect
      https://vladroff.medium.com/cheatsheet-how-to-use-git-bisect-8b1a88b44dae
      https://manpages.ubuntu.com/manpages/kinetic/man1/git-bisect.1.html

    }}}

  }}}

  Installing programs through git {{{
    UPDATE: Okay so after reading a bit more, you do NOT want to use checkinstall. Evidently it can
    introduce some pretty big security risks. That, and it is no longer maintained.
    UPDATE: Now here's something interesting... This whole time I've built things with some
    variation of the `make` command. Which is well and good, except for the fact that unless the
    source files contain an uninstall file, trying to "undo" a make build and install can be a HUGE
    pain the ass since files often get spread everywhere throughout the system. The alternative? To
    use something called `checkinstall`
      https://help.ubuntu.com/community/CheckInstall

      $ sudo apt install checkinstall

    Then you can "make" a package through checkinstall and it will create an "uninstall" file

      $ sudo checkinstall make <package>

    You may find from time to time that the repo package available through apt is not the most up
    to date. This is largely because of the rather lengthy and involved approval process that
    Apt/Ubunbut/Whomever goes through before greenlighting an update to the main repo distrubution
    thingymajig. Overall, this is a good thing, since it helps insure that things will ultimately be
    compatible and 'play nicely' together. But there are times where a lil' user override may come
    in handy. I'll demonstrate with vim...

    UPDATE: The following has been added to .bashrc as aliases that will be referenced below

      $ vimgitinstall # alias

      manual {{{
        $ sudo apt build-dep vim # this installs all dependencies related to vim

        $ sudo apt update

          Error on source URI's? {{{
            NOTE: If this returns an error about a missing source URI's in your sources.list, then
            you need to make a quick change to apt. Here's what you do:

              System Tools -> Synaptic Package Manager -> Settings -> Repositories -> Then check the
              box next to "Source Code"

            If that's not there, then try this through the terminal

              $ sudo sed -Ei 's/^# deb-src /deb-src /' /etc/apt/sources.list

        }}}

        $ sudo apt install libncurses5-dev libgnome2-dev libgnomeui-dev libgtk2.0-dev \
        libatk1.0-dev libcairo2-dev libx11-dev libxpm-dev libxt-dev libxtst-dev python-dev \
        python3-dev ruby-dev lua5.1 lua5.1-dev libperl-dev vim-nox vim-gtk

        $ sudo apt update

      }}}

      $ cd /usr/share
        default install location for all users, where you'll eventually unpack vim
      $ sudo git clone https://github.com/vim/vim.git <folder>
        I found that when I already had vim installed, the 'vim' directory was already there, which
        means git appropriately complained that the folder already existed and was not empty. To get
        around this, I simply pulled it into a separate directory (vim82), cd'd into that directory
        on the next step, and progressed from there
      $ cd <folder>
        ie vim82 of whatever you used above
      $ sudo git pull

      $ vimgitconfigandmake # alias

      manual {{{
      NOTE: What I noticed on the next step here here is that the python3 interp may first require
      that you have python3 setup and ready to go.

        $ sudo apt install python3-pip
        $ pip3 install --upgrade setuptools

      UPDATE: Yep, can confirm! Setting up the python3 stuff first then running the ./configure
      stuff is what did the trick

        $ sudo ./configure --with-features=huge \
                           --enable-multibyte \
                           --enable-python3interp=yes \
                           --enable-rubyinterp \
                           --with-python3-config-dir=/usr/bin/python3-config
        $ sudo make
          $ sudo make check
            This command is recommended, so it will run checks of some sort, but it's never worked
        $ sudo make install
        $ sudo make clean
        $ sudo make distclean
        <restart computer>
        $ \vim --version # Baddabing! Should be good to go muchacho :)
          NOTE: Make sure you're seeing
            +clipboard
            +xterm_clipboard

    }}}

  }}}

  Troubleshooting and notes on special situations  {{{
    corrupt branches {{{
      Now, you ran into an interesting situation that combined several of the commands you have
      here. What happened was you had a current branch that became corrupted. Not sure how, may have
      had something to do with a faulty RVM update to
      2.5.0 before it was fully supported... BUT! That's beside the point. What you ended up doing
      was going back to a prior commit where everything was working. You then merged individual
      files from the fubar branch that you needed. This is done with the `$ git checkout <branch>
      <file1> <file2> ...` command. That really saved your bacon on this'n!

      UPDATE: As you've noted above near your `$ git checkout` notes, this is also how you
      essentially "merge" only certain files from another branch.

    }}}

    file conflicts not resolved with `reset --hard` {{{
      Here's another hairy situation you ran into... You did a bunch of work on your laptop, but
      when you came home and did a pull, you ran into a bunch of file conflict issues that could not
      be avoided through a simple "git reset --hard". This led to a string of pulls and merge
      attempts only to find out that, after the dust had settled, that some of the merges that you
      pushed overwrote some files in a really fubar way. So then, you needed a way to get back to
      how things were like four or five commits ago before this whole fiasco started! Here's what
      you ended up doing:

        git reset --hard <commit_that_you_want_to_return_to>
        git status

      Now that you were at this "older" state, git rightly now complained that you were several
      commits behind and that you needed to use git pull to bring things back into order. But that's
      precisely what you DIDN'T want to do, since doing so would bring you right back where you
      started! So how the heck do you tell your git remote to simply forget about those messed up
      commits that were ahead of you? Like this

        git push --force origin <branch_name_that_you're_currently_on>

      DANGER DANGER, a note of warning is in order here because this is a very powerful (read:
      destructive) git command. What it does is it deletes commit histories from the git remote.
      Which means DO NOT use this if there is more than one user using the repo. Because if they're
      on one of those "ahead" commits and you wipe it from the remote server, it's gonna be one huge
      ball of wax for them to get their files and edits and stuff uploaded again. This is why nine
      times outta ten, it's best to instead use revert rather than reset, because revert will step
      things back without deleting things from the remote. Does that make sense? Maybe I could have
      used revert here too, but to be honest, the top three or four results I read through all
      recommended reset for solo user environments.

      Another way to accomplish something similar to the above. Let's say you just created and
      pushed over a new commit and you'd like to essentially "undo" that action and go back to how
      things were in the commit just previous. WARNING!! This only recommended for private respos.
      If you ever find yourself in a situation where it's possible another user already pulled down
      the commit you just uploaded, then you're probably better off simply reverting back to the
      older commit and then making and entirely new branch. That said, if this is a solo dealymabob,
      here's how'n ya can do it!

        $ git reset --hard <previous commit>
        $ git push origin HEAD --force

    }}}

    "ours" merge strategy to replace branch contents {{{
      To completely replace the contents of one branch with those of another, make use of something
      called the "ours" merge strategy. For example, when I fubar'd my master branch and wanted to
      use another recently created branch in its place.

      https://stackoverflow.com/a/2862938
        $ git checkout <branch you want to keep>
        $ git merge -s ours <branch you want to replace>
        $ git checkout <branch you want to replace>
        $ git merge <branch you want to keep>

    }}}

    remove files from git history {{{
      There may be a way to get rid of a file from ALL branches in a repo. I haven't tried it yet,
      but it's something I want to eventually do for the z_extras directory, my journal related
      files, and maybe some others...

      To kick things off, you'll want to double-check the absolute file/directory name you want to
      target. This is where the following aliases may come in handy

        $ gittracked
        $ gittrackedforever

      To remove a file named passwords.txt from your entire history, you can use the --tree-filter
      option to filter-branch:

        $ git filter-branch --tree-filter 'rm -f passwords.txt' HEAD

      To remove an entire directory, use something like (note the use of `-rf`)

        $ git filter-branch --tree-filter 'rm -rf <directory>' HEAD

      Then, after this has been carried out locally, you need to force push the structure to the
      remote repo. This typically looks something like

        $ git push origin master --force

      Now it sounds like, even after all this, there still may be some more house cleaning to do.
      Namely, you need to force your local repo to dereference and garbage collect. This looks
      something like:

        $ git for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin
        $ git reflog expire --expire=now --all
        $ git gc --prune=now
        $ git push origin --force --all
        $ git push origin --force --tags

    }}}

    extract portions to another repo {{{
      If you want to extract a portion of the repo into a separate repo all to itself, THAT IS
      POSSIBLE. The following command was listed on a site, though I'm not sure where it specifies
      the new repo name... So proceed with caution here, you'll likely want to pull up some more
      write-ups and/or consult the git documentation to make sure you're doing this properly :)

        https://sebastian-feldmann.info/rewriting-history-with-git-filter-branch/

        $ git filter-branch --force --prune-empty --subdirectory-filter <directory> @

    }}}

    repo access on fresh hard drive {{{
      See "Initial Install" process on a fresh system

    }}}

    "fatal: multiple stage entries for merged file..." {{{
      try the following fix in the app's root directory:

        $ rm .git/index
          Then add all files and commit and whamo! Should be fixed

    }}}

    "unrelated histories" {{{
      I was working with a git repo and deleted the .git directory. Without thinking much of it,
      I ran `$ git init` and repeated the same setup as before. But here's the deal... Because I had
      local changes that were not part of the remote, git complained something to the effect of
      "Fatal: refusing to merge unrelated histories". This issue seems to arise from the fact that
      git cannot make sense of how the branch jumped from the remote commits to where I had
      advanced. In order to allow the pull to go through, I had to run the following:

        $ git pull origin master --allow-unrelated-histories

      After that point, it let me pull, then I had a couple merge conflicts to resolve, but that's
      no biggie

    }}}

    git error object file is empty, loose object is corrupt {{{
      Received this error after my laptop's battery ran out during a git pull action. I fixed it by
      doing two things... Though I suspect there may be a better way about it given the first steps
      pretty much emptied out the files' contents, and the following steps per `revert` put things
      right again. Maybe next time, try using the marked answer as shown presented here?
      https://stackoverflow.com/questions/11706215/how-to-fix-git-error-object-file-is-empty#31110176
      In the below notes, .git should be replaced with .dotfiles if that's the realm you're working
      in...

        $ sudo cp -a .git .git-old
          creates a copy of the .git directory incase things go sideways
        $ find .git/objects/ -type f -empty | xargs rm
        $ git fetch -p
        $ git fsck --full

      NOTE: It was at this point that the files were rebuilt as essentially "blank" files. In order
      to restore their contents, I went back and reverted to the pre-fubar commit as follows:

        $ git revert <commit_hash>

      At that point, everything was back to being right as rain

    }}}

    "warning: Permanently added the RSA host key for IP address xyz to the list of known hosts" {{{
      I have no idea what caused this message to start appearing, but it would usually happen
      whenever I pulled things down from the repo. One way to deal with is to do'a this'a

        ~/.ssh/config
          Create this file and insert the following line:

            UserKnownHosts ~/.ssh/known_hosts

      Save that bad boy and after you've interacted with git another time or two you should stop
      getting the message

    }}}

    changes not staged for commit: ... (untracked content) {{{
      UPDATE: Here's what ended up working for me, at least for the time being.

        cd into each of the .vim/plugged/<repo> directories and run

          $ git status
          $ git reset --hard HEAD
          $ git clean -fxd

      I've found this message usually relates to vim plugins managed by vim-plug. Basically, the
      plugins' install directories themselves contain a .git directory, likely an unintentional
      carry-over or oversight by the author. In order to get around this, it seems like there are
      a few approaches.

      The first involves simply going into each of the offending plugins' directories and removing
      the git related folders. The only problem there is they have a tendency to creep back into the
      repo over time as the author updates them. This also doesn't seem to work with vim-plugged
      because it will simply reacquire those files later.

      A second way involves setting up a .gitignore regex to effectively ignore the offending
      directories.

      The third approach, and likely most preferable, involves setting up git submodules for these
      files/directories. I'm not entirely sure at this point what that entails...

        https://blog.github.com/2016-02-01-working-with-submodules/

      For the time being, I've taken the easy route and simply deleted the git related files, but at
      some point I'd like to learn the more proper way to handle it

    }}}

    conflicts with merge tool settings {{{
      I ran into a weird situation whereby the project's `.git/config` file contained a setting that
      overrode the merge tool setings established in the --global file. Once I removed the merge
      tool settings from the project's git file, everything fell back into place! Here are the
      commands I used to root out the problem:

        $ git config -l
        $ git config --global --list
        $ git config --list --show-origin
        $ git config --global --list --show-origin

    }}}

    "src refspec <branch> does not match any... " {{{
      This is an error message I received after I had created a new branch, made some commits, and
      then finally tried to push the new branch with `$ git push -u origin <branch>`. I'm suspecting
      this may be because I didn't push the initial commit, but instead kept working on the branch
      for some time before finally trying to push things over (??). Anyway, here's how you fixed
      it... You basically had to be a bit more specific in telling git what exactly it was you were
      wanting to push over

        $ git show-ref
        $ git push origin HEAD:<branch>

    }}}

    copying a file from one commit to another {{{
      I had a corrupted .viminfo file that I inadvertantly pushed through several commits. This left
      the master branch with a fubar .viminfo file. Here's how I replaced it with a uncorrupted
      .viminfo file from an earlier commit

        $ git checkout -b "temp"
          make a temporary branch, this will be where I pull the desired file into
        $ git checkout <commit> -- vim/.viminfo
        $ git add -A
        $ git commit -m "temporary branch with uncorrupted viminfo file from commit <commit>"
        $ git checkout master
        $ git merge temp
          this essentially brings over the full .viminfo file, replacing its contents
        $ git branch -D temp
          this deletes the locally created temporary branch
        $ git add -A
        $ git commit -m "fix .viminfo file, pulled from commit <commit>"
        $ git push

    }}}

    comparing and saving differences between a file/s on a different branch {{{
      First, make sure you're in the branch you want to RECEIVE the edits

        $ git checkout <receiving_branch>

      $ git checkout --patch <other_branch> <file>
        This will go through a sort of interactive mode

      If, on the other hand, you want to take the file as-is from the other branch, check it out
      entirely without `--patch`

        $ git checkout <other_branch> <file>

    }}}

    moving a recent set of commits into a new branch {{{
      In the past, I've tried doing this with cherry-pick, but I rencetly discovered a much easier
      approach. You simply create a new branch, which will contain everything up to that point.
      Then, go to the branch you want to remove the commits from, and reset the branch back to
      a certain commit hash. In practice, it looks something like this

        $ git checkout -b <new_branch>
        Then, go back to the branch you want to remove the history FROM
        $ git branch <from_branch>
        $ git reset --hard HEAD~3 --OR-- $ git reset --hard <commit>
        $ git checkout <new_branch>

      Now, if you've already pushed some of the commits that you're REMOVING from the from_branch,
      git will likely complain that you are behind the remote counterpart and need to 'pull' to
      bring things back into order. But this is not what you want to do. You want to force-push the
      new local branch structure out to remote. This is the exact same out-of-sync situation you
      describe in the cherry-picking notes, and it's resolved in the same manner as well

        $ git push -f

    }}}

    "autopacking repository in background for optimum performance..." {{{
      Probbaly got some dangling blobs hanging around. Try these commands to clean'em up 'fore you
      run off to see the doc...

      $ git fsck
      $ git gc --prune=now

    }}}

    merging only specific files from one branch to another {{{
      <checkout receiving branch>
      $ git checkout <branch that has what you want> <file_name>

    }}}

    updating github url {{{
      Ran into this whenever I changed my bitbucket profile to be BlakeLeBlanc
        NOTE: The below syntax is for an SSH setup, refer to the initial install syntax if you need
        to use HTTP

        $ git set-url origin git@bitbucket.org:BlakeLeBlanc/<REPO NAME>.git

    }}}

    sync bitbucket repo activity to a github account {{{
      https://medium.com/@dmitryshaposhnik/syn-bitbucket-repo-to-github-669458ea9a5e

    }}}

    Cloning outside repo then pushing to your OWN repo {{{
      After you've pulled down the repo...

        $ git clone https://github.com/<USER>/<REPO NAME>.git

      ...you essentially have a few options, depending on what you want to be able to do from that
      point forward.

      1. If you don't really need to the source repo any more, you can simply update the origin url
         to point to your own repo

        $ git remote set-url origin git@bitbucket.org:BlakeLeBlanc/<repo_name>.git

        $ git push origin master

      2. If you want to retain the ability to pull updates from the source repo, then you can either
         add a new remote that you can then reference in your git actions

        $ git remote add <new remote name> git@bitbucket.org:BlakeLeBlanc/<repo_name>.git
        $ git push <new remote name> master

        OR

      3. You can rename the 'original' origin to something else (this essentially deletes the
         existing 'origin' reference), and add a new 'origin' remote to point to your own repo

        $ git remote rename origin <new remote name>
          ie $ git remote rename origin upstream
        $ git remote add origin git@bitbucket.org:BlakeLeBlanc/<repo_name>.git
        $ git push origin master

      Whichever you use here, 2 or 3, you can pull updates from the named source with the following:

        $ git fetch <remote name>
          ie $ git fetch upstream (if you're using something like no 3)

        Or, let's say you'd like to pull the updates from the other repo into your own remote's
        branch

          $ git pull <other remote> master
            ie $ git pull upstream master

      4. For a more nuclear option, when you want to go scorched earth on anything and everything to
         do with the other repo, you can simply delete the .git directory altogether and start from
         scratch like you would in a fresh git setup

         $ rm -rf .git
         $ git init
         $ git add .
         $ git commit -m "message"
         $ git remote add origin
         $ git push origin master

    }}}

    recovering previously deleted files, with changes that were commited and pushed {{{
      $ git log --diff-filter=D --summary
        Lists all the deleted files, along with commit ID's
      $ git checkout <commit_id>^1 <file_name>
        Note the addition of `^1` at the end of the commit_id

    }}}

  }}}

}}}

Git Flow {{{
  NOTE: As cool as this tool sounds, it's actually a little controversial. The main critique is that
  is unnecessarily "over-engineers" a git repo to the point where trying to make sense of its `git
  lg` history is virtually impossible.

    https://www.endoflineblog.com/gitflow-considered-harmful
    https://www.endoflineblog.com/follow-up-to-gitflow-considered-harmful

  $ sudo apt install git-flow

  https://github.com/nvie/gitflow

  https://danielkummer.github.io/git-flow-cheatsheet/
  https://www.youtube.com/watch?v=BYrt6luynCI

}}}

Go {{{

  Build from source {{{
    $ cd /usr/share
    $ sudo git clone https://go.googlesource.com/go goroot
    $ cd goroot
    $ git checkout go1.15.4
      NOTE: This is optional, you can also just as easily stay on the master branch
    $ export GOROOT_BOOTSTRAP=/usr/local/go
    $ cd src
    $ ./all.bash

  }}}

  gvm {{{
    $ bash < <(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)
    $ gvm install go1.4 -B
      NOTE: Installing later versions requires that go is actually installed on the system.
    $ gvm use go1.4
    $ export GOROOT_BOOTSTRAP=$GOROOT
    $ gvm listall
    $ gvm install go1.15.5
    $ gvm list
    $ gvm use <version> --default

    Uninstalling gvm
      $ chmod u+w -R ~/.gvm/
      $ gvm implode

  }}}

  snap {{{
    $ sudo snap install go --classic

  }}}

}}}

HMTL and CSS  {{{
  https://css-tricks.com/almanac/properties/p/position/
  https://css-tricks.com/absolute-relative-fixed-positioining-how-do-they-differ/
  https://css-tricks.com/absolute-positioning-inside-relative-positioning/

  https://html.spec.whatwg.org/

  CGI - Common Gateway Interface {{{
    Great resources to get started, recommended to read/watch in the following order
      1. http://www.whizkidtech.redprince.net/cgi-bin/tutorial
      2. http://jkorpela.fi/forms/cgic.html
      3. https://www.youtube.com/watch?v=9Zx1vEmaQMc
      4. https://www.youtube.com/watch?v=P8aGeN-8-l4

    https://en.wikipedia.org/wiki/Common_Gateway_Interface
    https://stackoverflow.com/questions/2089271/what-is-common-gateway-interface-cgi

  }}}

  CSS (notes from freeCodeCamp course) {{{
    @media (min-height: 800px) {
      font-size: 10px;
      ...
    }

    Reponsive image sizes
      img {
        max-width: 100%;
        height: auto;
      }

    Responsive text sizes
      vw
      vh
      vmin, of the smaller dimension
      vmax, of the larger dimension

      example:
        body {
          width: 30vw;
        }

  }}}

  HTML5 (notes from freeCodeCampe course) {{{
    https://w3.org/TR/html52/single-page.html

    alt text
      mandatory for images, can be a blank string if described in a caption or used for decoration
      (ie a background image)

    article
      A more specific form of sectioning, wherein independent, self-contained content resides

    fieldset
      surrounds entire group of radio buttons

    figure and figcaption
      Wrap things like charts and tables

    header
      NOTE: There's a <header> and <h1>, serving somewhat different purposes.
      * Used to wrap introductory information or navigation links, works well around content that is
        repeated at the top on multiple pages
      * Each page should have only ONE h1 element
      * Used to denote and describe different subsections of the page, NOT solely for visual
        purposes. If you need text to have a larger size, use other CSS approaches like relative
        sizing rather than fallback on a particular header element. Because with header tags, they
        serve a more technical purpose of informing search engines, screen readers, etc.
      * They are meant to work in descending order, so a subsection of an h1 tag is h2. A subsection
        of an h3 tag is an h4.

    input
      type
        text
        submit
        date

    legend
      surrounds the initial question/whatever part of a radio/checkbox response set

    nav
      Meant to wrap around navigation links in your page

    section
      For grouping thematically related content.
      Can be used within Article. For example, if a book is the Article, each chapter could be
      considered a Section

    tabindex
      allow <tab> presses by the user to cycle through the denoted div, span, or p elements
        <p tabindex="0">...</p>
      Use 0 to add it into the mix with the "default" tab ordered items
      Use positive non-zero numbers to specify a specific order, these will be cycled first before
      then proceed through the default 0 numbered tab-enabled items

  }}}

  Analyze css {{{
    https://www.cssstats.com
    https://yellowlab.tools
    https://isellsoap.github.io/specificity-visualizer
    https://github.com/katiefenn/parker
    Chrome DevTools - CSS Overview Panel
      https://developers.google.com/web/tools/chrome-devtools/coverage
    With Puppeteer
      https://willmanntobias.medium.com/how-to-bulk-find-unused-css-and-javascript-with-puppeteer-and-chrome-coverage-f79f7d885d59

  }}}

  animations and transitions {{{
    https://cubic-bezier.com

    Yes, you can use variables!
      https://codepen.io/wesbos/pen/gPZBZQ

    Loading icons and bars, configure and download them here
      http://ajaxload.info/

    If you want transitions or animations to happen immediately, the setting you're likely may not
    be transition-duration, but instead transition-timing-function.

      https://www.w3.org/TR/css-transitions-1/#transition-timing-function-property
      NOTE: These equally apply to animations and transitions (ie animation-timing-function)

      I used this in landapp for the bottom panel. Because things were based off of a hover state,
      if the user quickly moved his mouse over an area, the hover state would kind of partially
      fire, leaving the underlying element in this confused state. On the off hover, the opacity
      would not go back to zero, but instead, be "stuck" at the 0.99 value. Why? I'm not entirely
      sure, but I suspect it was an issue of timing. The hover ON caused the fadeIn to trigger and,
      since the user would be off of the button before the opacity shift was over, the fadeOut never
      actually happened.

      Once I set the transition-timing-function to step(1, end) the opacity shift was immediate and
      thereby no longer caught "left out in the cold" on quick hover ON OFF events.

  }}}

  box-shadows {{{
    https://shadows.brumm.af

    The shadow actually becomes part of the parent container, meaning if you have child images
    within the div, the images will not be affected by the shadow because they're essentially
    rendered "on top" of the box-shadow. One way to get around this is to include the box-shadow as
    a separate class and then absolutely position that class accordingly so the shadow will render
    "on top of" the child images.

  }}}

  css reset files {{{
    https://github.com/csstools/sanitize.css
    https://github.com/necolas/normalize.css
    https://github.com/murtaugh/HTML5-reset

    meyerweb css reset {{{
      /* http://meyerweb.com/eric/tools/css/reset/
      v2.0 | 20110126
      License: none (public domain)
      */

      html, body, div, span, applet, object, iframe,
      h1, h2, h3, h4, h5, h6, p, blockquote, pre,
      a, abbr, acronym, address, big, cite, code,
      del, dfn, em, img, ins, kbd, q, s, samp,
      small, strike, strong, sub, sup, tt, var,
      b, u, i, center,
      dl, dt, dd, ol, ul, li,
      fieldset, form, label, legend,
      table, caption, tbody, tfoot, thead, tr, th, td,
      article, aside, canvas, details, embed,
      figure, figcaption, footer, header, hgroup,
      menu, nav, output, ruby, section, summary,
      time, mark, audio, video {
        margin: 0;
        padding: 0;
        border: 0;
        font-size: 100%;
        font: inherit;
        vertical-align: baseline;
      }
      /* HTML5 display-role reset for older browsers */
      article, aside, details, figcaption, figure,
      footer, header, hgroup, menu, nav, section {
        display: block;
      }
      body {
        line-height: 1;
      }
      ol, ul {
        list-style: none;
      }
      blockquote, q {
        quotes: none;
      }
      blockquote:before, blockquote:after,
      q:before, q:after {
        content: '';
        content: none;
      }
      table {
        border-collapse: collapse;
        border-spacing: 0;
      }

    }}}

    From thoughtbot stream
      https://www.youtube.com/watch?v=_wixlPQzpI8

      * {
        box-sizing: border-box;
      }

      body {
        padding: 0;
        margin: 0;
      }

  }}}

  colors {{{
    https://learnui.design/tools/data-color-picker.html
    https://learnui.design/tools/accessible-color-generator.html

    hsl()
      hue: what is typically thought of as color
      saturation: amount of gray in color
      lightness: amount of white or black in color

  }}}

  floats, flexbox, and grid {{{
    These videos offer excellent comparisons showing how these three layout systems are best used to
    play alongside each other rather than as some sort of "winner-take-all" approach. They each are
    useful and serve specific purposes for certain areas/components within your web page's layout

      https://www.youtube.com/watch?v=hs3piaN4b5I
      https://www.youtube.com/watch?v=hYJvxsgnGMA

    floats {{{
      https://www.youtube.com/watch?v=xara4Z1b18I
      ^ This is the best and most concise description for how float works that I've seen. Basically,
      view a webpage as being inside a fishtank. As you 'float' elements, they float above the
      normal document flow, giving room for the 'normal' html stuff to scoot in to fill the void. If
      you `float: left` several elements, they will 'stack' against each other in a sideways fashion
      from left to right. And this is where the `clear: left/right/both` stuff comes into play. It's
      your way of informing the browser, hey `float: left` this element, BUT! when I next inform it
      `clear: left`, I'm essentially telling the browser to NOT position it next to anything on its
      left. So if there are previous elements floated left, the 'new' floated element would be given
      it's own 'line' directly underneath.

      Floats apply only to block-level elements, things like images, paragraphs, divisions, or
      lists.

      One rule of thumb that some follow is to use floats when the potential length of the content
      is unknown, so the browser can have flexibility to adjust and accomodate accordingly, and use
      position when things are precise and flexibility/adaptability to different content lengths/etc
      is not a concern.

    }}}

    flexbox {{{
      https://www.w3.org/TR/css-flexbox-1/
      https://css-tricks.com/snippets/css/a-guide-to-flexbox/

      For the parent container/wrapper, specify one of the following:

        display: flex;
        display: inline-flex;
          dictates how the flexbox will related to surrounding elements

      Then you can add any number of additional configuration settings, including:

        justify-content MAIN flex-direction, starting leftmost
          flex-start
          center
          flex-end
          space-around
          space-between

        align-items CROSS flex-direction, starting topmost
          flex-start
          center
          flex-end
          stretch
          baseline

        flex-direction
          row (default)
          row-reverse
          column
          column-reverse

        flex-wrap
          wrap
          nowrap
          wrap-reverse

        order

      And then, within each child flex item, you can specify a few more things, including...

        flex        # shorthand for <grow, shrink, basis>, defaults to `0, 1, auto`
        flex-grow   # how resized when 'normal width' is exceeded, by x times value
        flex-shrink # how resized when 'normal width' is restricted, by x times value
        flex-basis  # desired 'normal' width, will reference any `width` settings by default

        align-self # MAIN flex direction, how individual content items 'fill' alloted space
          stretch <-- default, causes distortions in image aspect ratios, etc
          center
          start
          end

    }}}

    grid {{{
      This seems to be an approach that allows you to break away from extrinsicly setting up every
      little jot and tittle for your layouts, which can make go haywire if the size/length of your
      content ever changes, and instead use a sort of layout algorithm that will interpret your
      intent for you and make reasonable interpretations regardless of what the content might
      actually be

      Resources to get started {{{
        http://jensimmons.com/post/feb-27-2017/learn-css-grid
        https://gridbyexample.com/examples/
        https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Grid_Layout/Basic_Concepts_of_Grid_Layout
        https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Grid_Layout
        https://24ways.org/2016/css-writing-modes/
        https://www.cssgridplayground.com/

      }}}

      grid-template-columns: <width in px> <width in px> ...;
      grid-template-rows: <height in px> <height in px> ...;

      Can also use absolute and relative units of measure, including:
        fr        # sets to a fraction of hte available space
        auto      # just what it sounds like
        %         # percentage of container
        auto-fill # can be used along with minmax to let Grid create as many cells as can fit the
          current width or height. If all items CANNOT fit, adds them to the the next row/column. If
          there are not enough cells to fill the space, it fills the remaining space with empty
          cells
            repeat(auto-fill, minmax(60px, 1fr));
        auto-fit  # can be used along with minmax to let Grid create as many cells as can fit the
          current width or height. If all items CANNOT fit, adds them to the the next row/column. If
          there are not enough cells to fill the space, it will collapse the empties and stretch the
          existing items to fill in the space.
        minmax(<minimum>, <maximum>)
          grid-template-columns: 100px minmax(50px, 200px);
            Translates to two columns, the first @ 100px wide, the second min @ 50px max @ 200px

      For ease of entry, things can also easily be repeated
        grid-template-rows: repeat(100, 50px);
        grid-template-columns: repeat(2, 1fr 50px) 20px;
          Translates to grid-template-columns: 1fr 50px 1fr 50px 20px;

      grid-column-gap: _px;
      grid-row-gap: _px;
      grid-gap: _px;
        grid-gap: <row in px> <column in px>;

      The span of columns and rows can be customized so they stretch across more than one grid line
        NOTE: Grids start at 1, then the right/lower "line" is counted. So in a 3x3 grid, the
        rightmost and bottommost line number would be 4.
      grid-column: <start line> / <end line>;
      grid-row: <start line> / <end line>;

      Each "square" within the grid space is called a CELL. And the cell contents can be aligned in
      various ways:
        Horizontally
          justify-self (individually)
          justify-items (all at once)

        Vertically
          align-self
          align-items (all at once)

        Each accepting the following values
          stretch (default)
          start
          center
          end

      grid-template-areas
        Allows custom naming of the various grid areas, grouped by rows
        . denotes an empty cell in the grid

        Example:
          grid-template-areas: "header header header" "advert content content" "footer footer
          footer";

        And then, to use these defined areas, you reference them like'a so'a
          .item1 {
            grid-area: header;
          }
        This tells CSS Grid that you want the item1 class to go in all the areas noted as "header". So
        in the above example, it would fill all three cells in the top row.

      You can also jump right into setting up a grid-area without first establishing a template.
      grid-area: horizontal line start / vertical line start / horizontal line end / vertical line
      end
      .item1 {
        grid-area: 1/1/2/4;
      }
      This informs grid that you want .item1 to consume rows between lines 1 and 2 and columns
      between lines 1 and 4.

      @media queries can be used to create some dynamic layouts. Here's how it works...
        The default layout here sets the "smallest" size, and arranges the content stacked on top of
        itself in a horizontal fashion, header on top, then the adverts, then the content, then the
        footer across the bottom

        .container {
          ...
          grid-template-areas:
            "header"
            "advert"
            "content"
            "footer";
        }

        Then we follow up with a media query that says, IF the viewport size is at least 300px wide,
        it starts to change things up a bit, into a two column three row setup. And the
        grid-template-area, as you can see, changes to place the advert as a top to bottom column on
        the left.

        @media (min-width: 300px) {
          .container {
            grid-template-columns: auto 1fr;
            grid-template-rows: auto 1fr auto;
            grid-template-areas:
              "advert header"
              "advert content"
              "advert footer";
          }
        }

        Then it hits up another media query that says, IF the viewport size is at least 400px wide,
        it changes the template again. This time, the header and footer assume the full width of the
        viewspace, with the middle portion taking up two columns of advert and content.

        @media (min-width: 400px) {
          .container {
            grid-template-areas:
              "header header"
              "advert content"
              "footer footer";
          }
        }

        Make sense? Cool!

      You can also next grids within grids, just throw in another display: grid; with settings

    }}}

  }}}

  font sizing {{{
    NOTE: these relative sizings can also be used with other attributes like padding, etc!

    Sizing fonts with "rems" is really handy. It's basically a relative sizing based on a standard
    1rem. So by saying 2rem, 3rem, etc etc you'll get proportionately sized fonts. Very easy and
    very cool!

    em - based on font-family, by measures specific to the font itself. Relative sizing based on
    HEIGHT of uppercase "M"
      rem - "root em", equal to the font size fixed to the root element, which is usually <html>
    ch - based on font-family, by measures specific to the font itself. Relative sizing based on
    WIDTH of zero "0"

  }}}

  image sizing {{{
    https://stackoverflow.com/questions/15685666/changing-image-sizes-proportionally-using-css

  }}}

  nesting {{{
    In scss (and maybe even 'regular' css) you do NOT have to track out all of the intermediary
    selectors in order to "get to" a deeply nested child. My understanding is that it works similar
    to the `.find` syntax in javascript/jquery (which may very well be putting the cart before the
    horse, as it's likely js/jq modelled their approach off of that used by css selectors).

    This is likely why it's regularly advised to NOT get too specific with your css structures (ie
    don't get too nested). Because the more nested you get, the more specific you make the context
    of the selector's use. Whereas if you stick to "simple" nestings at one or --at most-- two
    levels deep, you're allowing the selectors to be more flexible.

    parent {
      child1 {
        child2 {
          child3 {

          }
        }
      }
    }

    While the above may "match" the structure you've imparted in your HTML, it's likely much too
    specific. Because if you ever decide you want to move child3 somewhere else within the parent,
    well guess what? You've "lost" its reference here, meaning you'll have to go back and rebuild
    the css structure.

    Whereas if you keep things more generic, you retain that flexibility. Like this

    parent {
      child1 {}
      child2 {}
      child3 {}
    }

    My understanding is that for, say, child3 the selector compiles to `parent child3` which in
    itself is kind of like, "find child3 anywhere within this parent and apply this styling"

  }}}

  overflow, overflow-x, overflow-y {{{
    https://medium.com/@justicart/overflow-x-scroll-overflow-y-visible-c1a98238e002
    https://codepen.io/agop/pen/itbew
    https://codepen.io/toomuchdesign/pen/RKLGba

  }}}

  position {{{
    https://developer.mozilla.org/en-US/docs/Web/CSS/position
    Whenever you specify a position other than static (or relative, though there's a bit more to it
    than that), the element is effectively removed from where it would normally fall within the
    document's "raw" layout and structure and instead, is placed according to a reference point that
    you must define with a combination of one or more top/bottom/right/left values. The
    ramifications of this is that, since the element is "outside" of the raw document structure, the
    changes do not have a cascading effect on other nearby elements. Think of it this way... The
    normal raw document flow is kind of like a traffic jam. Moving one element with padding or
    margin causes others around it shift as well. However, when you use one of the alternate
    positions, that element is then removed from the "ground level" traffic jam and becomes
    a helicopter, free to fly above the fray unencumbered by its surroundings (the height value,
    should two or more elements be competing for some of the same airspace, is goverend by the use
    of z-index, which specifies which element should fly the highest and thus, be seen by the
    viewer). The question then becomes, what reference point do you want it to use?

    static
      default, follows the word flow as established in the "raw" html document itself
    relative
      positioned relative to the top-left corner of its otherwise 'normal' origin within the page.
      Meaning it DOES respect the "raw" document flow to a certain extent
    absolute
      positioned relative to the nearest positioned parent element it's nested within (I think
      that's right?)
    fixed
      positioned relative to, not the document, but to the viewport
    sticky
      hybrid of absolute and fixed

  }}}

  rendering order {{{
    https://stackoverflow.com/a/53892597
    The drawing order of rendering layers is:

      layout layer
      paint layer
      compositor layer
      A redraw in a layer will trigger redraw in subsequent layers.

    Changing left or margin will trigger a redraw in layout layer (which, in turn, will trigger
    redraws in the other two layers) for the animated element and for subsequent elements in DOM.

    Changing transform will trigger a redraw in compositor layer only for the animated element
    (subsequent elements in DOM will not be redrawn).

    The difference in performance (hence in frames per second or, in simple terms, in animation
    smoothness) is exponential. Using the first technique will often result in jittery animations
    even on good machines (when the processor is busy), while the second will likely run smoothly
    even on systems with limited resources.

    Another advantage of using transforms is compositor redraws are heavily optimized (animations to
    multiple elements result in one redraw for all), while changing layout layer will trigger
    a redraw after each change of each element.

    For a more detailed explanation on rendering techniques and rendering performance I recommend
    Google's Web Fundamentals.

      https://developers.google.com/web/fundamentals/performance/rendering/

  }}}

  svg files {{{
    NOTE: To properly display svg files in Rails, you use a gem called inline_svg

    http://tavmjong.free.fr/INKSCAPE/MANUAL/html/Web-Inkscape.html
    http://edutechwiki.unige.ch/en/Using_SVG_with_HTML5_tutorial

    https://css-tricks.com/svg-path-syntax-illustrated-guide/
    https://css-tricks.com/using-svg/

    Here's a save workflow using Inkscape
    Fit document size to object/image size
      File --> Document Properties --> Custom size --> Resize page to content --> Resize page to
        drawing or selection
      File --> Clean up document
      File --> Save as --> Optimized SVG (select from drop-down menu)
        <new Output related dialogue box displays> --> SVG Output (tab across top) --> ENABLE
        "Enable viewboxing"

      Now, at this point the image will work, but the file itself will be fairly bloated. Running it
      through the svg-optimizer site noted below will drastically reduce the file size. At this
      point I haven't tried the node.js tool

    https://github.com/svg/svgo # node.js tool for optimizing svg files
    https://jakearchibald.github.io/svgomg/ # a web version of SVGO, really easy to use
    http://petercollingridge.appspot.com/svg-optimiser # online tool for optimizing svg files
    https://yoksel.github.io/url-encoder/

  }}}

  z-index {{{
    https://philipwalton.com/articles/what-no-one-told-you-about-z-index/
    Problem: https://jsfiddle.net/qf9gzutj/1/
    Solution: https://jsfiddle.net/qf9gzutj/4/

    Only applies to positioned elements (ie anything other than the default "static" position). And
    this makes sense, given that the only way you can get elements to break out of their regular
    page-flow ordering as determined by the codebase is to manually override their positioning.

      For example, in a normal document flow, the footer would be "on top of the stack" as far as
      order is concerned, meaning it's contents would naturally overlap the body and header. BUT!
      Let's say you wanted to make sure the header stayed on top of the body during scrolling. If
      you were to make the header be `position: fixed` with a z-index of 1, it would be placed on
      top of the other default position: static zero z-indexed elements.

        header
        body
        footer

    Context is key, since HTML elements can be nested within one another. Think of context as
    creating a "group" of z-indexed elements. Elements within the same context respect one anothers
    relative z-index values, whereas elements in different contexts are, in some senses, 'shielded'
    from one another's individual z-index values and, instead, have their shared context positioned
    in order of their parent element's context.

    There are also "events" that create new contexts. I'm wanting to say that animations create new
    contexts, for example.

    Stacking order that comes with the "normal" document flow, without z-index values involved:
      1. Root element background and borders
      2. Descendant non-positioned blocks, in order of appearance in the HTML
      3. Floated blocks
          https://jsfiddle.net/qfpjz2b1/
          https://jsfiddle.net/quLo64gv/15/
          https://codepen.io/blake-leblanc/pen/VEyddX
      4. Descendant non-positioned inline elements
      5. Descendant positioned elements, in order of appearance in the HTML

    As for the z-index value itself, the number determines in what order things are 'laid ontop of
    each other'. So z-index
    1 items go down first, then z-index 2 items, then 3, etc. Think of it like adding layers of
    wallpaper or stacking cards on top of one another. This is why you don't need huge numbers once
    you understand it's a matter of order rather than height.

    Here's something else that's interesting... Let's say you have two divs side by side. And let's
    say that you want it so that when the user hovers over items in one of the divs --let's say
    they're buttons-- the buttons will slide out and cover/overlap the other div. This is something
    I did within the land_app's image-viewer and it threw me for a loop for the longest time. It
    turns out the key is in how you set the overflow properties. You've gotta set the
    wrapper/container for the buttons as `overflow: visible`. If you try to specify separate
    overflow-x and overflow-y values, things get a little screwy in CSS3's internal calculation
    stuff and it will NOT allow the slide-out to cover/overlap

  }}}

}}}

HTTP stuff{{{
  Response codes {{{
    1xx Informational
    2xx Success
    3xx Redirection
    4xx Client Error
    5xx Server Error

  }}}

  Link status tracker {{{
    UptimeRobot
      https://uptimerobot.com/

  }}}

}}}

Interface Frameworks and Libraries  {{{
  NOTE: Rails related applications of javascript and its many forms will have their notes saved in
  the Rails section, not here. This section is meant to be specifically focused on these frameworks
  in and of themselves. Any quirks are oddball things that must be undertaken in order to use them
  within Rails are discussed elsewhere.

  Bootstrap  {{{
    https://getbootstrap.com

  }}}

  Bulma {{{
    https://bulma.io

  }}}

  charts.js {{{
    https://github.com/chartjs/Chart.js

  }}}

  d3 {{{
    "Data-driven documents", integrated closely with SVG graphics. Very cool!

    https://d3js.org
    https://github.com/d3/d3

  }}}

  Javascript {{{
    Helpful links {{{
      https://www.youtube.com/watch?v=K-sns5tNdTY
        Excellent rundown by Pragmatic Studio of how javascript fits into Rails
      https://www.youtube.com/watch?v=FBxVN7U1Qsk
        Railscasts doing his thang!
      https://www.youtube.com/watch?v=4jjL7DTiVvk
        Ruby Snack folks

      https://javascript.info/

      An excellent and thorough run-down on just about anything and everything Javascript, by the
      pros at MDN!
        https://developer.mozilla.org/en-US/docs/Learn/JavaScript/First_steps

      https://github.com/ryanmcdermott/clean-code-javascript

      http://blog.niftysnippets.org/2008/02/closures-are-not-complicated.html

      https://github.com/getify/You-Dont-Know-JS

    }}}

    Notes  {{{
      call, target, bind {{{
        These object methods provide ways for overriding/providing a different context to a function

      }}}

      Context {{{
        What `this` currently refrences

      }}}

      convert from jQuery object to a Javascript-ready element {{{
        $('#elementID').get()
        $('#elementID').get(0)
        $('#elementID')[0]

        https://stackoverflow.com/questions/19340097/converting-between-a-jquery-object-and-a-pure-js-object
          What is the difference between the object returned by $('#elementID') and the object
          returned by document.getElementById('elementID')?

          $('#elementID') returns an object with a ton of functions that all operate on the result
          of document.getElementById('elementID'). Think of the jQuery object like a giant robot
          that document.getElementById('elementID') is sitting inside of.

          You can access the wrapped DOM object with:

          $('#elementID').get() $('#elementID').get(0) $('#elementID')[0] If there's more than one
          element matched by the selector, you can access, for example, the second element with
          $elements.get(1) or $elements[1].

          Moreover, how can you easily convert from one to the other?

          To wrap an object with jQuery's convenience functions, just pass it into the $ function:

          $(document.getElementById('foo')) $(document.querySelectorAll('.foo:not(.bar)')) To go the
          other way, use .get() or the bracket notation.

          In your specific example, you don't need to do anything special because this is actually
          a normal DOM object. This is why you often see callbacks littered with $(this).

      }}}

      debugger {{{
        If you ever want to stop things in process like you can in ruby with binding.pry, simply put
        the word

          `debugger;`

        in the file

      }}}

      Event loop {{{
        Philip Roberts JSConf EU
        https://www.youtube.com/watch?v=8aGhZQkoFbQ&t=0s

      }}}

      Hoisting {{{
        In the order of how things are interpretted, declarations come before expressions, both for
        variables and functions. This is an important thing to be aware of because it has
        implications for what is actually availabe/known to javascript and what is not.

        For example, say we have something like this:
          console.log(x(argument));
          var x = function(parameter) {...}
          console.log(x(argument));

        Initially, Javascript will only "know" about the declaration of 'var x', which means the
        first attempt at calling the x(argument) will come up as undefined.
          NOTE: It's important to distinguish here that 'xyz is undefined' is a special term for
          Javascript. Technically, through "var x = whatever", x has been defined. But what
          Javascript is concerned with here is execution order. Javascript did go through the code
          in advance, and it "knows" that x was defined, so it goes ahead and allocated memory for
          whatever it will be set to. HOWEVER! When it then goes through and reads the code "in
          order from top to bottom" during the execution phase, what is it saying is something like,
          "Ahh, we see you've referenced the x variable. We know you have set this up with an
          appropriate 'def x = ...' thing, but at this stage, the code has not yet set it's value.
          So we're going to go ahead and say that, at this time, 'x is undefined'. Now, once we've
          passed that point in reading through the code sequentially, that's when the value of
          x will become available to you."

          Does that make sense? The key point is that this is different from the compiling error,
          where UNDEFINED is returned as one of those hard and fast error messages. That is an
          instance where, after gathering all of the declarations and functions, Javascript comes
          back and says, "Hey woa! We just went through this whole thing and never saw where you
          declared var x. So we're gonna throw out the grand-daddy UNDEFINED error message to you."

          So the full-stop red x's undefined error message is different from the "xyz is undefined"
          notice.

          In practice, what this means is that, even though varible declarations get "hoisted", that
          does not mean their values are immediately available. This is why, even with hoisting, it
          is smart to put your declarations at the physically at the top of your code so that their
          values are available when javascript goes through things from top to bottom in sequential
          order.

        Now, compare that to something like
          console.log(x(argument));
          function x(parameter) {...}
          console.log(x(argument));

        Because there is not a separate declaration being made, then entire function itself gets
        hoisted to the top, making it available to BOTH x(argument) calls

      }}}

      IIFE Immediatedly Invoked Function Expressions {{{
        Think of this as kind of a way of setting up an anonymous function that is automatically
        called up and brought into the fray without having to call it directly anywhere within the
        code. This is useful for cordoning off a codebase so that it's not available in the global
        scope

      }}}

    Image galleries and lightboxes  {{{
      https://github.com/kenwheeler/slick/
      https://github.com/sachinchoolur/lightGallery
      https://github.com/staaky/strip
      https://github.com/trentrichardson/jQuery-Rebox
      https://github.com/fengyuanchen/viewerjs

    }}}

    Modals  {{{
      https://github.com/VodkaBears/Remodal/releases/tag/1.1.1
      https://css-tricks.com/considerations-styling-modal/

    }}}

      Async, Promises, and MicroTask Queue {{{
        https://jakearchibald.com/2015/tasks-microtasks-queues-and-schedules/
        https://frontendmasters.com/courses/javascript-new-hard-parts/promises-microtask-queue/

        NOTE: These are both EXCELLENT articles that closely mirror what you learned in the
        Codesmith class with Shane

        https://medium.com/javascript-in-plain-english/asynchronous-javascript-explained-a4c1133f5544
        https://medium.com/javascript-in-plain-english/javasscript-promises-and-the-micro-task-queue-6111f7452f05

      }}}

      Scope is variable availability {{{
        One of the best analogies I've heard for this in Javascript (yes, this concept tends to be
        different for other languages) is that when a parent buys a box of cookies and puts them in
        the pantry, all of his children have access to the cookies. However, whenever a child saves
        up and buys himself a box of cookies, there no way jose that kid is letting his parents have
        any of his cookies.

      }}}

      toFixed(n) {{{
        This ends up being a very interesting topic. Because while this does accurately round to 'n'
        decimal places, in order to preserve the accuracy, the result is formatted as a string.
        Which means in order to "use" the returned value, you'll likely need to turn it back into
        a number with something like parseInt().

        In my reading on this, some advocated for the use of Math.round(), but the issue there is
        that Math.round() seeks to round to the nearest integer and does NOT allow the user to
        specify a desired number of decimal places. To counteract this, then, some use an inbetween
        multiplication step followed re-divising the answer to go back to a certain number of
        decimals. The problem with this is that it involves a lot of computations. Regardless, if
        you ever want to go that route, here's the best working function I've been able to find for
        that:

          https://stackoverflow.com/a/42368487/5474201
          function round(value, decimals) {
            return Number(Math.round(value+'e'+decimals)+'e-'+decimals);
          }
          round(1.005, 2); // 1.01

      }}}

      Naming constants {{{
        Typically use CAPS when naming a constant variable that is established prior to the code
        being executed.

        However, for a constant variable that is assigned during runtime, typically those are named
        regularly (ie as leading lowercase snakecase)

      }}}

      prompt() {{{
        Really cool way to read in something from the user.
        Example:

          const button = document.querySelector('button');

          button.onclick = function() {
            let name = prompt('What is your name?');
            alert('Hello ' + name + ', nice to see you!');
          }

      }}}

      template literals aka template strings {{{
        https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals

        let name = "Stephanie";
        let output = `Hi, how are you doing ${ name }`?;

        It even goes one step further and respects any linebreaks in the source code, without having
        to add in any pesky escape sequences. So the following is output on multiple lines:

          let output = `This is on the first line.
          This is on the second line.`

      }}}

    }}}

    Testing {{{
      https://github.com/testing-library/jest-dom
      https://github.com/testing-library/dom-testing-library

      Jasmine
      Jest
        Of the more fully-fledged, bells-and-whistles category, Jest sounds like it's the easiest to
        setup and get started with. Over something like, say, Moch or Jasmine.
      Mocha
      Tape
        https://github.com/substack/tape

        $ yarn add tape --save-dev

        A few people I read tout the simplicity and straightforwardness of Tape as being the GOTO
        testing solution. It may not be as flashy or popular as Mocha, etal, but it more than gets
        the job done. Its seemingly lack of the cool-kid bells and whistles is part of what makes it
        so rock-steady dependable and easy to use.

    }}}

    Troubleshooting  {{{
      Commenting a js.erb file can be complicated… You can use the standard js // HOWEVER this does
      not stop the code being sent to the client, it only stops it from being executed. Which means
      that any embedded <%= %> ruby type stuff will still get processed through (??). I don't
      know... Not completely clear on that yet but it's a start :)

    }}}

    ---- Duckett Javascript and JQuery {{{
      A prototype is similar to a parent object. If the object receives a request for a property it
      does not have, its prototype is checked. Then, that prototype's prototype is checked, etc etc
      on and on until eventually Object.prototype is reached. Object.prototype is the top dog in
      Javascript (similar to how Object is the top dog in Ruby iirc...)

      When you need to collect the text content of an element, use textContent, NOT innerText. This
      is because innerText is not a standard practice adopted by all browsers so it will prove to be
      unreliable in practice. Also, innerText takes into account CSS layout changes, which means if
      something is hidden per CSS rules, it will "skip" those node elements, which again can make it
      unreliable. Since textContent does NOT make accomodations to CSS rule sets, it's easier to
      step into various node elements knowing it will fire each and every time, regardless of the
      CSS.  See Duckett p 216 Using innerHTML to edit page contents can pose serious security risks
      if you don't prepare for them properly.

      It seems to all come down to being very cautious (read: PARANOID!) about all data input by the
      user. This means that if you take in user input, do NOT turn around and use that same input to
      execute code without whitewashing it first. You need to strip it of certain code symbology and
      things like that so it results in just plain text, otherwise it could contain commands and
      code snippets that run without your knowledge or intent.  See Duckett 228

      Modifying attributes
        1) FIND the element node that you want to work with with some sort of DOM query.
        2) Then you use a particular method to affect the content in some way. These could be:
          getAttribute()
          hasAttribute()
          setAttribute()
          removeAttribute()

      DOM has four types of nodes: document nodes, element nodes, attribute nodes, and text nodes.

      You can select element nodes by their id or class attributes, by tag name, or using CSS
      selector syntax

      Whenever a DOM query can return more than one node, it returns a NodeList which functions like
      an array

      From an element node, you can access and update its content using properties such as
      textContent and innerHTML or DOM manipulation

      An element node can contain multiple text nodes and child elements that are siblings of each
      other

      Since the DOM can be somewhat inconsistent between browsers, this is why jQuery became popular

      How events trigger
        1) Select the element node, typically through some sort of DOM query
        2) Indicate which event on the selected nodes will trigger the response, aka
          binding
        3) State the code you want to run when the even occurs, can be a named or
          anonymous function

        ex: var element = document.getElementById('username');
            element.onblur = checkUserName;

      You can also use event listeners, which are a more recent approach with some more bells and
      whistles that are not supported in older browsers.

        Element.addEventListener('event', functionName, Boolean );
          ex: element.addEventListener('blur', checkUserName, false);

      The boolean attribute is for something known as capture, usually set to false. This is used to
      set what order you want the event to travel through the DOM hierarchy, known as Event Flow.
      Event Flow comes in two flavors, bubbling (inside out, from specific to general) or capturing
      (outside in, from general to specific).

        False = bubbling
        True = capturing

      See Duckett p 260
      To use parameters with event listener functions (like in the case below, you add the length
      5 to the checkUserName function, you have to wrap the function like this:

          elementUserName.addEventListener('blur', function() {
            checkUserName(5);
          }, false);"

      If you've got a lot of javascript event listeners on a page, use event delegation to minimize
      how many event listeners you have at one time. From what I gather so far, this involves
      referencing parent elements as "catch alls" rather than targetting each individual element
      itself. See page 266

      Events {{{
        There are several different specifications floating around out there, each bringing its own
        set of capabilities and stuff. At the time of the text, there were W3C DOM events, HTML5
        events, and Browser Object Model (BOM) events.

        User Interface Events
          Usually attach to the browser window (ie window.addEventListener(...))
          load, unload, error, resize, scroll, etc

        Focus and Blur events
          Usually attach to an HTML element
          focus, blur, focusin, focusout

        Mouse Events
          Typically all elements on a page support mouse events, and all mouse events bubble
          click, dblclick, mousedown, mouseup, mouseover, mouseout, mousemove, etc

          Something to keep in mind when using mouse events is user interaction. Is there a chance
          the user may be interacting with the element through the keyboard? If so, consider using
          a focus event instead as that would capture both mouse and keyboard inputs

          Where mouse events occur
            screenX screenY - cursor position within entire screen monitor

            pageX pageY     - cursor position within entire page, which may or may not be within the
            current viewport

            clientX clientY - cursor position within browser's viewport

            A key aspect to keep in mind is that it's entirely possible (common?) for the page and
            client cursor positions to be different from one another

        Keyboard Events
          input, keydown, keypress, keyup

        Form Events
          submit: fires on the node representing the <form> element

          change: fires when status of form elements change. Could be a selection from a dropdown
          select box, radio button being selected, checkbox being selected, etc

          input: commonly used with <input> and <textarea> elements

          The change event is usually prefereable to a click event when dealing with forms because
          the user may choose to interact with his keyboard rather than his mouse

        Mutation Events and Observers {{{
          Whenever elements are added to or removed from the DOM, its structure changes. This change
          triggers a mutation event

          DOMNodeInserted
          DOMNodeRemoved
          DOMSubtreeModified
          DOMNodeInsertedIntoDocument
          DOMNodeRemovedFromDocument

          You didn't end up using it, but here's an excerpt from how you implemented a MutObs into
          land_app
            {{{
            const callback = function(mutationList, observer) {
              const filesElement = 'uppy-Dashboard-files'
              const fileItemElement = 'uppy-DashboardItem'

              for(let mutation of mutationList) {
                if (mutation.type === 'childList') {
                  if (mutation.addedNodes.length) {
                    let addedNodes = mutation.addedNodes;

                    for (let entry of addedNodes.entries()) {
                      entry.forEach(function(item) {
                        if ($(item).length) {
                          if (item.classList.length) {
                            if (item.classList.contains(filesElement) || item.classList.contains(fileItemElement)) {
                              <whatever function you want to have executed at this point>
                            }
                          }
                        }
                      });
                    }
                  } else if (mutation.removedNodes.length) {
                    let removedNodes = mutation.removedNodes;

                    for (let entry of removedNodes.entries()) {
                      entry.forEach(function(item) {
                        if ($(item).length) {
                          if (item.classList.length) {
                            if (item.classList.contains(filesElement) || item.classList.contains(fileItemElement)) {
                              <whatever function you want to have executed at this point>
                            }
                          }
                        }
                      });
                    }
                  }
                }
              }
            };

            const observer = new MutationObserver(callback)

            let node = $("#uppy-form")[0];
            let config = {
              attributes: false,
              characterData: false,
              childList: true,
              subtree: true,
            };

            observer.observe(node, config)

          }}}

        }}}

        HTML5 Events
          This is being added to and improved all the time

          DOMContentLoaded: Can be attached to window or document objects

          hashchange: fires when the URL hash changes, works on window object, works with oldURL and
          newURL properties

          beforeunload: can be used for things like warning the user that changes on a form have not
          been saved if they try to navigate away without saving

      }}}

      The jQuery chapter notes are in the jQuery section
}}}

    ---- Hartl's Learn Enough Javascript to Be Dangerous {{{
      $ node # this fires up Node.js in the command prompt and gives direct console access
      $ node <filename.js>
        executes the js file within node, outputs results to terminal
      From what I can tell so far, using node is aking to firing up the ruby console to fact-check
      your reasoning, try out some ideas, as well as to help debug why something may not be working.
      aka REPL

      let firstName = "Blake"
        the use of 'let' is a relatively new thing with ES6 syntax. It acts very much like 'var'
        except that 'let' is ALWAYS confined to a local scope and is never allowed to be a global
        variable. This is a bit different from how 'var' works, if you remember. Because 'var' was
        a bit more dynamic in that it could conform to local or global depending on where it was
        declared within the codebase. That said, you'll likely run into the use of 'var' often

        https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/let

        `This displays the firstName: "${firstName}" through interpolation, similar to Ruby`
          Note the use of backticks here, not single quotes. This is limited to ES6 and beyond.

      Also, just from messing around with it, it doesn't seem to process things very well... For
      example, if I were to use the interpolation syntax in an effort to have it display the result
      within the string (ie `This is an ${t.example();}`), it seems to return the output rather than
      the executed result. But perhaps I'm using it incorrectly on my end?
        UPDATE: Yes, it seems that in addition to enclosing the full enchilada in backticks, the
        interpolated ${} portion also must be surrounded by quotes. I corrected the above example to
        reflect this...
        UPDATE 2: Also! I had to correct some of my syntax in order to call the functions properly.
        For example, instead of object.reverse, I had to use object.reverse(). JS is really picky
        about having a literal use of parens. This is different from Ruby, which is able to
        interpret things without them most of the time.

      Triple equals is an equality check both of the value and the type, and is what is generally
      recommended for use. Why?
        > "1" == 1
        true
          The two values compared with == are considered equal, since the object types are not taken
          into consideration

        > "1" === 1
        false
          Here, compared with ===, the two are shown to be unequal, which is likely more in line
          with what you'd expect

      Certain methods like .sort() and .reverse() mutate the object they act upon, which means they
      permanently change them as a result of their operation. So for JS, these are Ruby equivalents
      of BANG ! methods

      Objects in Javascript can be thought of as a collection of properties, stored in a hash-like
      structure.
        let user = {}; # this creates a new object named "user"
        user["firstName"] = "Blake";
          notice how the interaction closely resembles that of a hash structure with key/value
          pairs. Interesting!

      Something to watch out for though, when it comes to extensively using/relying on JS objects,
      is that they do have their weaknesses, such as slower performance and limited support for
      extracting keys and values. As a response to these and other issues with objects, JS also has
      a Map object, with get and set methods.

      Hartl: "I find it helps to not pronouce 'function' (whether aloud or in your head), so that it
      sounds like 'array: for each element <do something>.'"

        array.forEach(function(element) {
          console.log(element);
        });

      => syntax is a way to shorten the function calls, think it might be an ES6 thing...
        For example, these two statements are equivalent:

          return states.map(function(state) { return urlify(state) });
          return states.map(state => urlify(state));

        See how it compacts the statement and removes the need to type out the function() and return
        stuff?

      Hartl's Big Three Javascript Abilities!
        MAP
          Apply, or aka "map", a function over an array of elements, returns results in a new array
            From what I can tell, this functions nearly identically to how .map is handled in Ruby
          // From lejs2bd/functional.js
          states.map(state => urlArray.push(`${urlPrefix}` + urlify(state)));

        FILTER
          Filter a dataset based on certain criteria, returns results in a new array
          // From lejs2bd/functional.js
          states.filter(state => state.split(/\s+/).length === 1);

        REDUCE
          So this one is a bit of a misnomer... By reduce, think "process". Reduce is a function
          that iterates through an array, PROCESSES some sort of operation on each element, and
          accumulates the result in a variable/object.

          const array = [1, 2, 3, 4];
          const accumulator = (accumulator, currentValue) => accumulator + currentValue;

          array.reduce(accumulator)
            => 10

          The starting value defaults to zero, so the second argument can be left off if that's what
          you want. Otherwise

            array.reduce(accumulator, 5)
              => 15 ie 5 + 1 + 2 + 3 + 4

          Another way to approach it could be something like...

            array.reduce((previous, current) => previous + current, 0);

      $ npm test
        should any notes related to testing js, since they involve npm, be moved to the npm section?

      Javascript's handling of REGEX is a bit different from Ruby. This site will come in handy when
      trying out ideas

        https://regex101.com

      !! the use of double bang in front of an object/variable effectively calls NOT NOT, which
      "forces" it to return either true or false. For example, a single ! might return a falsey
      value (like 0, null, or undefined).

        https://stackoverflow.com/a/784946/5474201

      Browsers don't support the use of `require` to bring in extra files. To get around this, Hartl
      recommends an npm module called 'browserify'

        $ npm install --global browserify
        $ browserify <desired_js_file.js> -o bundle.js
          $ browserify main.js -o bundle.js
            example from Hartl lejs2bd ss 9.1
        The resulting bundle.js is a file you can then source into your webpage with
          <script src="bundle.js"></script>
      NOTE: In order to use this appropriately, the bundle.js must be re-browserified with the above
      command each time one of the "sourced" .js files is modified.

      Hartl recommends using .querySelector over .getElementById. Says .qS is more versatile and
      more powerful.

      the `event` object is a parameter that's automatically available through Javascript

      $ npm install --global fs
        FileSystem module that Hartl used in ss 10 to read the contents of a file Has some really
        neat capabilities including writing streams of data to a file with the use of
        createWriteStream and writeStream

      $ npm install --global request
        scrape the contents of a webpage

    }}}

    ---- Raushmayer, A Speaking Javascript (2014) {{{
      Parameters are used to define a function
        function foo(param1, param2)
      Arguments are used to invoke a function
        foo(arg1, arg2)

    }}}

  }}}

  JQuery {{{
    Resources {{{
      https://jquery.com
      http://www.unheap.com
      http://keycode.info

    }}}

    Plugins{{{
      File Uploads
        https://github.com/blueimp/jQuery-File-Upload {{{

        }}}

      Image cropping
        https://fengyuanchen.github.io/jquery-cropper/  {{{

        }}}

      Image Galleries
        https://github.com/blueimp/Gallery  {{{
        https://github.com/dimsemenov/Magnific-Popup
        https://github.com/jackmoore/colorbox
        http://kenwheeler.github.io/slick/
        https://github.com/sachinchoolur/lightGallery
          NOTE: Evidently it is possible to orient the thumbnail strip vertically along the side
          https://sachinchoolur.github.io/lightslider/examples.html
          NOTE: How to possibly add your own custom sidebar elements
          https://stackoverflow.com/questions/35076206/lightgallery-showing-local-comment-box-html-instead-of-fb-comment-box
        http://nanogallery.brisbois.fr/
        http://www.jqueryshare.net/light-box/fancybox/
          https://github.com/fancyapps/fancyBox
          http://michaelsoriano.com/create-an-awesome-photo-gallery-with-fancybox-and-timthumb/
        https://github.com/fengyuanchen/jquery-viewer

        }}}

      Lazy-Loading
        https://github.com/dinbror/blazy  {{{
        https://github.com/tuupola/jquery_lazyload

        }}}

      Animation related
        https://github.com/jquery/jquery-color  {{{
          recommended in Duckett's text, allows you to animate between two colors

      }}}

    }}}

    Duckett Javascript and jQuery {{{
      At the outset, Duckett says that jQuery is most powerful when it is appropriately combined
      with Javascript, not when it's used as a replacement for Javascript. jQuery's strength is in
      how it selects elements, perform tasks, and handle events

      Find elements using CSS-style selectors

      The selectors return a list of sorts, with each matching node element given it's own 0-index
      position.

      A jQuery object stores references to elements, it does NOT create copies of them. For this
      reason, since everything is being managed in memory, it is better to reuse active jQuery
      objects rather than repeat selection processes, etc. This can be achieved by storing jQuery
      objects in variables

        It is customary to prefix jQuery related object variables with a $

          $listItems = $('li');

      You can chain jQuery methods together in one huge string, make the law-of-delimiter have
      a heart attack :)

        $('li[id!="one"]').hide().delay(500).fadeIn(1400);

        Or, to make things a bit more legible, you can stack them like'a so'a

        $('li[id!="one"]')
          .hide()
          .delay(500)
          .fadeIn(1400);

        The only catch is that chaining doesn't work on everything. Most update methods work okay.
        But anything involving retreival from the DOM cannot be chained. Also, if one method in the
        chain fails, the remaining methods fail too.

      Checking that a page is ready to work with
        .load()
          Use this if you require assets to first be fully loaded. Like to get dimensions of an
          image, for example.
        .ready()
          Does NOT wait for all assets to be loaded, it fires as sooon as the DOM has loaded.

        It's worth nothing that a shorthand style for writing $(document).ready() is simply
          $(function() {
            // script code here
          }

      Getting element content
        .html()
          retreives HTML inside the first element in the matched set, along with any of its
          descendants. For example

            $('ul').html(); returns...
              <li id="one"><em>fresh</em> figs</li>
              <li id="two"><em>pine nuts</em> figs</li>
              <li id="three"><em>honey</em> figs</li>

            $('li').html(); returns...
              <em>fresh</em> figs

            See how that works? If you want to get ALL of the li elements, you can use .each() which
            comes up later

        .text()
          Returns only the content from the element selection, along with text from every
          descendant. For example

            $('ul').text(); returns...
              fresh figs
              pine nuts
              honey

            $('li').text(); returns...
              fresh figspine nutshoney (Note the lack of any spacing since it's only grabbing the
              contents). To get the content from <input> or <textarea> elements, use the .val()
              method, which comes up later

      Updating element contents
        NOTE! Both the .html and .replaceWith methods need to be properly escaped and sanitized
        before use, same as how you would protect against using innerHTML()

        .html()
        .text()
        .replaceWith()
        .remove()

        And with these functions, you can pass in a string, a variable, or a function.

      Inserting new elements
        1. Create the new elements in a jQuery object
        2. Use a method to insert the content into the page

        .before()
        a.prepend(b)   => adds b to a
        a.prependTo(b) => adds a to b
        a.append(b)    => adds b to a
        a.appendTo(b)  => adds a to b
        .after()

        Here's where each of these methods target
          BEFORE <li> PREPEND item APPEND </li> AFTER

      Getting and setting attribute values
        .attr() # Can get or set attribute values
          $('li#one').attr('id'); # gets id attribute 'hot'
          $('li#one').attr('id', 'hot'); # sets id attribute to hot
        .removeAttr()
        .addClass()
        .removeClass()

      Getting and setting CSS properties
        NOTE: Is it generally better to change the value of a class attribute rather than to change
        the css property itself from within javascript, but I'll include it here for reference

        Use comma separated to set a single property's value
        Use object notation to set multiple property values
          ({ prop:1 value,
              prop2: value })

      Iterating through each element in a selection of multiple elements
        NOTE: When deciding between this or $(this), it all comes down to what type of object you're
        trying to create and/or interact with.

          If you're wanting to refer to a pure Javascript related object, use this
          If you're wanting to refer to a jQuery object and use jQuery related methods, use $(this)

        .each()
          The thing to take away here is that although it may look like you're just dealing with one
          element, when you use .each(), the included function is actually ran on every single
          element within the selection. So in these examples, every list item is receiving the
          append

          $('li').each(function() {
            var ids = this.id;
            $(this).append(' <em class="order">' + ids + '</em>');
          });

      Event Methods
        Ahh, the tried and true workhorse :)
        Some of the more popular jQuery event methods (but there are a LOT more!):
          UI       : focus, blur, change
          Keyboard : input, keydown, keyup, keypress
          Mouse    : click, dblclick, mouseup, mousedown, mouseover, mousemove, mouseout, hover
          Form     : submit, select, change
          Document : ready, load, unload
          Browser  : error, resize, scroll

        .on(<jQuery event listener>[, selector][, data], function() {
          ...
        })
          NOTE: The bracketed items listed above are optional arguments that can be passed in. The
          selector allows you to apply a filter to further refine the selection, while the data
          allows you pass in additional information to the event object for use by the function for
          it work with when the event is triggered. Object literal notation can be used in for the
          [, data] argument if you want more than one data item to be passed in. I'm not sure if
          object literal notation can be used for the [, selector] as well, but I'll cross that
          river if/when I need that functionality. Examples of these are shown a bit further down.

          For example, to add an on click event listener to every li element...
            $('li').on('click', function() {
              $(this).addClass('complete');
            });

          One of the cool things is you can list more than one event as part of a single .on call.
          For example, the function runs whenever the user either mouses over list item or clicks on
          a list item as shown below.

          NOTE: Recall that $(function() is shorthand for $(document).ready()

            $(function() {
              var ids = '';
              var $listItems = $('li');

              $listItems.on('mouseover click', function() {
                ids = this.id;
                $listItems.children('span').remove();
                $(this).append(' <span class="priority">' + ids + '</span>');
              });
            });

      The Event Object
        Every event handling function receives an event object, which has methods and properties
        related to the event that occurred. This is commonly referred to as simply 'e'

          $('li').on('click' function(e) {
            eventType = e.type;
          });

        Property
          type      : type of event (eg click, mouseover, etc)
          which     : button or key that was pressed
          data      : an object literal containing extra information passed to the function when the
                      event fires
          target    : DOM element that initiated the event
          pageX     : mouse position from left edge of viewport
          pageY     : mouse position from top of viewport
          timeStamp : number of milliseconds from 1/1/1970 to when the event was triggered
                      (ie Unix Time)
            NOTE: From what I've seen, its common to run .toDateString() on this object so its
            displayed in a more human-readable form

        Method
          .preventDefault()  : prevents the default action from taking place (eg POST action from
                               clicking submit)
          .stopPropagation() : stops event bubbling up to its ancestors

      Effects
        NOTE: At the time of the book, he even notes that CSS3 techniques are often faster than
        their jQuery counterparts when it comes to animation and effects. Makes me wonder what
        further advancements have been made since then. Maybe jQuery's performance has improved?
        Maybe there's now something beyond even CSS3?

        Also, remember that when it comes to hiding and showing elements, keep in mind that these
        effects can cause surrounding elements to move around a bit as they either "make room" or
        "fill in room" when something is shown or hidden.

        Further, you need to make sure your event ordering makes sense from the computer's
        standpoint. For example, let's say you want something to fade into view when the user visits
        the page. Well, because the HTML code is first taken in as a whole, before you can fade the
        element INTO view, you'll first have to hide it since the "default" HTML response is to show
        everything. This isn't as bad as it sounds though thanks to method chaining. For example

          $('h2').hide().slideDown();
          var $li = $('li');
          $li.hide().each(function(index) {
            $(this).delay(700 * index).fadeIn(700);
          });

        Do you see how things are first hidden so they can subsequently fade into view? Very cool!

        Basic
          .show()
          .hide()
          .toggle()

        Fading
          .fadeIn()
          .fadeOut()
          .fadeTo()
          .fadeToggle()

        Sliding
          .slideUp()
          .slideDown()
          .slideToggle()

        Custom
          .delay()
          .stop()
          .animate()

      Animating CSS Properties
        You can animate any CSS value whose property can be represented as a number (ie height,
        width, font-size), but not those which use a string value such as a font-family.

        CSS properties themselves are written in camelCase and specified using object literal
        notation

        .animate({
          // Styles you want to change
        }[, speed][, easing][, complete]);

        Complete is used to call a function that should run when the animation has finished. It's
        essentially a callback function for the animation

      Traversing the DOM
        The selector as mentioned below references the argument that is passed in with the method.
        Think of it as a sort of filter to narrow down what is being acted upon.

        For example, if you start with a selection that contains one list item, you could create
        a new selection containing the other elements from the list using the .siblings() method. If
        you added a selector into the method like .siblings('.important') then it would find only
        siblings with a class attribute whose value included important

        Selector required
          .find()    = all elements within current selection that match selector
          .closest() = nearest ancestor (not just parent) that matches selector

        Selector optional
          .parent()
          .parents()
          .children()
          .siblings()
          .next()     = next sibling of current element
          .nextAll()  = all subsequent siblings of current element
          .prev()     = previous sibling
          .prevAll()  = all previous siblings

      Filter methods with .add()
        NOTE: When deciding between the use of .not() or :not() for example, the :not() will perform
        faster because jQuery is not required to get involved in the middle ground.

        .filter()     = finds elements in matched that in turn match a second selector
        .find()       = finds descendants of elements in matched set that match the selector
        .not()/:not() = finds elements that do not match the selector
        .has()/:has() = finds elements from the matched set that have a descendant that matches the
                        selector
        .contains()   = selects all elements that contain the text specified

        The following are equivalent
          $('li').not('.hot').addClass('cool');
          $('li:not(.hot)').addClass('cool');

        .is() = checks whether current selection matches a condition, returns a BOOLEAN. Typically
                used in if/thens

          var $listItems = $('li');
          $listItems.filter('.hot:last').removeClass('hot');
          $('li:not(.hot)').addClass('cool');
          $listItems.has('em').addClass('complete');

          $listItems.each(function() {
            var $this = $(this);
            if ($this.is('.hot')) {
              $this.prepend('Priority item: ');
            }
          });

      Finding items by order
        .eq() = element equal to index number
        .lt() = element less than index number
        .gt() = element greater than index number

      Selecting form elements
        NOTE: As said elsewhere, it's important to note that using jQuery selectors is not as fast
        as relying on CSS selectors since jQuery has to essentially act as a middleman. The main
        advantage of jQuery, however, lies in how it appropriately handles any browser
        inconsistencies that may arise.

        :button
        :checkbox = better performance with $('[type="checkbox"]')
        :checked
        :disabled
        :enabled
        :focus = better performance with $(document.activeElement)
        :file = all elements that are file inputs
        :image = better performance with [type="image"]
        :input = all <button> <input> <select> and <textarea> elements.
          Better performance with .filter(":input")
        :password = better performance with $('input:password')
        :radio = to select a group of radio buttons, refer to the name
          eg $('input[name="gender"]:radio')
        :reset
        :selected = better performance with .filter(":selected")
        :submit = better performance with [type="submit"]
        :text = likely better performance with ('input:text')

        .val() = get value of the first element in a matched set, or update value in all of them
        .filter() = provides for a way to use a second selector to further narrow a selection
        .is() = described above
        .isNumeric() = returns boolean, checks whether supplied value is a number or not
          $.isNumeric(1)

      Cutting and copying
        Cut
          .remove() = removes matched elements in DOM tree, does NOT preserve them in memory
          .detach() = removes matched elements in DOM tree, DOES preserve them in memory, usually to
                      be pasted elsewhere
          .empty()  = removes child nodes and descendants from any elements in matched set
          .unwrap() = removes parents of matched set, leaving matched elements
        Copy
          .clone()  = creates a copy of matched set, including any descendants and text nodes

      Box dimensions
        .height()          = box - margin - border - padding
        .width()           = box - margin - border - padding
        .innerHeight()     = box + padding
        .innerWidth()      = box + padding
        .outerHeight()     = box + padding + border
        .outerWidth()      = box + padding + border
        .outerHeight(true) = box + padding + border + margin
        .outerWidth(true)  = box + padding + border + margin

      Window and page dimensions
        .height()     = height of jQuery selection
        .width()      = width of jQuery selection
        .scrollLeft() = horizontal scrollbar position for the first element in the jQuery selection,
          or sets the horizontal scrollbar position for matched nodes
        .scrollTop()  = vertical scrollbar position for the first element in the jQuery selection,
          or sets the vertical scrollbar position for matched nodes

      Position of elements on the page
        .offset() = coordinates of element relative to top-left corner of the document object
        .position() = coordinates of element relative to any ancestor that has been taken out of the
          normal flow (ie using CSS box offsets). If no ancestor is out of normal flow, it will
          return/act the same as .offset()

        NOTE: These various dimension settings and readings can be used to know where a user is
        seeing on a page. The example he gives here is that once the user has scrolled with 500
        pixels of the bottom of the document, a small advertisement slides in from the right along
        the right hand side of the page. See Ch 7 Position for example

    }}}

    ---- Notes {{{
      $.fn.newFunction = function{...} # This allows you to extend jQuery with your own functions. Kind of like you're baking
      a new function into the set of functions available to all jQuery objects. So this allows you to do something like...
        $('#element').newFunction()

      $('<selector>').length
        This is a way to check and see if a given selector exists on the page. With Turbolinks,
        since it aggregates everything into one giant js file, this can be a way of checking that
        the user is on a certain page before attempting to run certain js code

      $.trim()
        removes only leading and trailing whitespace

      $.each() is different from $(<element>).each.
        The former allows you to iterate through jQuery objects themselves, whereas the latter only
        lets you iterate through the selected element collection. $.each() came in handy for me
        whenever I was passing in one or more crops into a function. Because if I tried to use
        $().each, if I only passed in a single crop element, it would hit an exception since .each
        is only available to collections, not to single elements. To get around this, I set it up
        with $.each() which allowed it to iterate through one or more crops without a hitch. Very
        cool!

      .stop() various uses, with respect to animation sequencing and "checking"
        https://css-tricks.com/examples/jQueryStop/

      .filter() reduces the set of already matched elements
      .find() looks for descendants of the matched element

      https://stackoverflow.com/questions/950087/how-do-i-include-a-javascript-file-in-another-javascript-file
      $.getScript("<scriptname>.js", function() {
        alert("Script loaded but not necessarily executed.");
      }

      console.log relatd stuff {{{
        Computed property names
          console.log({ <object or variable> })

        console.table([array])

        console.trace

      }}}

      event delegation {{{
        https://www.youtube.com/watch?v=gXLgfwC6lXQ
        https://www.youtube.com/watch?v=0zybq9BE1hg
        https://api.jquery.com/on/
        https://www.thecreativedev.com/event-delegation-jquery/
        http://jqfundamentals.com/chapter/events

        The idea here is to move the event handler further up the DOM (usually to the parent), and
        let that element be responsible for initiating the event on a child element. This way, newly
        added children can "get" the same functionality as those present on initial load.
        Performance can be markedly improved as well, especially in those situations were a LOT of
        individual handlers would have otherwise been created without delegation

          $(<ancestor_selector>).on(<event>, <child_selector>, function() {
            ... code here
          })

            Example:
              $("#parent-element-id").on("click", ".child-element-class", function() {

              })

        Something interesting here is that not all jquery "custom" events translate over to the
        .on() method. For example, .hover(), when translated to .on() is not .on('hover'...), but
        .on('mouseenter'...) and a separate .on('mouseleave'...). The jquery hover method is simply
        a shorthand for mouseenter and mouseleave. This comes into play when you're need to set up
        delegation for a 'hover-like' event.

        Also, in several situations, you've also had to add an .off() to the handler to prevent it
        from "duplicating" after each use. .off().on('click'...)

          UPDATE1: A shorter version of this is to simply use .one(), this accomplishes the same
            thing as the .off .on combo
          UPDATE2: Okay! So evidently it's not the same thing! At least not for how I've been using
            in as part of the land_app.js file. Because using .one() throughout causes a lot of
            'repeat' buttons to not work on their subsequent load-ins.

      }}}

      waiting for one function to end before progressing {{{
        If you ever want to make sure one function is done before moving on to the next action, one
        way is to set up a new function with the logic you want to check for completion, and then
        include that function in a $.when .done setup. This is something I used to confirm a css
        value was changed before continuing.

        let increaseZIndex = function() {
          $("#bottom-panel").css('zIndex', '6');
        };

        $.when( increaseZIndex() ).done(function() {
          addInteractionBarrier();
        })

        What's REALLY cool here is that you can 'stack' multiple items into a single $.when().done()
        So if you need to wait for a few things to finish before moving on to the next thing, simply
        include them in the .when like this

            $.when(
              $activeHighlight.removeClass('active'),
              $nextHighlight.addClass('active'),
              $nextHighlight.fadeIn(250),
              scrollToActiveHighlightConfirmationSlide(0),
            ).done(
              lazyloadHighlightConfirmationHighlights.load(highlightImage[0])
            );

        Other approaches include using callbacks or promises
          https://www.impressivewebs.com/callback-functions-javascript/
          https://www.learn-js.org/en/Callbacks
          https://api.jquery.com/promise/
          https://maori.geek.nz/jquery-promises-and-deferreds-i-promise-this-will-be-short-d10275f82717
          http://blog.blackninjadojo.com/javascript/2019/02/27/using-jquery-promises-and-deferreds.html

      }}}

      form submit events {{{
        Okay so this was a really interesting thing I ran into with Rails form submissions. For some
        reason, the .submit event ajax stuff quit executing immediately after a new crop was
        submitted by the user. The respond_to format block kept using the .html redirect_to rather
        than looking for the *.js file. After searching for a day and a half, I stumbled across
        a stack-overflow response about a tangentially related question wherein the user talked
        about how there is a difference between a form submission that occurs due to a literal
        interaction with the submit input element on the page vs one that is 'faked' or
        'called-into-being' within the codebase. When it comes to Rails, the `remote: true` form
        option is only taken into account on input element interaction form submittals and NOT on
        code-base triggered form submittals.

        Now, while that is interesting, what led me to re-enabling jquery_ujs was merely the fact
        that he mentioned he was developing a Rails 6 app with it enabled. "Aww, what the heck?
        Lemme try re-enabling it on my end to see if it does the trick..." And viola! It worked!

        I honestly don't know why this resolved the issue, or for that matter, whether this was
        the actual issue at play within my code... Because my obvious question is, "Well wait a sec.
        If the problem was with jquery_ujs, why the heck didn't this show up in the commit
        differences?" It's all very odd.

        BUT! I got it working again, which at this point in time, is good enough :) I'll file this
        securely under Hartl's advice of being an expert: "There are many times where you don't
        gotta know HOW something works in order to put it to effective good use! And that's okay :)"

      }}}

    }}}

    ---- Troubleshooting {{{
      If you're having a hard time getting jQuery to run, make sure you have
      app/assets/javascripts/application.js setup to include:

          //= require jquery = require jquery_ujs = require jquery-ui/effects/effect-blind
            This is an extra one, provides some cool inline highlighting effects.
            Note that the actual path name has changed a few times already so if it all the sudden
            stops taking, check to see if it's been update again

        You'll also need to be sure your gemfile includes
        gem 'jquery-rails'
        gem 'jquery-ui-rails'"

      If you're wanting to cherry-pick what jquery pieces get included in your app (ie for example,
      you want one or two of the jquery effects, but not all of them), here's how you go about it.

        1. First off, it is strongly recommended that, regardless of what you choose to include in
           your app, that you actually download and install the jquery package as a whole. This
           helps with keeping things properly versioned and compatible with one another
        2. In your application.js file, you must name the jquery elements in line with their storage
           directories. In the case of Rails, since jQuery is likely included by way of a gem, this
           means going to the following file to check how the elements are actually named:

            /home/linux/.rvm/gems/ruby-2.4.1@land_app/gems/jquery-ui-rails-6.0.1/app/assets/javascripts

    }}}

  }}}

  React JS  {{{
    NOTE: Your setup instructions for Rails are in the Rails Webpacker section
    NOTE: Notes on Redux area also included as a subsection of ReactJS

    Since you first started learning about React there have several framework type thingies released
    built upon React that add some new features and make certain parts of React more accessible and
    stuff like that. One of those is Next.js, which you've created a subsection for below

    Learning Resources  {{{
      https://reactjs.org
      http://www.reactiflux.com/learning
      https://www.airpair.com/reactjs/posts/reactjs-a-guide-for-rails-developers
      https://hackhands.com/react-rails-tutorial/
      https://learnetto.com/users/hrishio/courses/the-free-react-on-rails-5-course
        Hello World
          https://codepen.io/learnetto/pen/rWPVaG?editors=1010
        Props
          https://codepen.io/learnetto/pen/PbVqPY?editors=1010
        State
          https://codepen.io/learnetto/pen/XNObdV?editors=1010

      https://blog.andrewray.me/reactjs-for-stupid-people/
      https://mattboldt.com/react-rails-form-refactor/
      https://www.webascender.com/blog/rails-react-forms-validations-real-time-updates/
      https://tylermcginnis.com/reactjs-tutorial-a-comprehensive-guide-to-building-apps-with-react/
      https://github.com/EbookFoundation/free-programming-books/blob/master/free-programming-books.md#react
      https://scotch.io/tutorials/learning-react-getting-started-and-concepts
      https://camjackson.net/post/9-things-every-reactjs-beginner-should-know
      https://tylermcginnis.com/react-aha-moments/
      https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0
      https://hackernoon.com/upgrade-your-react-ui-with-state-machines-30d1298e90be
      https://github.com/adam-golab/react-developer-roadmap

    React-Router
      https://github.com/ReactTraining/react-router

    The difference between three of the moving parts of React: Components, Elements, Instances
      https://reactjs.org/blog/2015/12/18/react-components-elements-and-instances.html
      https://medium.com/@dan_abramov/react-components-elements-and-instances-90800811f8ca
      https://toddmotto.com/react-create-class-versus-component/

    }}}

    React Component Resources  {{{
      https://github.com/brillout/awesome-react-components
      https://devarchy.com/react
      https://react.rocks
      http://www.reactjsx.com
      http://react-toolbox.io
      https://github.com/markerikson/react-redux-links
      https://github.com/enaqx/awesome-react

      Image cropping
        https://github.com/DominicTobias/react-image-crop

      Image gallery
        https://www.npmjs.com/package/react-photo-gallery

      Credit Cards
        https://github.com/amarofashion/react-credit-cards

      Components recommended by various resources
        $ npm install prop-types --save
        $ npm install query-string --save
        $ npm install axios --save
        $ npm install react-router-dom --save

    }}}

    React Notes  {{{
      These will be pretty scatterbrained for a while, just wanting a place to dump all of my tid
      bits, will recompile and make things more coherent once I've grown my understanding some more
      :)

      At the end of the day, all React is doing is generating Javascript. Nothing more, nothing
      less.

      Two main things you're always going to be considering when it comes to React:
        What do I want my UI to be like?
        What kind of state is needed?

        If you get these right, everything else really falls into place. Let all of the nitty gritty
        details about the DOM and VirtualDOM take care of itself. Push all that behind the curtain
        and focus on UI and State.

      {{  }}
        Double curlies are a way to declare an inline object literal. What the hell does that mean?
        lol Well, check this out:

          <span dangerouslySetInnerHTML = {{ __html: rawMarkup }} />

        This is the same thing as...

          let obj = { __html: rawMarkup };
          <span dangerouslySetInnerHTML = { obj } />

        Notice how, with the double curlies, you're able to take care of the whole shindig in one
        fell swoop without having to bother to "temporarily" setting up a the obj variable in
        advance.

      <li key={xyz} ... />
        jsx can take in a key attribute for any of its return statements. Ideally, this is set to
        some sort of unique identifier that is part of the returned record and it serves as a way
        for React to properly differentiate the items that it's working with. In the even that the
        record does NOT have a unique identifier baked into itself, you can use something like its
        index position but this can be problematic... Because as items are manipulated, their
        position in the index will likely change, which can fubar the keys that jsx is using.

        "Keys only need to be unique among sibling elements in the same array. They don't need to be
        unique across the whole application or even a single component."

        This also means that if you need to use a random number for any reason, it needs to be
        established outside of the key in order for it to remain consistent during it's lifetime.
        Otherwise, each and every call to the component will result in a new key being given to the
        element.

      jsx
        Only certain portions of a jsx file must strictly be written in jsx syntax. Typically, jsx
        takes over whenever you are in a return() section. The remaining spaces accept plain old
        Javascript, which means you can use all of your JS prowess there.

      React Element
        Contains THREE properties
          1) Type of HTML element to use, or custom-made element
          2) props aka properties, specify the attributes of the element. Some attributes are
             modified a bit here since some names are already taken by JS. For example, className is
             used instead of class; htmlFor in place of for, style strings are instead camelCased
             object names, etc etc
          3) children - where the content of the element should be included The format for
             React.create Element sponges everything in the 3+ argument position as children. So the
             end-result looks something like this: React.createElement( <TYPE>, <PROPS>, <child>,
             <child>, <child...>)

      React Components
        are functions that accept the props of a React Element. Whether defined as part of the
        element or not, the element always accepts a children:... entry accordingly.

        Make your components more reusable by passing as much data as you can through props.

        There are three main things that you work with when it comes to a component
          1. State
          2. Lifecyle events
          3. UI with the render() method

        Controlled vs Uncontrolled Components
          "React has two different approaches to dealing with form inputs.

          "An input form element whose value is controlled by React is called a controlled
          component. When a user enters data into a controlled component a change event handler is
          triggered and your code decides whether the input is valid (by re-rendering with the
          updated value). If you do not re-render then the form element will remain unchanged.

          "An uncontrolled component works like form elements do outside of React. When a user
          inputs data into a form field (an input box, dropdown, etc) the updated information is
          reflected without React needing to do anything. However, this also means that you can’t
          force the field to have a certain value.

          "In most cases, you should use controlled components."
          https://reactjs.org/docs/glossary.html

      React Elements
        describes what you want to see on the screen. React elements are immutable. Typically,
        elements are not directly used, but instead, get returned from components.

      Props
        inputs to a react component. Data passed down from a parent component to a child component.

        Props are read-only and should not be modified in any way. If you need to modify some value
        in response to user input, or whatever, use state instead.

      State
        managed by the component itself. Only components defined as classes can have state. Unlike
        props, state CAN change.

        "For each particular piece of changing data, there should be just one component that 'owns'
        it in its state. Don't try to sync states of two different components. Instead, 'lift it up'
        to their closest shared ancestor, and pass it down as props to both of them."

      While typical programming functions may take in arguments to return a resulting VALUE, React
      components are essentially functions that take in arguments to return a resulting VIEW (ie
      a UI element).

      pure functions are functions that
        always return the same result given the same arguments
        does not depend on the state of the application
        do not modify the variables outside of their scope

        In this way, a simple add(x, y) function is considered a pure function.
        Javascript's slice method is considered to be a pure function
        Javascript's splice method, however, is NOT considered to be a pure function because each
        time it is ran, it modifies the underlying array that is being acted upon. Which means
        running it multiple times has the potential to return different results, even if the same
        arguments are passed in with it.

        All this is to point out that at its core, React's render method NEEDS to be a pure function
        in order to be consistent and reliable. This also means that React plays best the more you
        yourself can conform to writing pure functions while implementing functional programming.
        This is true for pretty much all of programming too, so the thought-process gains made here
        will likely transfer directly over to how you write/think for Ruby or Python, etc etc.

      PropTypes
        $ npm install prop-types --save
        This npm package for React allows you to set the type of object you're building, and then
        warn you if what is being passed in does not match the state type. This helps developers and
        yourself keep track what data type you expect to work with in your components, etc. For
        example, array.map is all good as long as you're dealing with an array. But what if you
        accidentally passed in a string of "Dog, Bear, Cat"? It would break your program because in
        JS/React, a string does not respond to the .map method.

        Some notes on some of PropTypes's quirks...
          PropTypes.func should be used in place of PropTypes.function
          PropTypes.bool should be used in place of PropTypes.boolean

        Another cool thing about PropTypes is that you can pretty much drill down to as much or as
        little specificity that you want. You can say it must be an array. You can say it must be an
        arrayOf(objects). You can say it must be an arrayOf(objects) with a given "shape", which
        lets you set up how the key/value pairs should look. So you can say the objects need to have
        a name: string, friend: boolean.

        It also lets you set up whether something isRequired or not.

        So, at first blush, I see it as being a customizable error-reporting tool as well as an
        ongoing documentation tool to help keep things in line.

      Stateless functional components
        When you get used to working with React components, there's a good chance you'll start to
        find that you're creating a lot of components that do nothing more than render the
        information that's provided to them in a certain way. To put it differently, all they do is
        return some JSX code that React instinctively knows to do with it from there.

        So, if all the React component does is render and return JSX, what that means is that there
        may be no reason to bring in all the extra baggage and abilities that come from sublassing
        a new React component! This means the "component" may be able to be rewritten as a simply
        javascript function.

        Here's an example...
          class HelloWorld extends React.Component {
            render () {
              return (
                <div>Hello {this.props.name} </div>
              )
            }
          }
          ReactDOM.render(<HelloWorld name='Tyler' />, document.getElementById('app'))

        The above works just fine! But do you notice what it does? All it does is make a call to
        render and then return a bunch of jsx stuff. If that's all it does, then why does it even
        need all the extra trappings and abilities that come with declaring it as a subclassed
        React.Component? Spoiler: It doesn't need to!

        Which means it can be rewritten as "just" a normal javascript function, like so:
          function HelloWorld (props) {
            return (
              <div>Hello {props.name}</div>
            )
          }
          ReactDOM.render(<HelloWorld name='Tyler' />, document.getElementById('app'))

        Setting things up this way allows you to clearly separate out components that do nothing
        more than PRESENT information. Tyler calls these presentational components, as compared to
        all of the other components you can make.

        One side benefit of using stateless functional components is that they do not contain any
        state, which means there is no way to call/use .this anywhere. This can be a good thing
        because it removes the possibility for it to be used in different contexts and possibly
        fubar whatever result you were actually expecting. By taking out .this, the function starts
        to act more like a tried-and-true, ever-faithful, always-know-what-yer-gonna-get function
        like .add(num1, num2). And in this case, that's a very good thing to have :)

      Life Cycle Events
        There are a lot of different life cycle methods available for use, with some covering 95% of
        use-cases.

        On the face of it, these remind me very much of the before and after callbacks used in
        Rails.

        2 categories of life cycle methods:
          1. When a component gets mounted or unmounted to the DOM.
          2. When a component receives new data.

        What kind of things might we want to have happen whenever a component is mounted or
        unmounted?

          - establish some default props for the component
            .defaultProps =

          - set an initial state in the component
            constructor (props) {
              ...
            }

          - make an ajax request to fetch some data that the component needs
            componentDidMount() {
              ...
            }

          - set up any listeners (ie websockets or firebase listeners)
            componentDidMount() {
              ...
            }

          - remove any listeners (usually after the component is unmounted)
            componentWillUnmount() {
              ...
            }

          - receive new data from its parent component
            getDerivedStateFromProps(nextProps, prevState) {

            }

      AJAX Requests
        axios and promises

      Turbolinks
        Add to your JS related stuff is 'turbolinks:load'. Here are some examples:

          $(document).on('turbolinks:load', function() {
            console.log("It works on each visit!");
          });

          document.addEventListener('turbolinks:load', function() {
            var element = document.getElementById("pay-type-component");
            ReactDOM.render(<PayTypeSelector />, element);
          });

          (coffeescript example)
          $(document).on "turbolinks:load", ->
            alert "page has loaded!"


    }}}

    Redux Notes   {{{
      https://egghead.io/lessons/react-redux-the-single-immutable-state-tree

    }}}

    Next.js {{{
      https://www.freecodecamp.org/news/the-next-js-handbook/

    }}}

  }}}

  Solid JS {{{
    Really freaking cool! Just watching the main intro vid, I can say that I already like it much
    more than something like React. Plus, it is MUCH more performant!

  }}}

  StimulusJS {{{
    This sounds kind of like a small way to give some additional controls/behaviors to JS in a Rails
    project. Designed by the Basecamp team, so that's why I'm assuming it has somewhat of a Rails
    bent to it... It is **NOT** meant to be a huge framework solution like Vue or React, that's for
    sure!

    https://stimulusjs.org

    https://medium.com/better-programming/how-to-add-stimulus-js-to-a-rails-6-application-4201837785f9

  }}}

  Surface  {{{
    NOTE: This is a CSS based framework that does not rely on any javascript
    https://mildrenben.github.io/surface/

  }}}

  VueJS {{{
    NOTE: HIGHLY recommended that you also checkout Nuxt.js, it is intended to be run ON TOP of vue,
    improving the experience and adding more features and organization, etc.

      https://nuxtjs.org

    With that out of the way, back to Vue...

    Install and Create {{{
      $ pnpm install --global vue
      $ pnpm install --global @vue/cli

      And then, onto actually creating an app

        $ vue create <directory_name>

      It will ask you a few questions here. If you ever want to change the packagemanager from what
      you selected, you can edit the default config through ~/.vuerc file

        <cd into the created directory>

        $ pnpm run serve

      As an aside, you can also manage the vue stuff through a GUI browser interface. To go that
      route, use

        $ vue ui

      One of the cool things about this is it gives you a really cool analytics type view of your
      running server instance

    }}}

    v-cloak {{{
      a sort of "temporary" flag that you can use to target with CSS as a way to "wait" for Vue to
      finish bringing everything onto the page before showing them. Otherwise, there's a chance the
      user will see a brief flicker of "raw" content just before Vue goes in and works its magic to
      replace everything. How v-cloak works is, if you include in a Vue related attribute (ie <div
      v-cloak>), Vue will NOT render anything with the v-cloak tag. The catch is that once Vue has
      finished pulling everything in and prepping stuff, it removes any and all instances of v-cloak
      tags. So if you were to do something in your css like

        <style>
          [v-cloak] {
              display: none;
          }
        </style>

    }}}

    data object vs methods vs computed properties {{{
                       | Readable? | Writeable? | Takes Args? | Computed? | Cached?
                       ------------------------------------------------------------
      data object      | Yes       | Yes        | No          | No        | n/a
      Methods          | Yes       | Yes        | Yes         | No        | No
      Computed Props   | Yes       | Yes        | No          | Yes       | Yes

    }}}

    Props can pass data DOWN from the parent to the child
    Emits can pass data UP from the child to the parent

    When binding a data object to a child component with v-model, it forces the child component to
    "receive" it by referencing a prop called 'value' and emitting an 'input' event to send the
    updates back to the parent component. At some point in 2.x, this was updated to allow the user
    to define a custom prop 'name' and 'event'. This is done through using the 'model' object
    configuration. The drawback? It only allows for one v-model prop to be shared between parent and
    child. This is supposedly something they've expanded upon in 3.x, it allows multiple data
    objects to be passed in as props to child components

  }}}

}}}

JSON {{{
  JSON Web Token (JWT) {{{
    https://jwt.io

    Composed of three parts, concatenated together with a period (.)
      1. header - contains validity date of the token
      2. payload - may contain any data
      3. signature - verifies the token has been encrypted by the application

    Ends up looking something like
      eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0I
      joxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c

  }}}

}}}

julia {{{
  UPDATE: So unfortunately, it sounds like the Toshiba laptop's CPU is too old to support Julia.
  Because other people that ran into a similar error did not meat the minimum system requirements.

  installation {{{
    As outlined in:
      https://ferrolho.github.io/blog/2019-01-26/how-to-install-julia-on-ubuntu

    $ cd ~/Downloads
    $ wget https://julialang-s3.julialang.org/bin/linux/x64/1.4/julia-1.4.2-linux-x86_64.tar.gz
    $ tar -xvzf julia-1.4.2-linux-x86_64.tar.gz
    $ sudo cp -r julia-1.4.2 /opt/
    $ sudo ln -s /opt/julia-1.4.2/bin/julia /usr/local/bin/julia

    Once completed, you need to completely close out of the terminal and re-open

  }}}

}}}

Kakoune {{{
  https://delapouite.github.io/kakoune-explain/keys.html
  https://github.com/mawww/kakoune
  https://github.com/mawww/kakoune/blob/master/contrib/TRAMPOLINE

  Initial installs {{{
    $ sudo apt install kakoune
    $ kak # start

    Or, if you want a potentially more recent version, you can build it from the github repo

      $ sudo apt install libncursesw5-dev pkg-config
      $ git clone https://github.com/mawww/kakoune.git && cd kakoune/src
      $ make
      $ PREFIX=$HOME/.local make install

  }}}

}}}

keyboards {{{
  From my reading thusfar, two of the most-wanted features you want in a "true" ergonomic keyboard
  are split + tent

  Manuform
    https://geekhack.org/index.php?topic=46015.0

  Let's Split
    https://github.com/nicinabox/lets-split-guide

  Dactyl
    https://github.com/adereth/dactyl-keyboard

  Redox
    https://github.com/mattdibi/redox-keyboard

  Moonlander Mark 1
    https://zsa.io/moonlander

    Seems to be an improvement on the already well reviewed ErgoDox EZ

  Keyboardio Model 01
    https://keyboard.io

  Curated lists
    https://github.com/BenRoe/awesome-mechanical-keyboard
    https://github.com/help-14/mechanical-keyboard
    https://keebfol.io

}}}

Linux   {{{
  $ which <program>
  $ whereis <program>
  $ command -v <program>

  abduco  {{{
    Terminal session management tool commonly used alongside something like dvtm. If setup properly,
    from what I can tell, it might allow you to save window layouts/etc similar to tmux's
    tmux-resurrect plugin

    https://github.com/martanne/abduco

  }}}

  Analyzing, diagnosing, and maybe even improving boot times {{{
    NOTE: Many of these diagnostic commands have been added to the `diagnose_boot` alias in your .bashrc {{{
      $ systemd-analyze
      $ systemd-analyze blame
      $ systemd-analyze time
      $ systemd-analyze critical-chain > ~/criticalchain.txt
      $ systemd-analyze plot > ~/plot.svg

      $ dmesg > ~/dmesg.txt
      $ dmesg | less

      $ sudo systemctl list-unit-files --state=enabled > ~/unitfilesenabled.txt
        NOTE: You can also use this same systemctl tool to enable/disable/start/stop services

      $ journalctl -b > ~/journalctl.txt

    }}}

    $ sudo vim /etc/systemd/system.conf
    Update and uncomment the following lines to read:
      DefaultTimeoutStartSec=5sec
      DefaultTimeoutStopSec=5sec
    $ sudo update-initramfs -u

    $ sudo vim /etc/default/grub
    Update the following string so that it includes 'noresume', for example:
      GRUB_CMDLINE_LINUX_DEFAULT="quiet splash noresume"
    $ sudo update-grub

  }}}

  Antivirus  {{{
    NOTE: Now, before you rush off to get setup with a virus scan, you need to realize that because
    of how linux handles its files and stuff, many people do NOT feel the need to regularly run
    virus scans, if at all! Another thing to be aware of is that because the virus program either
    isolates or removes files from the system, it can inadvertently render vital programs inoperable
    without any recourse for fixing them. For this reason, if you feel the need to run a scan, be
    sure you do set it up so it does NOT remove files, but instead outputs the results to a report.

    Then, after reviewing the report, you can decide whether or not you want any of the listed files
    removed.

    LXLE comes with something called "Penguin Pills", not sure if that relates to any of the below

      $ sudo apt install clamav clamdscan clamtk # clamtk is the optional GUI interface

      $ sudo freshclam
        updates the definition list from the terminal. Now, a note on this. Whenever you initially
        install freshclam, it likely runs the update process automatically. Which means that your
        attempt to manually update the definitions may give you an error about the freshclam.log
        being used in another process. To stop the process and reconfigure freshclam, use the
        following:

          $ sudo dpkg-reconfigure clamav-freshclam

    $ clamscan --help
      lists all the scan types

    $ sudo clamscan --max-filesize=3999M --max-scansize=3999M --exclude-dir=/sys/* -i -r /
     This will check nearly all of the files on the computer and only report back warnings and
     potential infections.

  }}}

  autostart programs  {{{
    File manager to /usr/share/applications
      -> r-click "Copy" -> navigate to ~/.config/autostart -> r-click "Paste"

    Might also be done with the GUI through
      Preferences -> Default Applications -> Autostart
      but it's hit or miss for me as to what programs are available to be selected.

    $ update-rc.d
      This is a command you can use to disable programs from starting

  }}}

bash {{{
  ~/.bash_profile should source ~/.bashrc
    https://askubuntu.com/a/121075
    http://mywiki.wooledge.org/DotFiles

  $ shopt login_shell

}}}

  bat {{{
    cat, on steroids

    NOTE: modify the below for whatever version you're trying to install
    $ cd ~/Downloads
    $ curl -LO https://github.com/sharkdp/bat/releases/download/v0.17.1/bat_0.17.1_amd64.deb
    $ sudo dpkg -i bat_0.17.1_amd64.deb

  }}}

  Blue light reduction software  {{{
    flux {{{
      $ sudo add-apt-repository ppa:nathan-renniewaldock/flux
        UPDATE: Do NOT go this route, try to get it another way, maybe through git?
      $ sudo apt update
      $ sudo apt install fluxgui

    }}}

    redshift {{{
      https://github.com/jonls/redshift

      $ sudo apt install redshift && sudo apt install redshift-gtk

    }}}

}}}

  Boot errors {{{
    System hangs on boot OR logging in to a black screen with cursor {{{
      First, for the system hang... {{{
        On the recurring black screen hangs on boot I may have found a solution. First off, you
        gotta boot up the OS with a command called nomodeset. To get there, after you’ve chosen your
        boot device, you’ll then see the GRUB loader menu where it lets you pick which ubuntu you
        want to use. You hit e or c (whichever one brings up the command line menu) and you'll see
        this full page output. Toward the bottom, look for a line containing something about "quiet
        splash". Right after quiet splash add the command "nomodeset" (no quotes). This essentially
        disables the graphics card from running on boot, which may prevent some of the problems
        you're having so you can get to the package manager, etc.

        Then, once the system is up, open a terminal and type the following commands
          sudo apt update
          sudo apt install fglrx-amdcccle-updates
          sudo aticonfig --initial

        UPDATE per 17.10+... So the plot thickens... Evidently AMD is not so good at maintaining
        support for its older graphics cards and, in addition, the fglrx pakckage falls out of
        repair. MEANING you may have to go about this a different way.

          $ unbuntu-drivers devices
            This will show current packages available for your system. Search and install them with
            the package manager

        Restart that bad boy and HOPEFULLY! you’ll be greeted with a smooth load-in, video card and
        all!

        Now, in practice this hasn't done so much and the external drive has repeatedly encountered
        issues. So, for the next bit, we'll turn to the infamous fsck stuff

      }}}

      Now, for the black screen with cursor {{{
        UPDATE 2: On VirtualBox, I noticed that whenever this would happen, simply going out and
        back into full screen fixed the problem (ie right-ctrl + c then right-ctrl + f)

        UPDATE 1: Now, this did seem to fix the problem. On the laptop, however, this eventually
        seemed to cause problems with the ATI Radeon drivers or something, because there came
        a point when the system would no longer boot with the correct resolution and would not allow
        me to select the correct resolution. Removing the `nomodeset` configuration as you've
        outlined below did restore the system to its proper resolution.

        You resolve this one much the same way as the system hang, but I wanted to share the more
        permanent solution. In this case, I found the VM starting up just fine, letting me login and
        all that jazz, but after login, I had nothing but a black screen with a visible cursor.

        To fix this, I booted into recovery mode. This allowed me to get to the desktop, albeit
        without any video card drivers up and running (similar to safemode on windows). From there,
        I went into the grub file itself and directly changed its contents.

          $ sudo nano /etc/default/grub

          Edit the file's contents similar to the system hang by adding `nomodeset`. I also prefer
          to remove `quiet` and `splash` to allow for a more text oriented startup view

            GRUB_CMDLINE_LINUX_DEFAULT="... nomodeset"

          Save the file and exit nano (may look something like ^x y <cr>)

          $ sudo update-grub

          <now reboot the system and you should be good to go!>

      }}}

    }}}

    fsck and initramfs {{{
      In most cases, when the system fails to boot and you're faced with an initramfs prompt, the
      following command will do the trick:

        $ fsck -y /dev/sdb2

      In the even that sdb2 is not the right drive to target, give find out the correct identifier
      with:

        $ lsblk
        $ ls -l /dev/disk/by-id
        $ fsck -l

      Now, the remainder of this section is a more indepth look, one of the options may come in
      handy if the above never fixes the issue.

      NOTE: In order for fsck to do it's thang, you've gotta be in an environment that allows it to
      work on an unmounted drive. More often than not, this means booting linux from a live CD or
      a thumb drive and running the commands in the terminal

      UPDATE: I have found that whenever the drive boots in maintenance mode, I'm able to $ umount
      /dev/sdb3 (which is usually the drive with the issue), then run fsck on that drive. It will
      normally fix any present problems and allow a clean boot.

      NOTE: Though I haven't used it yet, there's supposedly a really great tool for this kind of
      thing, search for "smartmontools" with synaptic

      $ lsblk
      $ ls -l /dev/disk/by-id
      $ fsck -l # will show partition paths and device paths/names
      $ umount <drive_path>
        This can also be done in the GUI menu, Pref > Disks
      $ sudo fsck -y /dev/<drive_num> # ie "$ sudo fsck -y /dev/sdd3"
        Once it's complete, just type `$ exit` and the system will quit fsck and proceed to boot as
        normal
      $ sudo e2fsck -fy
        Not sure what the difference is here, but I saw some mention it. Maybe be worth looking into
        if the "regular" fsck isn't getting the job done
      $ sudo dd if=/dev/<drive_name> of=/dev/null count=1
        performs a thorough check of the disk for read write errors and bad blocks, etc

      If you want to run this manually with your VirtualBox VM, you can. Just "insert" the ISO into
      the VM machine before you start it, through the VirtualBox settings. Now, what happened to me
      when I did this was that the screen was all garbled and messed up. The fix? CTRL+ALT+F1, then
      ALT+F7

      NOTE: If it continues to give issues, then the only thing that has caught my eye on this is
      that sometimes device encryption can make things a bit unstable and unreliable. SO! If you
      want to go the extra mile and remove the encryption you activated in your initial install,
      then keep a'reading...

      If push comes to shove and everything you try doesn't seem to work, another way to approach
      this issue is to dump your system info and post it somewhere like www.askubuntu.com for help
      on what may be causing the problem. Here's the sequnce of terminal commands you can use to do
      this:

        $ lspci -nnvv
        $ mount
        $ dmesg
        $ dpkg -l

        https://askubuntu.com/a/741113

      Another thing that pops up from time to time is that whenever you run your `$ update` alias,
      you may see a message along the lines of "cryptsetup: WARNING: target cryptswap1 has a random
      key, skipped"
      This message is usually okay, and has to do with how your swap space is setup. To doublecheck,
      run

        $ swapon -s

      If it says soomething like `Filename /dev/dm-0 Type partition Size xyz Used xyz Priority
      xyz` then you're good to go and can safely ignore it.

    }}}

    REISUO/REISUB greatness {{{
      I've been continuously plagued by faulty bootups. The fix has so far been to run fsck on the
      affected drive like this

        $ fsck -y /dev/sdb2
          replace sdb2 with whatever drive you want to check

      After reading about it, it seems like this is most commonly caused by an improper shutdown
      procedure. But as far as I can tell, I'm doing everything I can to shut down the system
      properly. So what gives?

        ALT+SysRq, then while holding those keys down, slowly type out
          r e i s u b to restart
          r e i s u o to shutdown

        Here's what each of the keypresses does
          r: Switch the keyboard from raw mode to XLATE mode
          e: Send the SIGTERM signal to all processes except init
          i: Send the SIGKILL signal to all processes except init
          s: Sync all mounted filesystems
          u: Remount all mounted filesystems in read-only mode
          b/o: Immediately reboot/shutdown the system, without unmounting partitions or syncing

    }}}

  }}}

  Boot messages about microcode being out of date or something?  {{{
    sudo apt install intel-microcode
    sudo apt install iucode-tool

  }}}

  Changing admin user name  {{{
    https://askubuntu.com/questions/34074/how-do-i-change-my-username#34075
    If'n you're needing to change the admin's username, give this a whirl:
      $ sudo usermod -l <new_name> <old_name>
    Then, rename the home directory to match with:
      $ sudo usermod -d /home/<new_home_directory> -m <new_name>
    Now, there is a chance this gives you a headache since you're logged in as the same user you're
    trying to change. To get around this, create a temporary profile with sudo rights, login as that
    user, edit the "primary" account, log back into the main account, then delete the temporary
    account:

        $ sudo adduser temporary
        $ sudo adduser temporary sudo
        log out, log back in as the temporary account, work your magic, then log back in as the
          "new" primary account
        $ sudo deluser temporary
        $ sudo rm -r /home/temporary

        NOTE: If this doesn't work, you can also remove the user with the System Tools > Users and
        Groups tool. It will also prompt to remove the related directory. Very cool!

    OR! If it still complains about the account being in use by a process...
      # logout
      CTRL+ALT+F1
      $ sudo su - # this will make you true reboot
      # If you need to kill a process use
      $ ps -u <username> --OR-- $ ps -a
        then look for the matching PID
        NOTE: Another way to do something like this is through bash's `jobs` commands, though at
        this point in time I don't know any of the specifics. I saw somewhere that `$ jobs -p`
        should work, but it doesn't output anything unless you're in the same terminal/bash instance
        that the process is backgrounded in. Whereas the `$ ps -a` command lists everything from all
        terminals.
      $ kill <PID-number>

  }}}

  Clipboards {{{
    Linux and VIM use different sets of clipboards, which makes moving copied text around a pain in
    the katoosh. Luckily, there's a utility that serves to combine them together, though it does
    take a little elbow grease to set up properly between the environments you regularly work in.
    It's called xclip

      $ sudo apt install xclip

    Then, for tmux, you'd add the following to your .tmux.conf file (already there if you pull from
    the repo):

      bind ^C run "tmux save-buffer - | xclip -i -sel clipboard"
      bind ^V run "tmux set-buffer \"$(xclip -o -sel clipboard)\"; tmux paste-buffer"

  }}}

  Commandline {{{
    https://www.youtube.com/playlist?list=PLT98CRl2KxKHaKA9-4_I38sLzK134p4GJ
    https://www.explainshell.com

    cheat.sh {{{
      https://github.com/chubin/cheat.sh

      $ curl cht.sh
        # displays an overview of most commands. Very cool!

      $ curl cht.sh/<command>

      $ curl cht.sh/<language>
      $ curl cht.sh/<language>/:list
      $ curl cht.sh/<language>/:learn

      If you'd rather get away from the curl command directly, you can use a client like interface

      $ sudo apt install rlwrap
      $ curl https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh
      $ sudo chmod +x /usr/local/bin/cht.sh

      Now, you can call it directly and use spaces instead of +'s

        $ cht.sh go reverse a list

      Or even enter into a completely separate shell mode

        $ cht.sh --shell
        cht.sh> go reverse a list

      Or even a shell mode with a specific context

        $ cht.sh --shell go
        cht.sh/go> reverse a list

      The possibilities are endless :)

    }}}

    tealdeer {{{
      https://github.com/dbrgn/tealdeer
      NOTE: This is a rust implementation (read: faster!) of a popular CLI tool called "tldr"

      $ cargo install tealdeer
      $ tldr -h

    }}}

    thefuck
      https://github.com/nvbn/thefuck

      $ pip3 install thefuck --user

    trash-cli {{{
      https://github.com/andreafrancia/trash-cli

      $ pip install trash-cli
        $ sudo mkdir --parent /.Trash
        $ sudo chmod a+rw /.Trash
        $ sudo chmod +t /.Trash

      $ trash-put <file>
      $ trash-list
        $ trash-list | grep foo
      $ trash-restore
      $ trash-empty
        $ trash-empty <number of days>
      $ trash-rm \<file regex>
        $ trash-rm \*.mp3

    }}}

    navi {{{
      https://github.com/denisidoro/navi

      $ cargo install navi
        # NOTE: Trying to update this threw up some errors related to a thing called
        # "clap". One workaround is to install with the `--locked` flag as follows
        #
        #   $ cargo install --locked navi

    }}}

    useful commands {{{
      ls -al               # list contents, options a hidden l long R nested
        NOTE: alias `ls` to `$ ls -la`
      mkdir -p <dir>/<dir> # creates parent and child directories, add -v option for confirmation
      touch                # creates a file
      mv                   # move
      cp                   # copy
      rm                   # remove, option -R for dir and contents
      sort                 # just what it sounds like
      grep                 # search
      echo                 # write to standard output
      cat                  # read and concatenate files
      less                 # read one page a time
      >                    # redirect output to a file
      >>                   # redirect and append output to a file
      |                    # pipe output to another command

      And one of the interestings things here is these can be chained together a bit
        $ cat books.txt | grep Mil
        $ cat books.txt | sort > sorted_books.txt

      ^R
        This is a handy way to pull up a history of your most recently used commands

      stty sane
        If you ever find that the terminal isn't responding/showing your text inputs, try typing
        this command

      reset
        this does something similar as the above, though I've only tried it when using the pry rails
        terminal, but maybe it's a universal command?

      There came a time whenever I was working in vim that it would not let me create new files or
      directories in any way, either through netrw commands or even to initialize new .git
      repositories on new directories. Something that helped me regain permissions for the directory
      I was working in was to effectively transfer ownership of the "problematic" directory back to
      the "linux" user:

        https://www.linode.com/docs/tools-reference/linux-users-and-groups/
        $ sudo chown -R linux:linux ~/Programming/Tuts/js_jq_ifewd

      Another way I approached it was to back out of tmux altogether and launch it with
        `$ sudo tmux` and that gave me root authority throughout

      low power states
        $ sudo systemctl suspend
          NOTE: aliased as `suspend`
        $ sudo systemctl hibernate
          NOTE: aliased as `hibernate`
        $ sudo pm-suspend
        $ sudo pm-hibernate
        $ pmi action suspend
        $ pmi action hibernate
          $ sudo apt install powermanagement-interface

    }}}

  }}}

  cron jobs {{{
    These are script like things you can schedule to be run by the system

    https://crontab.guru

  }}}

  curl {{{
    https://curl.haxx.se/docs/manual.html
    https://linux.die.net/man/1/curl
    https://www.lifewire.com/curl-definition-2184508

    $ curl --help
    $ curl --manual

    $ curl [options] [URL...]

    Some of the options include:
      --create-dirs
        used in conjunction with the -o option
      --form, -F
        emulate filled-in form data in which a user has pressed the submit button. For example, here
        is the 'password' form field being filled-in with '/etc/passwd' to the site
        www.mypasswords.com

        $ curl -F password=@/etc/passwd www.mypasswords.com

      --get, -G
        specified data is used in a GET request, appended to the URL with a '?' separator

      --libcurn <file>
        append to any curl command line and you will receive as output C source code that uses
        libcurl, written to the file that does the equivalent of what your command-line operation
        does.

      --output, -o
        write output to <file> instead of stdout. If you are using {} or [] to fetch multiple
        documents, you can use '#' followed by a number in the <file> specifier. That variable will
        be replaced with the current string for the URL being fetched. For example:

          $ curl http://{one,two}.site.com -o "file_#1.txt"

        This can be combined with --create-dirs to create the local directories dynamically,
        matching them up with what is output/generated here. Very cool!

      --remote-name, -O
        write output to a local file named like the remote file we get

      --upload-file, -T
        transfers the specified local file to the remote URL.

          $ curl -T "{file1,file2}" http://www.uploadtothissite.com
          $ curl -T "img[1-1000].png" ftp://ftp.picturemania.com/upload/

      --trace <file>
        enables full trace dump of all incoming and outgoing data to the given output file. Use "-"
        as the filename to have the output sent directly to sdtout

      --trace-ascii <file>
        same as above, but only shows the ASCII part of the dump, which can generally be more
        human-readable

      --write-out, -w
        defines what to display on stdout after a completed and successful operation

      --progress-bar, -#
        display progress info as progress bar rather than the default statistics

  }}}

  data analysis {{{
    visidata {{{
      $ sudo apt install visidata

    }}}

  }}}

  debian packages .deb {{{
    Install with

      $ sudo dpkg -i <filename>.deb

  }}}

  Desktop Environments {{{
    Now this is an interesting thing about Linux. I'm not sure I understand it 100% correctly, but
    here's what I can make of it... Linux "separates" the desktop interface schtuff from the
    distros. So you can run a debian based distro and then, through that pipeline/delivery system,
    install a bunch of different desktop flavors/styles etc each complete with their own default
    program setups and stuff. It's really interesting, TONS of variety!

    So, most likely, you're running Lubuntu, which comes packaged with a default desktop environment
    called LXDE. BUT! Get this... You can install other desktop environments, like Unity or Gnome
    and use those too in conjunction with your Lubuntu install. Very cool!

      LXDE    {{{
        Even more stripped down and lightweight than standard Lubuntu, comes with a Lubuntu install
        - Transparency
        $ sudo apt install lxde

      }}}

      Xubuntu {{{
        When you want a little more pizazz than what Lubuntu offers, but still as lightweight as
        possible
        + Transparency
        $ sudo apt install xubuntu-desktop
          Settings Manager -> Window Manager -> Keyboard # To set keybindings for resizing windows

      }}}

      XFCE    {{{
        Same as LXDE is to Lubuntu, it's a more stripped down version of Xubuntu. Terminal appears
        blurry though...
        - Transparency

        $ sudo apt install xfce4

      }}}

}}}

  disabling internal webcam and microphone  {{{
    $ lsmod
      Displays a list of kernel drivers. Identify which drivers control the devices, then add to the
      blacklist
    $ vim /etc/modprobe.d/blacklist.conf
      `blacklist <name_of_module>`
    OR If you want to do it all in the command line
    $ echo 'blacklist <name_of_module>' | sudo tee -a /etc/modprobe.d/blacklist.conf

  }}}

  disk space analysis {{{
    $ sudo du -sh /var/cache/apt
      $ sudo apt autoclean
      $ sudo apt clean
    $ du -sh ~/.cache/thumbnails
      $ rm -rf ~/.cache/thumbnails

    Built-in tools {{{
      $ df --help
      $ df -ah
      $ df -hT /home

      $ du -ha / | sort -n -r | head -n 10
        NOTE: This one takes a long time to run through, and by the end of it, I didn't find it's
        output that useful. It goes through the whole shebang and lists the top ten largest (set in
        the last `-n 10`).

      <cd into a directory>
      $ du -sh # to see how much space it takes up

    }}}

    Third-party tools {{{
      du-dust, gdmap, dua-cli, and dutree are pretty slick and fairly quick

      baobob {{{
        $ sudo apt install baobab

      }}}

      du-dust {{{
        $ cargo install du-dust
        $ dust

      }}}

      dua-cli {{{
        $ cargo install dua-cli

        `cd` into desired parent directory then use one of the following
          $ dua
          $ dua i

      }}}

      duc {{{
        $ sudo apt install duc
          $ duc index /usr
          $ duc gui /usr
          $ duc ui /usr

      }}}

      dutree {{{
        $ cargo install dutree

      }}}

      gdmap {{{
        NOTE: This is kind of like what you use on Windows, great and fast!
        $ sudo apt install gdmap

      }}}

      gdu {{{
        curl -L https://github.com/dundee/gdu/releases/latest/download/gdu_linux_amd64.tgz | tar xz
        chmod +x gdu_linux_amd64
        mv gdu_linux_amd64 /usr/bin/gdu

      }}}

      ncdu {{{
        $ sudo apt install ncdu

      }}}

      qdirstat {{{

      }}}

      xdiskusage {{{
        $ sudo apt install xdiskusage

      }}}

    }}}

  }}}

  display key presses on screen  {{{
    NOTE: If you're using Lubuntu, its default window manager (openbox) does not allow for
    transparency, which means in order for something like screenkey to have a transparent
    background, you'll need to either install and use another window manager for Lubuntu or switch
    to a different desktop environment like gnome

    screenkey
      https://gitlab.com/screenkey/screenkey
      $ sudo apt install screenkey
      # interact with the tray icon to change settings or quit

  }}}

  distributions aka "distros" {{{
    https://distrowatch.com
      This site keeps track of a lot of the software distributed throughout the linux universe

    What you're choosing here is what type of "universe" you want Linux to get all of its program
    files from. Some distros are known for having strict guidelines for what gets admitted into
    their platforms as well as when how often/when updates are cleared for distribution to its
    users.

    Debian is seen as being a bit more stubborn and "old" in that it's generally not in a rush to
    get the latest and greatest of anything and instead, prioritizes stability first and foremost.

    In contrast, something like Arch seeks to be more on the "bleeding edge" of its program
    versions. For the most part, it's also a more barebones setup, meaning that users must go
    through and customize a LOT more aspects of their systems in order to get them up and running.
    The advantage being that after you're done, you're left with something this is 'yours' per se.
    That said, there are arch based distros that seek to relieve much of this setup/install headache
    by offering default setups that allow them to be more plug and play feeling, not requiring as
    much to-the-metal configuring (for example, see Manjaro, Archbang, etc)

    Arch based {{{
      UPDATE: Something that's come out since I last explored this space is the availability of
      script based installs. Evidently these are fully customizable and rock-solid dependable
      scripts that get you from zero to 100% in no time flat.
        Examples include
          https://wiki.archlinux.org/title/archinstall
          https://picodotdev.github.io/alis/
          https://github.com/MatMoul/archfi

      Manjaro and Endeavor are my current go-tos in this realm. One of the only drawbacks, however,
      is I'm still not sure what to do about vim related plugins that have apt dependencies. Maybe
      there's a way to get comparable packages off of pacman or whatever it is that Arch uses?

      ArchBang

      ArchLabs {{{
        During install, lets the user choose a window manager (useful if you want to go with dwm)

        https://archlabslinux.com

      }}}

      ArchLinux {{{
        LXDE
          https://wiki.archlinux.org/index.php/LXDE

        XFCE
          https://wiki.archlinux.org/index.php/Xfce

      }}}

      Artix {{{
        https://artixlinux.org/

      }}}

      Chakra

      EndeavorOS {{{
        https://endeavouros.com
        Word on the street is that it's a very user-friendly installation and user experience, lot
        of recommendations

      }}}

      Manjaro {{{
        Whenever I installed Manjaro, I ran into a lot of issues trying to get it to display
        properly on both monitors.

        To get it working properly, here's what I had to do
          Before the install process was ever begun, I made sure the setup the new virtual machine
          to use the VBoxSVGA graphics controller and enabled 3D acceleration

          During the install process itself, choose NON-free drivers if you're on a system with
          a GPU. This will 'force' it to use a driver set that is proprietary to nVidia rather than
          than the third-party open source drivers.

          Once the installation is complete, setup |vbox-guest-additions|

      }}}

    }}}

    Debian based {{{
      LXLE, Bodhi, and BunsenLabs are my current faves. Mint is also really good if you want a bit
      more bling, comes in MATE, Cinnamon, and LXLE configurations

      Bodhi, it turns out, is even LIGHTER than LXLE! And once you get your custom setup environment
      up and running, you can tell the difference! PLUS! It works well with dual monitors right out
      of the box once you switch to the VBoxSVGA controller

      Lubuntu is still good, just a tad more hungry than before. The only major problem I'm facing
      currently is that, on VirtualBox 6.16.16, I cannot get dual monitors to work properly.
      Hopefully that gets sorted out at some point, but for the time being, as long as you're
      looking to rock a single monitor setup, LXLE is the way to go!

        $ lsb_release -a
          See details about current Ubuntu version

      absolute linux {{{
        https://www.absolutelinux.org/

      }}}

      antiX {{{
        https://antixlinux.com

      }}}

      Bodhi Linux {{{
        http://www.bodhilinux.com/download/

        pavucontrol is what is used to control audio settings

        For screensaver settings, go to
          Settings -> All -> Screen
            Blanking

      }}}

      BunsenLabs {{{
        https://bunsenlabs.org

      }}}

      crunchbang++ {{{
        https://crunchbangplusplus.org

      }}}

      Gnome {{{
        This is getting more into the "full-fledged" category, more resource intensive
        + Transparency

        Install procedure  {{{
          $ sudo apt install gnome-shell
          $ sudo apt install ubuntu-gnome-desktop
            NOTE: This might not be necessary, just copying from remove process
          $ sudo apt install gnome-tweak-tool
          $ gnome-shell --version
          $ sudo apt install gnome-shell-extensions
          $ sudo apt install gnome-menus

          To get extensions, install the extension for whatever browser you're using. Search for Gnome shell
          $ sudo apt install chrome-gnome-shell
            This is an extra step required to install the required host connector thingymabob

          From here, you can go to the https://extensions.gnome.org site and simply click the toggle
          button to effortlessly install and manage your extensions. Very cool!

          To change some of the gnome3 desktop settings, there's a handy GUI tool you can install.
          Just hit <SUPER> and search for dconf-editor. This should direct you to a GUI app you can
          install.

              $ sudo apt install dconf-editor

          To that point, here are some changes I recommend you make.
            org/gnome/desktop/wm/keybindings, then move the value from switch-applications to
            switch-windows. This will give you an alt-tab menu that does not group apps together. If
            you need to blank out the switch-applications key value, just input [] to essentially
            set it to <nothing>

                ['<ALT>Tab']

            Extension Shortcuts

            Themes: (Others are saved in the .themes directory)
              www.gnome-look.org
              Adapta
                sudo apt-add-repository ppa:tista/adapta -y
                  UPDATE: Do NOT go this route, try to get it another way, maybe through git?
                sudo apt update
                sudo apt install adapta-gtk-theme
              Arc-Maia-Grey
                NOTE: Copy and paste this whole segment below into the terminal
                git clone https://github.com/mustafaozhan/Arc-Maia-Grey-Theme.git && \
                cd Arc-Maia-Grey-Theme && \
                chmod +x install.sh \
                && sh install.sh
              Equilux
                https://www.github.com/ddnexus/equilux-theme
              Pop
                sudo add-apt-repository ppa:system76/pop
                  UPDATE: Do NOT go this route, try to get it another way, maybe through git?
                sudo apt update
                sudo apt install pop-theme

        }}}

        Uninstall procedure {{{
          $ sudo apt remove ubuntu-gnome-desktop
          $ sudo apt remove gnome-shell
          $ sudo apt remove --auto-remove ubuntu-gnome-desktop

        }}}

      }}}

      Linux Lite {{{
        https://www.linuxliteos.com/download.php

      }}}

      Lubuntu {{{
        https://lubuntu.net

        My original tried-and-true, really liked it up through 18.04 version, after that, it started
        to become a bit more resource intensive

      }}}

      LXLE {{{
        https://www.lxle.net/
        Kind of a "classic" Lubuntu feel, pre 18.x or whatever when Lub started becoming a little
        less lightweight

        To disable the lockscreen, do'a this'a

          Start -> Control Menu -> Settings -> Default applications for LXSession -> Autostart tab
          -> #locker (unckeck)

      }}}

      Mint

      MX {{{
        Nah, most my recent try of this one saw that it uses LXQt, and I just don't like that

      }}}

      sparky linux {{{
        https://sparkylinux.org

        Nah, consumed a surprising amount of RAM

      }}}

      Ubuntu

      XFCE {{{
        https://www.xfce.org/

      }}}

      xubuntu {{{
        https://xubuntu.org

        a mix of ubuntu and xfce

      }}}

      zorin {{{
        https://zorinos.com

      }}}

    }}}

    Independent {{{
      Porteus {{{
        http://www.porteus.org/
        https://distrowatch.com/table.php?distribution=porteus

        Super duper small and fast (like, run from a CD small), but I'm not entirely sure what it's
        based off of... Does it use apt, for example?

      }}}

      Puppy Linux {{{
        https://puppylinux.com/

        Looks and runs great, the only major downside is that it does not use apt, but its own "pet"
        type files native to puppy. So I'm not sure how I'd go about installing many of the apps and
        plugins I want/need.

      }}}

      Solus

      Void {{{
        https://voidlinux.org/

      }}}

    }}}

  }}}

  dtach{{{
    Terminal session management tool commonly used alongside something like dvtm. Many users
    recommend abduco over dtach, describe abduco as being "dtach on steroids" with more features

    $ sudo apt install dtach

  }}}

  dvtm  {{{
    Super lightweight alternative to tmux's multiple terminal thingy. It keeps with the unix design
    philosophy of only doing one thing and doing that one thing extremely well. Often paired with
    something like dtach or abduco (unix session management tools) to get tmux-like session
    management.

    http://brain-dump.org/projects/dvtm/

    $ sudo apt install dvtm

    https://media.ccc.de/v/cosin2018-128-abduco_dvtm_session_and_tiling_window_management_for_the_console#t=0

    Commonly used startup commands:
      https://gist.github.com/shreyasrk/12cd0c9f5d344f116242  {{{

      -v              prints version information to standard output, then exits.
      -m <mod>        set default modifier at runtime.
      -d escdelay     set  the delay ncurses waits before deciding if a character that
                      might be part of an escape  sequence  is  actually  part  of  an
                      escape sequence.
      -h nnn          set the scrollback history buffer size at runtime.
      -s status-fifo  if  status-fifo  is  a  named  pipe  it’s  content  is  read and
                      displayed. See the dvtm-status script for an usage example.
      [cmd...]        Execute cmd after dvtm is started.

    KeyBoard Shortcuts:
      ^G Default mod key (similar to tmux's prefix, just called a different name), can be
        modified in config.h or at startup with -m command line option. If you want to change it to
        ^A, you'd use something like this:

              $ dvtm -m ^A

        mod c          Create a new shell window
        mod x          Close focused window
        mod l          Increases the master area width about 5% (except grid and fullscreen layout)
        mod h          Decreases the master area width about 5% (except grid and fullscreen layout)
        mod j          Focus next window
        mod k          Focus previous window
        mod [1..n]     Focus the nth window
        mod .          Toggle minimization of current window
        mod u          Focus next non minimized window
        mod i          Focus prev non minimized window
        mod m          Maximize current window (change to fullscreen layout)
        mod PageUp     Scroll up
        mod PageDown   Scroll down
        mod Space      Toggle between defined layouts (affects all windows)
        mod Enter      Zooms/cycles current window to/from master area
        mod t          Change to vertical stack tiling layout
        mod b          Change to bottom stack tiling layout
        mod g          Change to grid layout
        mod s          Shows/hides the status bar
        mod r          Redraw whole screen
        mod G          Escape the next typed key
        mod a          Toggle keyboard multiplexing mode, if activated keypresses are sent to all
                       non minimized windows
        mod X          Lock screen
        mod B          Toggle bell (off by default)
        mod M          Toggle dvtm mouse grabbing
        mod q          Quit dvtm

    }}}

  }}}

  email {{{
    It's important to note at the outset that getting email setup through something like mutt takes
    more than simply installing mutt. Why? Because mutt is only ONE aspect of the environment. And,
    in keeping with Unix's mantra of programs doing ONE thing and doing it well, there's more to the
    story than just mutt.

    aerc {{{
      https://aerc-mail.org/

      Requires go version 1.11

      Requires scdoc
        $ cd /usr/share
        $ sudo git clone https://git.sr.ht/~sircmpwn/scdoc
        $ sudo make
        $ sudo make install

      https://oren.github.io/articles/text-based-gmail/
        $ cd /usr/share
        $ sudo git clone https://git.sr.ht/~sircmpwn/aerc
        $ cd aerc
        $ sudo make
        $ sudo make install

    }}}

    astroid mail {{{
      https://astroidmail.github.io

    }}}

    mutt / neomutt {{{
      https://github.com/LukeSmithxyz/mutt-wizard
        $ cd/usr/share
        $ sudo git clone https://github.com/LukeSmithxyz/mutt-wizard
        $ cd mutt-wizard
        $ sudo make install

        NOTE: 20210418 This video recommends setting up a few additional packages before diving
        directly into mw for the first time. Certain things, like neomutt itself, are actually
        "required" prior to launching mw

          https://www.youtube.com/watch?v=iwYL3JzVVXM

            $ sudo apt install neomutt curl isync msmtp pass pam-gnupg notmuch abook urlview cronie

          Also, when it comes to entering the password for your gmail account, this is where you'll
          need to setup an app-specific password through gmail rather than use your "normal" login
          password. This is similar to how you handled setup Steph's yahoo email on her phone, same
          thing here.

        $ mw
          NOTE: Once you've completely gone through the wizard, you then transition to using neomutt
          as a standalone program, ie

            $ neomutt

      https://webgefrickel.de/blog/a-modern-mutt-setup
      https://webgefrickel.de/blog/a-modern-mutt-setup-part-two

      Some additional recommended tools
        abook
        gpgme
        khard
        mbsync
        mu
        notmuch
        pam-gnupg
        ripmime
        urlscan
        urlview
        vdirsyncer
        w3m

    }}}

  }}}

  encryption {{{
    Removing home folder encryption after install  {{{
      1. Backup the home directory while you are logged in
        $ sudo cp -rp /home/<user> /home/<user>.backup
          Check and see that the backup has everything you need before proceeding further!
      2. Relogin to the system with the root account (or anything other than the above user that also
         has sudo priveleges)
      3. Delete your home directory
        $ rm -rf /home/<user>
      4. Remove some packages
        $ sudo apt remove ecryptfs-utils libecryptfs0
          If it doesn't let you do this, you may first need to remove

            /home/.ecryptfs/<user_name>

      5. Restore your home directory
        $ mv /home/<user>.backup /home/<user>
      6. Reboot
      7. Remove the following directories
        $ rm -rf ~/.Private
        $ rm -rf ~/.ecryptfs
      8. Done!

    }}}

    Adding home folder encryption after install {{{
      $ sudo apt install ecryptfs-utils cryptsetup
      You'll need to create an alternate user account with administrator (sudo) privileges
      Log in as that new user
      $ sudo ecrypt-migrate-home-u <your original user account name>
      Once done, you then need to do these following steps, in this order (VERY IMPORTANT):
        1. Log in as the original user account immediately - DO NOT reboot
        2. Generate and record a recovery phrase
        3. Encrypt swap partition
          $ sudo ecryptfs-setup-swap
        4. With that done, restart your system a few times just to make sure things are working
           properly
        5. Delete the temp alternate user account made for this process
          $ sudo rm -rf /home/<alternate user name>
        6. Delete the backup home directory for your original user that was created in the process

    }}}

  }}}

  epub and pdf readers {{{
    Calibre {{{
      $ sudo apt install calibre

    }}}

    chm2pdf {{{
      $ sudo apt install chm2pdf # converts chm to pdf

    }}}

    Evince {{{
      https://wiki.gnome.org/Apps/Evince

    }}}

    Foxit {{{
      https://www.foxitsoftware.com/downloads/

    }}}

    kchmviewer {{{
      Search in package manager, provides support for .chm files

    }}}

    Mupdf {{{
      $ sudo apt install mupdf
      https://mupdf.com/downloads/

    }}}

    Okular {{{
      https://okular.kde.org/

    }}}

    xpdfreader {{{
      $ sudo apt install xpdf

    }}}

    zathura {{{
      $ sudo rm /etc/apt/preferences.d/pin-zathura
      $ sudo add-apt-repository ppa:spvkgn/zathura-mupdf
      $ sudo apt install zathura zathura-pdf-mupdf

        zathura-pdf-mupdf # epub support
        zathura-djvu # djvu support

      ^r # invert colors
      d  # toggle dual-page view
      a  # fit-page
      s  # fit-page with slight zoom

    }}}

  }}}

  error logs and system troubleshooting  {{{
    $ xdg-open https://errors.ubuntu.com/user/`sudo cat /var/lib/whoopsie/whoopsie-id`

    https://askubuntu.com/a/149455/757251
    https://launchpad.net/

  }}}

  file managers  {{{
    Sidenote (not sure where to put this)
      If you ever want to open a graphical user interface oriented program with sudo permissions
        $ sudo -i
          opens a root shell
        $ pcmanfm
          or whatever program you want to open. This is a file explorer, for example
        $ exit
          whenever you're done doing what you're doing in the GUI program, go back to the terminal
          to exit properly

    lf {{{
      https://github.com/gokcehan/lf

    }}}

    midnight commander {{{
      terminal based does NOT have vim-like bindings
      $ sudo apt mc
      run with `$ mc`

    }}}

    nnn {{{
      https://github.com/jarun/nnn

      https://youtu.be/-knZwdd1ScU

      $ sudo apt install nnn
        UPDATE: Use `$ installnnn` alias

      $ nnn
        ?
        q
        . # show hidden files

      There is also a vim plugin for this

      Plugins {{{
        https://github.com/jarun/nnn/tree/master/plugins
        https://github.com/jarun/nnn/blob/master/plugins/getplugs
        https://github.com/jarun/nnn/blob/master/plugins/fzplug
        https://github.com/jarun/nnn/blob/master/plugins/nbak

        $ curl -Ls https://raw.githubusercontent.com/jarun/nnn/master/plugins/getplugs | sh
        Plugins are installed to ${XDG_CONFIG_HOME:-$HOME/.config}/nnn/plugins.

      }}}

    }}}

    pcmanfm {{{
      gui based
      $ sudo apt install pcmanfm

    }}}

    ranger {{{
      terminal based with vim-like keybindings, though not as closely as vifm, it has different
      goals

      https://github.com/ranger/ranger

        NOTE: You can also use apt, but it's version is a bit out of date compared to what's
        available on git

        $ sudo apt install ranger

      There is also a ranger plugin for vim too
      https://github.com/francoiscabrol/ranger.vim

    }}}

    spacefm {{{
      interactive shell based
      $ sudo apt install spacefm

    }}}

      vifm {{{
        terminal based with vim-like keybindings, building in accordance with vim is one if its
        hallmark principles

          $ sudo apt install vifm

        NOTE: Supposedly, you can use ~/.vifm/vifmrc to set persistent configuration settings,
        similar to a .vimrc file. I'm assuming that it overrides the same file found in
        /usr/share/vifm/vifmrc
          NOTE: Use Stow for vifm to import .vifmrc

        run with `$ vifm`
        quit with `:q`

        https://vifm.info
        https://vifm.info/manual.shtml
        https://vifm.info/cheatsheets.shtml

        /              # search
        za             # toggle hidden files
        ^O             # backwards in nav history
        :!<program> %c # open file with named program
          :!brave-browser %c
        :file          # display options on cursored file
        :mkdir         # make directory
        :touch         # create file
        :tree          # toggle tree view
        :view          # toggle view contents mode

        $ man vifm
        :h

      }}}

    xplr {{{
      https://github.com/sayanarijit/xplr

      $ cargo install --force xplr

    }}}

  }}}

  file syncing  {{{
    syncthing {{{
      https://syncthing.net
        Haven't felt the need to mess around with this yet, but evidently combined with git repo's
        this can become very powerful

    }}}

  }}}

  file manipulations (renaming, upper to lower, etc) {{{
    https://linuxconfig.org/rename-all-files-from-uppercase-to-lowercase-characters
      $ sudo apt install rename
      $ sudo apt install mmv

      Rename FILES to lower
        $ for i in $( ls | grep [A-Z] ); do mv -i $i `echo $i | tr 'A-Z' 'a-z'`; done
        $ rename -f 'y/A-Z/a-z/' *
        $ mmv '*' '#l1'

      Rename FILES to lower - RECURSIVELY
        $ find . -depth -type f | xargs -n 1 rename 's/(.*)\/([^\/]*)/$1\/\L$2/' {} \;

      Rename DIRECTORIES to lower - RECURSIVELY
        $ find . -depth -type d | xargs -n 1 rename 's/(.*)\/([^\/]*)/$1\/\L$2/' {} \;

      Rename FILES and DIRECTORIES to lower - RECURSIVELY
        $ find . -depth | xargs -n 1 rename 's/(.*)\/([^\/]*)/$1\/\L$2/' {} \;

  }}}

  fonts  {{{
    $ sudo apt install fontconfig
      This little badboy lets you use the $fc-x commands in your terminal

    For linux, any fonts you install into ~/.fonts will be automatically pulled in when you run the
    command. Which means you can pull down fonts from github into that directory, run your

      UPDATE: No, the ~/.fonts directory doesn't seem to be recognized
      $ fc-cache -f -v

    $ cachefonts alias and, boom! Call it a day
      UPDATE: Hmm... Not so fast muchacho... Seems it may not quite be that easy...

    $ sudo apt install unifont
      This packages includes the vast majority of all unicode related symbols and glyphs you might
      encounter from time to time. For example, some of the vim status bar plugins use special
      characters, those are included here as well.

      HOWEVER, that said, even with these installed, you still need to use a font that incorporates
      the symbols.

    NOTE: True-Type Fonts are a bit of a misnomer when it comes to displaying on a screen. It sounds
    like it'd be a good thing, but it's actually not. In my experience, a TTF is meant to be at
    a certain size (usually 12). Anything that deviates from that size introduces some weird
    artifacting and/or ghosting effects. A non-TTF, on the other hand, will retain its crispness
    regardless of the size you choose. It's very similar to how vector graphics editors work, like
    Inkscape. You can resize the image as much as you want without any fear of distorting the
    result.

    Another type of font you'll see from time to time is a bitmap font. These fonts usually come in
    many different sizes, each corresponding to a particular font size.

    TLDR: Favor NON-TTF fonts where available, especially if you're looking for a smaller font

    For fonts that you may pull down from github or wherever, you can move them into one of the
    following

      $ mv <font path> ~/.fonts
        This is a "local" install only for the logged in user
      $ sudo mv <font path> /usr/share/fonts/
        This is a "global" install to the system for ALL users

    For any fonts that you install manually by pasting files into the font directories, you then
    need to run:

      $ fc-cache -f -v
      UPDATE: aliased to simply `cachefonts`

    $ fc-list # view all installed fonts

    There are fonts available in the package manager, search for "font"
      $ sudo apt install ...
        console-terminus
        fonts-fantasque-sans
        fonts-firacode
        fonts-hack-ttf
        fonts-inconsolata
        fonts-iosevka
          NOTE: This never showed up in apt for some reason...
        ttf-anonymous-pro
        ttf-dejavu
        ttf-droid
        ttf-inconsolata
        ttf-liberation
        ttf-mplus
        ttf-mscorefonts-installer
        xfonts-jmk # the font is called Neep
        xfonts-terminus

    Github
      https://github.com/adobe-fonts/source-code-pro
      https://github.com/be5invis/iosevka
      https://github.com/cormullion/juliamono
      https://github.com/JetBrains/JetBrainsMono
        $ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/JetBrains/JetBrainsMono/master/install_manual.sh)"
      https://github.com/larsenwork/monoid
      https://github.com/microsoft/cascadia-code
      https://github.com/mozilla/Fira
      https://github.com/nathco/Office-Code-Pro
      https://github.com/NerdyPepper/scientifica
      https://github.com/ProgrammingFonts/ProgrammingFonts
      https://github.com/romeovs/creep
      https://github.com/rubjo/victor-mono
      https://github.com/source-foundry/font-line
        an interesting little tool modifies the line spacing of any font
      https://github.com/turquoise-hexagon/cherry

    Other sites
      https://mozilla.github.io/Fira/ # this may not be the exact same firacode font from apt
      https://dribble.com # For design inspiration
      https://fontsquirrel.com
      https://hellohappy.org/beautiful-web-type
      https://input.fontbureau.com/
      https://lowing.org/fonts/
      https://mplus-fonts.osdn.jp/about-en.html
      https://typewolf.com
      https://www.nerdfonts.com/font-downloads

    Chart-o-fonts
      UPDATE: Something I've been experimenting with lately is increasing the line-height. This
      allows the size to remain relatively small, while giving things room to "breathe" a bit more.
      Unfortunately, this cannot be set in terminal vim. In order to get something similar, you need
      to adjust it in the terminal settings themselves.

      Fonts are sized just before their width exceeds the 125 character limit you've set for vim,
      this insures that a vertically split view will still be able to show most of the code in each
      pane.

      Values should be read as estimates, I've noticed they'll differ by a few points here and there
      depending on what terminal/etc I'm using at the time. So it's best to use them as bases of
      comparison rather than hard and fast rules

        * = more stars, more likey-likey
      Row = How many lines display on a full height window/pane
      Col = How many 'letters' wide display on a half vertical-split window/pane
      TTF = fonts that do not scale well "off-size"
      lt  = size recommendation for laptop

                          | Row | Col | Sz | TTF | lt
                          ----------------------------
    * Bitstream           | 67  | 129 |  9 |     |
  *** Fira Mono Reg       |  ?  |   ? |  ? |  ?  | ?
  *** Hack                | 67  | 129 |  9 |     | 6
      Inconsolata         | 67  | 129 | 10 |     |
    * Input Mono Cond Reg | 63  | 129 |  9 |  ?  | 7
   ** Input Mono Reg      | 78  | 129 |  8 |  ?  |
   ** Iosevka Term Reg    | 58  | 129 | 10 |     | 8
  *** JetBrains Mono NL   |  ?  |   ? |  ? |  ?  | ?
   ** M+ 1m>1mn>2m        | 47  | 129 | 10 |     |
      Proggy Clean        | 71  | 129 | 12 | TTF |
      Proggy Square       | 83  | 129 | 12 | TTF |
   ** Proggy Tiny         | 93  | 151 | 12 | TTF |
      Roboto Mono         | 66  | 151 |  8 |     |
    * Share-TechMono      | 61  | 129 |  9 |     | ?
    * Source Code Pro     | 63  | 129 |  9 |     |
  *** Ubuntu Mono Regular | 68  | 129 | 10 |     | ?
      Whois mono          | 63  | 129 | 11 | TTF |

	}}}

  FZF {{{
    This is a system-wide fuzzy file searcher that, in your case, is installed and maintained
    through the Vim plugin manager. It has a vim-specific version (fzf.vim), but this section will
    be dealing with fzf as it relates to the system-wide version.

    Most of these notes are taken from: https://www.youtube.com/watch?v=tB-AgxzBmH8

    At its heart, FZF does exactly and only what it says it does. It allows for fuzzy-file searches.

    Now, that in and of itself may not sound very interesting, but once you start digging into it,
    the possibilities really start to shine.

    Commands
      ^r    # bash history, reverse
      ^t    # file search
      <M-c> # directory search

    Syntax
      '<word>  # exact search
      ^<word>  # starts with
      <word>$  # ends with
      !        # inverter
      !<word>  # does NOT contain
      !^<word> # does NOT start with
      !<word>$ # does NOT end with
      |        # or operator

        NOTE: And you can string these together with spaces. So if you're looking for a file named
        music that could end in .mp3 or .opus you'd use something like

          ^t # at terminal, to go into FZF file search mode
          ^music .mp3$ | .opus$

    Recommended you pair it with fd-find, check out its install notes in your vimrc if you don't
    already have it up and running.

    One of the things you can do is call up a file in the command line and "pipe" it into fzf in
    order for it to gain the fuzzy search powers

    You can also combine it with other commands. So let's say you want to open a particular file in
    vim.

      $ vim ^t <search related stuff>
        The ^t will call up FZF, allowing you to search for a particular file, then press <CR>
        and voila! Vim will open the selected file.

    Here's a quick rundown of some of the big ones

      $ man fzf

      $ <filename> | fzf
        $ printenv |fzf

      $ <command> | fzf
        $ find * -type f -not -path '*/\.*' | fzf
          '$ fzf'
            this is an alias for the above since it's evidently such a common thing to do

  }}}

  greeters {{{
    lightdm
      the default of many distros

    ly
      https://github.com/nullgemm/ly

  }}}

  gsettings  {{{
    This is a command line interface you can use to get deep into the nitty gritty for various
    program and application settings. I included an example for one of the pomodoro apps, that's how
    the user can change it's taskbar appearance. The developer has said he does not have any plans
    to extend this setting into the app's GUI interface any time soon

  }}}

  hardware info {{{
    $ sudo lshw -html > hardware.html

  }}}

  image viewers {{{
    simple x
      https://github.com/muennich/sxiv

    feh {{{
      $ sudo apt install feh
      $ feh --bg-scale <image filename>
        sets background image
      $ sudo apt install xcompmgr transset
        The transset utility ended up being part of a X11-Utils package or something like that

        Then, to set transparency, you do something like

          $ xcompmgr &
          $ transset <decimal value>
          Then click on whatever window you'd like to make transparent

      NOTE: In order to view webp images in feh, you need to install the following:
        $ sudo apt install libwebp-dev
        $ sudo apt install libimlib2
        $ sudo apt install libimlib2-dev

        $ cd/usr/share
        $ sudo git clone git://github.com/gawen947/imlib2-webp imlib2-webpgit
        $ cd imlib2-webpgit
        $ sudo make
        $ sudo make install

    }}}

  }}}

  kernels {{{
    Upgrading Kernel {{{
      There a couple ways I've seen this done.

        $ sudo apt safe-upgrade
          NOTE: For me, running this did not result in any new Kernel being "found", so it didn't
          accomplish anything.

        Another approach, is to "find" it more directly, like so

          $ dpkg -l | grep "linux-image" # displays currently installed kernel versions
          $ sudo apt upgrade linux-image-* && sudo apt dist-upgrade linux-image-*
            NOTE: When I tried this, it complained of "E: Broken packages" and various "Conflicts".
            I ran through all of the notes I have regarding broken package errors as well as some
            other websites, but so far no luck!

        $ sudo apt dist-upgrade
        $ sudo do-release-upgrade
          NOTE: Last time I ran this with Bodhi, everything seemed to well, but due to an
          interrupted process (?) I ran into a bunch of little annoyances after the fact. To "fix"
          many of these, I used the following commands

              $ sudo dpkg --configure -a
              $ sudo apt install -f

          But that didn't resolve all of it. During the install process, I rec'd some sort of
          message like "re-enable with software properties tool to rebuild the sources.list file" or
          something like that. And I don't know for sure, but I suspect that may have something to
          do with the absolute headache I ran into when it came to trying to compile vim with +lua
          +python and +python3

    }}}

  }}}

  keyboard repeat-delay and repeat-interval  {{{
    UPDATE: The below `xset` settings have been added to the .bashrc file

      $ xset r rate 225 40
      NOTE: To set them back to their default settings, use `$ xset r rate`

    Another approach is to edit a config file for LXDE (assuming you're running the Lubuntu distro)
      $ vim ~/.config/lxsession/LXDE/desktop.conf
      In this file, you'll see settings you can change for mouse interval and delay.

  }}}

  Latex {{{
    http://detexify.kirelabs.org/classify.html
    https://en.wikibooks.org/wiki/LaTeX

    Beamer
      Latex plugin for making slide-shows kind of like Powerpoint

  }}}

  libvips {{{
    https://github.com/libvips/libvips

    Installing from .tar file {{{
      Download the tar file, extract it somewhere, open the terminal to the extracted directory then
      run the following:

        $ ./configure
          NOTE: This can take a long time, but just let'er do'er thing
        $ make
        $ make install
          NOTE: You may need to run these as `sudo` if you receive a pemissions error

    }}}

    Building and updating from source {{{
      UPDATE: The below for libvips and nip2 has since been aliased as

        $ gitinstalllibvips

      The following is excerpted from:
        https://github.com/jcupitt/libvips/wiki/Build-for-Ubuntu#building-from-source

        $ sudo apt install git build-essential libxml2-dev libfftw3-dev \
          libmagickwand-dev libopenexr-dev liborc-0.4-0 \
          gobject-introspection libgsf-1-dev \
          libglib2.0-dev liborc-0.4-dev autoconf

        $ sudo apt install automake libtool swig gtk-doc-tools

        NOTE: You need to get the root directory, not just the home ~ directory
        $ cd /usr/share # takes you to the 'default' install location for all users
        $ sudo git clone https://github.com/jcupitt/libvips.git
          NOTE: If you already have libvips installed and want to update it, then cd into the
          libvips directory then run

            $ sudo git pull

          Then run the same following ./autogen.sh etc stuff after that
        $ cd libvips
        $ sudo ./autogen.sh
        $ sudo ./configure
        NOTE: Site said to run `make`, but this failed due to permissions. Skipping straight to
          `$ sudo make install` worked
          $ make
        $ sudo make install
          NOTE: This can take a long time. It will seem like the text is simply repeating itself,
          but keep waiting :)
        $ vips -v

      Then, add the following paths to your .bashrc (they're already there if you pulled from
      dotfiles):

        export VIPSHOME=/usr/local
        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$VIPSHOME/lib
        export PATH=$PATH:$VIPSHOME/bin
        export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:$VIPSHOME/lib/pkgconfig
        export MANPATH=$MANPATH:$VIPSHOME/man
        export PYTHONPATH=$VIPSHOME/lib/python2.7/site-packages

      Now, onto the nip2 tool. If you built libvips from source, you also need to built nip2 from
      source in order for it to be compatible.

        $ sudo apt install libgtk2.0-dev flex bison
        NOTE: Before you start blowing and going, make sure you're at the /usr/share directory just
          as you started above
        $ cd /usr/share
        $ sudo git clone https://github.com/jcupitt/nip2.git
        $ cd nip2
        $ sudo ./autogen.sh
        $ sudo ./configure
        NOTE: Site said to run `make`, but this failed due to permissions. Skipping straight to
          `$ sudo make install` worked
          $ make
        $ sudo make install
          NOTE: This can take a long time. It will seem like the text is simply repeating itself,
          but keep waiting :)
        $ nip2 -v

    }}}

  }}}

  Media players  {{{
    NOTE: If these command-lines don't work, just search for them with the Synaptic Package Manager.
    Also, there are other players out there that are regularly mentioned, like mpv, etc but I've
    found that VLC is the most reliable. It simply works without any fiddliing or messing around.
    Lubuntu's built-in Audacious player is also perfectly fine for mp3 files

    amarok {{{
      https://amarok.kde.org/en

      $ sudo apt install amarok

    }}}

    clementine {{{
      Is supposed to have Onedrive support, though when I tried it (20210318) it seemed to have
      issues. Reading through the github issues, it looks like it's currently being worked
      on. Hopefully will be fixed soon.
        https://github.com/clementine-player/Clementine/pull/6931

      $ sudo apt install clementine

        $ sudo add-apt-repository ppa:me-davidsansome/clementine
        $ sudo apt update
        $ sudo apt install clementine

    }}}

    cmus {{{
      https://cmus.github.io/
      $ sudo apt install cmus

    }}}

    moc {{{
      https://wiki.archlinux.org/title/MOC

      $ sudo apt install moc
      $ mocp

    }}}

    mpd {{{
      https://github.com/MusicPlayerDaemon/MPD
      https://github.com/seebye/fmui

      $ sudo apt install mpd
      $ sudo apt install ncmpcpp

    }}}

    mplayer {{{
      $ sudo apt install mplayer mplayer-gui mplayer-skins mplayer-fonts
        NOTE: There's a glitch with the default skin that prevents it from starting. To fix, use
        $ sudo add-apt-repository ppa:mc3man/mplay-skins
          UPDATE: Do NOT go this route, try to get it another way, maybe through git?
        $ sudo apt update
        $ sudo apt upgrade

    }}}

    mpv {{{
      $ sudo apt install mpv
      $ mpv --no-video <directory>

    }}}

    musikcube {{{
      https://github.com/clangen/musikcube

      $ installmusikcube
        Alias

    }}}

    strawberry {{{
      https://github.com/strawberrymusicplayer/strawberry
        $ git clone https://github.com/strawberrymusicplayer/strawberry
        $ cd strawberry
        $ mkdir build && cd build
        $ cmake ..
        $ make -j$(nproc)
        $ sudo make install

        $ sudo add-apt-repository ppa:jonaski/strawberry
        $ sudo apt-get update
        $ sudo apt-get install strawberry

    }}}

  vimus {{{
    https://github.com/vimus/vimus

  }}}

    vlc {{{
      NOTE: Can also be ran completely within the terminal!

      $ sudo apt install vlc
      $ vlc -I ncurses --no-video

    }}}

    troubleshooting {{{
      Time and again I could never get a DVD to play on linux. Here are some suggestions...

      $ sudo apt install libdvd-pkg
        NOTE: will may launch some sort of secondary setup. If it does, just follow the prompts then
        head on to the next step
      $ sudo dpkg-reconfigure libdvd-pkg

      Once those are both taken care of, restart your system, cross you fingers, then give it a go!

    }}}

  }}}

  Mouse Wheel Scrolling  {{{
    By default, Linux caps the mouse wheel scrolling to 1 line. To increase this sensitivity, you'll
    need to install a third-party app and create a config file as follows:

        $ sudo apt install imwheel

        NOTE: the .imwheelrc dotfile is part of your .dotfiles repo so you should already have it,
        if not here's an example

          $ vim ~/.imwheelrc
          And then, in that file, type the following, to whatever per setting you like:

            ".*"
            None, Up, Up, 3
            None, Down, Down, 3

      Then, to run imwheel...

        $ imwheel

      To turn it off...

        $ killall imwheel

  }}}

  Network management {{{
    https://linuxconfig.org/connect-to-wifi-from-the-linux-command-line
    https://wiki.archlinux.org/title/NetworkManager

    $ nmcli
      https://developer.gnome.org/NetworkManager/stable/nmcli.html

    $ nmtui

    $ sudo wpa_cli
      > scan

    NOTE: If you're on a wifi connection that requires an additional login step on a webpage and
    can't get the site to pop-up automatically, try directing your browser to the the default login
    page for the router

      192.168.1.1
      127.1.1.1
      1.1.1.1
      http://localhost

    Troubleshooting {{{
      Wireless adapter not working? {{{
        After searching around forever about how to get the network listing to NOT show as
        disabled/disconnected, it turns out I had accidentally pressed the FN+F8 key on the laptop,
        which disables the WIFI.

        That said! I learned some interesting stuff in the meantime that may come into play in
        another situations later down the line :)

        https://forums.linuxmint.com/viewtopic.php?t=343651

        $ sudo lshw -C network

        $ ip addr show
        $ ip -s addr
        $ ip -s -c -h addr

      }}}

    }}}


  }}}

  Node and Node related stuffins  {{{
    UPDATE: So the thing you were originally missing from this intro is that, yes, while there are
    various node package managers out there to use, and yes, while pnpm is a great one, it goes one
    step deeper from there. It is actually highly recommended that you use "Node Version Manager" to
    manage your packages/package manager rather than interacting with the package manager directly.
    This is why you've put 'nvm' notes at the top.

    To be quite honest though, I'm still a little confused about all of this... For example

      Does nvm work with pnpm? What do you do about packages that were originally installed with
      `sudo npm/pnpm ...` stuff?

      Is nvm really a package manager or is it just something for node.js? Because looking through
      the readme, I don't see anything about install/upgrading node packages...

    -----

    See notes on Rails Webpacker on how some of this is incorporated into a Rails application

    Node Package Managers {{{
      Node and its various packages can be managed through several node package managers. The most
      popular are npm, yarn, and pnpm. They each have their own takes on things and over various
      advantages and disadvantages. Overall, npm is the reigning champ, with the longest history and
      --possibly because of this?-- the most adoption. Yarn entered the scene in 2015 ish and is
      relative newcomer that improves some how dependencies are handled. pnpm is relatively new-ish
      too, it's main difference from npm or yarn is in how packages are saved and referenced.
      Supposedly pnpm can drastically cut down on hard-drive space usage, especially once you start
      running multiple projects that rely on node packages.

      My impressions from reading the web as of 20180612 is that PNPM is the better of the three,
      combining some of the features of Yarn with even faster speeds, etc than npm. HOWEVER, that
      said, some Rails related things might require the use of Yarn (??) so you may need to run some
      yarn commands at least once in order to set things up properly.

      UPDATE 20200523: Yes, reading the documentation on the webpacker gem that's part of Rails 5/6+,
      webpacker uses Yarn as the default package manager for node_modules

      nvm  {{{
        https://github.com/creationix/nvm/blob/master/README.md

        Supposedly managing node.js and npm through nvm helps alleviate some of the permission
        access issues that can crop up. And the best thing about it, is it can be readily installed
        over/alongside an existing npm node installation without having to delete or rebuild your
        existing modules, etc

        Install {{{
          $ cd
          $ sudo curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.34.0/install.sh | bash

          If it complains about the .nvm directory not existing, just create it manually then rerun
          the above command

          -- restart terminal --

          $ command -v nvm
          $ nvm --version
            These should both return something, lets you know it's setup properly and ready to go
          $ nvm install node
          $ nvm install-latest-npm

          There's a good chance after this is done that you'll need to reboot. That's how I got rails
          to recognize the install

        }}}

        Use {{{
          Checkout the readme linked above for all the how-to details.
          When it comes to actually installing node related modules/packages/etc, from what
          I understand so far, you use

            $ nvm install <xyz>

          If you want to uninstall a version of node use this syntax

            $ nvm ls # to list the versions you have installed on the system
            $ nvm uninstall <version number>

        }}}

        Update node version with nvm {{{
          There are two approaches. The first seems a bit easier in my experience
          1. nvm ls-remote
              <find the latest version listed>
              nvm use <version>
                Example: $ nvm use 13.8.0

          2. Pull down the latest version from git with the following commands
              $ cd ~/.nvm
              $ git fetch --tags
              $ TAG=$(git describe --tags `git rev-list --tags --max-count=1`)
              $ git checkout "$TAG"
              $ source ~/.nvm/nvm.sh

        }}}

          Migrate/update previously installed packages with npm to nvm {{{
            This one seems to be a little trickier...

              For updating npm
                $ nvm install-latest-npm

              For updating other packages
                $ nvm reinstall-packages system
                $ nvm reinstall-packages-from=<prior-version>

          }}}

          Troubleshooting {{{
            After installing nvm, I started receiving an error message relating to NPM Update Check
            Failed because sudo access was not granted. To stop this from happening, I deleted the
            following folder:

                ~/.config/configstore/

            Upon restart, the message went away and "fresh" version of that same directory was created

            "N/A version is not installed"
              Had this message start showing up in every console instance. Started happening right
              after I went through and rebuilt my nvm and npm installations froms scratch to get rid
              of any remaining junk. The problem is that the default nvm version is trying to
              reference a version that no longer exists on the system. To fix, run the following:

                $ nvm ls
                  this returns the version of nvm you have installed
                $ nvm alias default <desired version number from above output>
                $ nvm alias use default

          }}}

      }}}

      node.js  {{{
        NOTE: This step should not be required if you installed node with nvm as listed above

        www.nodejs.org
          $ sudo curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -
          $ sudo apt install -y nodejs

      }}}

      npm  {{{
        UPDATE: It is STRONGLY encouraged that you **NEVER** use `sudo npm` for anything. Because if
        there is any sort of post-install hook, it will be ran with sudo priveleges. With this
        knowledge, I went ahead and changed all mentions of `sudo npm` and replaced them with simply
        `npm`

        IF YOU DO! However, find yourself caught in some sort of god-forsaken user permission hell,
        from what I can tell, you need to rebuild the permissions for the related node_modules
        directory using `chown`. Here's what I did when I faced this beast with the land_app:

          !!!WARNING!!!
          $ sudo chown -R linux:linux /home/linux/Programming/Projects/land_app/node_modules
          !!!WARNING!!!

          You have to be VERY careful with how you use this command. You came across some other
          permissions related to your systemwide npm/yarn stuff as well, so you ran this command on
          its offending folders and one its parents and guess what? Yep, that parent folder you ran
          it on? You ended up bricking the entire user permission hierarchy for the OS, which
          rendered it practically useless. You had to fire up a new VM over it. So be VERY VERY
          careful with it. When used properly, it can be the right tool. The best approach, in this
          situation, is to avoid using `sudo` with yarn/npm related stuff in the firstplace

        The `linux:linux` stuff is a reference to `<yourusername>:<yourusername>`

        -------------

        NOTE: Now, before you get rocking and rolling with the below NPM related stuff, know this!
        From what you've read, it seems like pnpm may be a better package manager, kind of like npm
        on steroids. In general, pnpm uses all of the same commands as npm, all you've gotta do is
        use 'pnpm' rather than 'npm' and you're good to go. Cool?

        Install {{{
          NOTE: Installing npm/pnpm through nvm as you noted above is generally more reliable and
          gets around some permission issues. But in case you need to do things manually, here's
          a rundown

            $ sudo apt install npm
            $ npm install -g npm
              updates npm to latest
            $ npm ls -g minimatch
            $ npm install -g npm-check
              $ npm -g i npm-check
              $ npm-check -u -g
              $ npm-check -u

          Now, there's also a chance you may receive an error message complaining about a missing
          json package or something like that. In order to fix it, try this command, I had luck with
          it in the past

              $ npm init
                And just <enter> through all of the prompts
              $ npm install npm@latest -g
                give that a whirl if the first doesn't work

        }}}

        Packages {{{
          $ npm install <package>

          $ npm up
            update npm packages

          $ npm uninstall <package_name>
          $ npm uninstall -g <package_name>
            uninstall global package

          $ npm doctor
            does a checkup of sorts to verify that things are in order

          Old notes on some javascript related packages {{{
            $ npm install coffeescript
              If you plan on using it, obviously
            $ npm install jshint -g
              for javascript error messages in vim
              NOTE: Also recommended to put ~/.jshintrc file in $HOME directory. This has been added to
              .dotfiles repo

            ESlint
              Run this within the app's root folder
                $ npm install eslint --save-dev
                $ sudo eslint --init

              This is supposed to also take care of a local install too, but I'm the command doesn't
              seem to be working on the virtual machine...

              If, instead, you'd like to install it globally, use the following

                $ npm install -g eslint
                  for javascript error messages in vim, use with syntastic via syntastic-checker

          }}}

        }}}

        Delete npm completely {{{
          $ npm uninstall npm -g
          This will remove npm, but leave behind any npm packages you've installed. To delete those...
            "...Local installs are completely contained within a project's node_modules folder. Delete
            that folder, and everything is gone"

            $ sudo rm -rf /usr/local/{lib/node{,/.npm,_modules},bin,share/man}/npm*

            $ npm ls -gp --depth=0 | awk -F/ '/node_modules/ && !/\/npm$/ {print $NF}' | xargs npm -g rm
              NOTE: This is another command that should work as well, removes all globally installed
              packages

        }}}

        Troubleshooting {{{
          I ran into an issue repeatedly with the land_app wherein I was not able to to interact
          with npm through that its root directory without including `sudo`, it would constantly
          complain about not having proper access permissions the land_app/node_modules
          directory. In order to resolve this, I updated the pemissions to the directory with:

            $ sudo chown -R $(whoami) ./node_modules

          From that point on, I was able to run npm commands without sudo.

        }}}

      }}}

      pnpm  {{{
        Install {{{
          $ npm install -g pnpm

          Or, if you'd prefer a standalone script, use:
            $ curl -L https://unpkg.com/@pnpm/self-installer | node

          $ npm install @pnpm/logger@1
            NOTE: This package is a required dependency of pnpm for it to properly manage the symlinks

        }}}

        Use {{{
          As far as commands go, it basically works the exact same as npm, just replace npm with pnpm
          and you should be good to go for the majority of commands :)

          https://pnpm.js.org/docs/en/pnpm-cli.html

          $ pnpm import
            switch from npm to pnpm within application

        }}}

        Upgrade {{{
          $ pnpm i -g pnpm

        }}}

        Troubleshooting {{{
          ERR_PNPM_FETCH_404 {{{
            $ pnpm config set registry https://registry.npmjs.org

          }}}

        }}}

      }}}

      yarn  {{{
        NOTE: Same as with npm, you MUST avoid running any `yarn` commands as sudo. Doing so will cast you firmly into
        user permission hell.

          $ pnpm install -g yarn

          $ yarn init
            get yarn up and running in your app, run this in the root directory for the app
            NOTE: When I went to set up yarn on an LXLE distro, I ran into some issues. It turns out,
            there was a package called "cmdtest" on the system that was trying to "act like" yarn. To
            get around this, I simply uninstalled cmdtest and yarn through apt, and then reinstalled
            yarn with pnpm as above

              $ sudo apt remove cmdtest

          $ pnpm install -g yarn
            upgrade yarn

      }}}

    }}}

    Useful packages {{{
      https://browsersync.io/
        $ pnpm i -g browser-sync
        $ browser-sync start

      live-server
        $ pnpm i -g live-server
        $ live-server # run at root directory to start

    }}}

}}}

  Note taking {{{
    If you're looking for something separate from vim's tools like vimwiki, etc, then give these
    a look

      tomboy-ng
      gnote
      nvPy
        https://github.com/cpbotha/nvpy
      terminal velocity
      simplenote

  }}}

  Package Managers{{{
    cargo {{{
      This is a more go/rust-centric thing, but I'm adding it here as there are some cargo related
      packs you like to use :)

      https://doc.rust-lang.org/cargo/index.html
      https://doc.rust-lang.org/cargo/commands/index.html

      Install
        $ curl https://sh.rustup.rs -sSf | sh

      Update
        $ rustup update

        $ cargo install --list
        $ cargo install <crate>
          NOTE: Running install on a pre-installed crate will cause it to update

          Another way to approach it is through the following crate

            $ cargo install cargo-update
            $ cargo install-update -a
              # Updates all installed packages

    }}}

    dpkg {{{
      $ dpkg -l
      $ dpkg --get-selections
        List all installed

      $ dpkg -L <package>
        See where package is installed

    }}}

    flatpack {{{
      https://flatpak.org/
      https://flathub.org/home

      Similar to Snap in that it installs apps that are completely isolated from your main system,
      but evidently a LOT better than snaps.

      One of the downsides however is that due to the nature in how the packages are managed, they
      can take up a bit more space. And from what I can tell, this is primarily because they allow
      for a kind of "independent" cluster of dependencies. So when you download app XYZ, you're also
      getting an isolated/independent set of dependencies for JUST that app. And when you get app
      XYZ2, even though it may use some of the same dependencies, since they're not "globally"
      avaialble systemwide, it has to download its "own" version to use.

      $ sudo apt install flatpak
      $ sudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo

      $ flatpak search <app>
      $ flatpak install flathub <app ref>
      $ flatpak run <app>
      $ flatpak list
      $ flatpak update
      $ flatpak uninstall <app>
      $ flatpak uninstall --unused
        Gets rid of unneeded depedencies

    }}}

    pacman {{{
      UPDATE: Supposedly there's a popular wrapper called powerpill that adds a lot of cool features
      and stuff to pacman's already existing skin and bones.

        $ sudo pacman -Syu powerpill

      And from there on out, you can simply preface the below with powerpill rather than pacman

      https://wiki.archlinux.org/index.php/pacman

      $ pacman -Sh                 # show help

      $ sudo pacman -Ss <package>  # search for package
      $ sudo pacman -Si <package>  # show detailed info for package
      $ sudo pacman -Sg <group>    # show packages pertaining to a certain group

      $ sudo pacman -S <package>   # install package
      $ sudo pacman -Syu           # update/upgrade all packages
      $ sudo pacman -Syu <package> # install package and update/upgrade all packages

      $ sudo pacman -R <package>   # remove package
      $ sudo pacman -Rs <pacakge>  # remove package and dependencies not used by other packages

    }}}

    pip3  {{{
      NOTE: My understanding when working with pip is that you should **NOT** prefix your commands
      with `$ sudo`, because this causes pip to install into some system type level rather than on
      the current user level.

      UPDATE: Okay, so here's something else that's interesting. Seeing that I haven't been able to
        get Python3 to "work" with a +python3 compile of vim, I did some more reading. And because
        of headaches like the one I'm experiencing here, some advocate doing away with all of the
        "fancy" third-party stuff altogether and simply relying on python's built-in tools -pip and
        venv- to help manage these sorts of things. So I'm going to backtrack and try that route. If
        for any reason I want to "go back" to using something like pyenv, its setup is really easy
        so I'm not about getting things back to where I was when I decided to cut bait.
        Though I wonder... Could it have been because I didn't aslo pass the shell related
        invocations to bash_profile? Hmmm...

          $ pyenv versions
          $ pyenv uninstall <version>
            NOTE: The <tab> complete stuff of linux works with this as well. High five linux!

        To disable pyenv, remove all pyenv related invocations from your shell startup config files.

        If you want to go one step deeper and actually get rid of pyenv as well, along with removing
        the pyenv invocations as mentioned above, just remove the pyenv directory

          $ rm -rf $(pyenv root)

        NOTE: After running into multiple recurring headaches surrounding installing and maintaning
        python and its packages, notices about "site-packages is not writeable", blah blah
        blah... I've since learned that it's commonly recommended to use some sort of python version
        manager. This helps get around the pesky sudo/non-sudo stuff and can also help manage
        dependencies a bit better.

        So, on that front, a lot of people seem to prefer pyenv to manage python versions.  And when
        it comes to python related packages, if you want or need to get fancy with having isolated
        pip stuff, you can use something like pipx or pipenv. BUT! This, for the "normal" user, I've
        seen many recommend to simply stick with "basic" pip/pip3 since it is widely used and
        understood and thus, readily provides help, etc.

        One thing that's important is to NOT mess with any python install that comes default with
        your distro/system. Just let that be there and do its thing, ignore that it even exists and
        just continue to skirt around it with these other approaches.

        TODO: I am in the process of going through the motions, will update the following
        install/update etc related categories below as I understand them :) My main reference for
        the time being is here: https://realpython.com/intro-to-pyenv/

      Installing python and pip  {{{
        pyenv {{{
          NOTE: aliased to `$ installpyenv` {{{
            $ sudo apt install -y make build-essential libssl-dev zlib1g-dev \
            libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \
            libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl

            $ curl https://pyenv.run | bash

          }}}

          https://github.com/pyenv/pyenv/blob/master/COMMANDS.md

          $ pyenv install -l | grep " 3"
          $ pyenv install 3:latest

          $ pyenv version
          $ pyenv versions
          $ pyenv global
          $ pyenv global <desired_version>
            NOTE: Once you've set a global version, this version should be returned whenever you run
            the standard `$ python --version` command

          $ pyenv root
          $ pyenv prefix

        }}}

        Manual {{{
          Pip comes in several different flavors depending on what version of python you want it to
          use.

          NOTE: If you run with the synaptic/apt version, you must rely on that package system for
          any and all upgrades. If you instead install pip through some other means, you may be able
          to stay more up to date by essentially circumventing synaptic's gatekeeping stuff

            $ sudo apt install phython3-pip # python 3

            OUTDATED: $ sudo apt install python-pip # python 2
              NOTE: As of 20200820, it sounds like python 2.x is considered to be old hat and no
              longer used. Go with 3 instead. It's important to note that python 3 seems to use the
              alias `pip3` for its commands rather than `pip` as you were previously used to.

        }}}

      }}}

      Upgrading pip  {{{
        UPDATE: Okay, so evidently when you want to upgrade your system install of pip, some say
        that you need to rely on the same package installer you used for its initial, which in most
        cases, will be through something like synaptic. I'm still not 100% sure on how to handle
        this though...
          UPDATE: Okay so here's how I understand it now. There is a version of Python that comes
          installed with the SYSTEM/OS. And for compatibility reasons, it's advised to kind of leave
          this one alone. Let it be managed completely by the OS/distro's own update/upgrade
          processes.
          NOW! That said, it's likely that you'll want to install and have available a more current
          version of Python. And that's where pip comes in. You basically add in new python versions
          as pip related packages rather than try to overwrite or fuss-with the in-built system
          python version. Does that make sense?
            NOTE: At least I think that's handled through pip (???) You might get it from
            somewhere else, but the overall gist is correct.

        Here's some light reading on the issue:

          https://github.com/pypa/pip/issues/5599
          https://stackoverflow.com/questions/60029215/warning-pip-is-being-invoked-by-an-old-script-wrapper

        Others say you're free to treat it just as you would any other python package and upgrade
        through pip3 itself

          $ pip3 install --upgrade pip

      }}}

      Managing packages  {{{
        $ pip3 show <package> # view installed version of package
        $ pip3 list --outdated --format columns # view outdated packages
        $ pip3 install --upgrade <package_name> # upgrade named package
        $ pip3 uninstall <package_name> # uninstall named package

        UPDATE: The lack of a 'universal' upgrade-all type command may be intentional here. Because
        the packages themselves may be finicky in that for all you know, the next upgrade could
        break some things or introduce some headaches here and there. For this reason, it's
        recommended that you only upgrade what you need to be upgraded. Is there a reason you want
        to upgrade package xyz? A certain feature or fix that would benefit you? If not, it's likely
        best to leave it alone and "don't fix what ain't broken".

        As of this writing, upgrading all packages at once is a little more tricky... For best
        results, upgrade them individually using the above commands. That said, here is one command
        some have said works for them, though I couldn't get it to take on my end...

          $ pip3 list outdated --format=freeze
          $ pip3 list outdated --format=freeze | grep -v '^\-e' | cut -d = -f 1 | xargs -n1 pip install -U
            # Aliased to
            #   $ pip3outdated
            #   $ pip3updateall

      }}}

      Uninstalling pip completely {{{
        https://askubuntu.com/questions/969463/python3-pip3-install-broken-on-ubuntu

        NOTE: Throughout the following, I recommend removing the directories through pcmanfm or
        something other than rm -rf because if you make a mistake, there's no turning back muchacho

        $ sudo apt purge python3-pip
        $ sudo rm -rf '/usr/lib/python3/dist-packages/pip'
        $ sudo apt install python3-pip
        $ cd .local/lib/python3/site-packages
        $ sudo rm -rf pip*
        $ cd .local/lib/python3.5/site-packages
        $ sudo rm -rf pip*
        $ python3 -m pip install --user xlwt

        Another offered approach
        $ rm /user/bin/pip3 /user/local/bin/pip3

      }}}

      Troubleshooting  {{{
        sudo woes {{{
          After unknowingly trying to upgrade pip with the `$ sudo` prefix, I started receiving the
          following error message:

            "ImportError: cannot import name 'main'"

          Diving deep into the rabbit hole led me to the following command that seemed to fix the
          issue:

            https://stackoverflow.com/questions/49836676/error-after-upgrading-pip-cannot-import-name-main
            $ sudo python2 -m pip uninstall pip && sudo apt install python-pip --reinstall

        }}}

        ImportError {{{
          I ran into some problems when installing packages, it would say "ImportError: No module
          named pathlib". A possible fix for this is to run, but so far, I haven't been able to get
          it to work...
            UPDATE: I'm pretty sure you can disregard using the below. See your notes on the
            install, how pip3 is preferred

            $ sudo pip install -U setuptools
            $ sudo pip install -U pip

          Now, saying that, when I found the solution to the above problem that actually worked,
          here's what it entailed

            https://stackoverflow.com/a/30993148/5474201

            $ locate pip3
              If it complains about not having `mlocate`, follow the prompt to install it
            < the above command should return a result that looks something like >

              $ /<path>/pip3
              $ /<path>/pip3.x.gz

            To "fix" the issue, you need to create a symbolic link associating the two together. In
            your case, it looked like

              $ ln -s /usr/share/man/man1/pip3.1.gz /usr/local/bin/pip3

            Then, from that point on, everything pip3 related seemed to work like a charm!

        }}}

        "not found" after installing packages {{{
          Make sure the following is in your .bashrc

            export PATH="$HOME/.local/bin:$PATH"

        }}}

      }}}

    }}}

    snapd {{{
      UPDATE: I'm going to start steering away from Snap. I noticed, for example, that installed
      packages seem to be in a semi-loaded state upon launch, consuming system resources even when
      the apps are not in use. Not sure what it's doing...

      NOTE: One of the main complaints against Snap is that, contrary to official distribution
      platforms like apt, its packages do not undergo any sort of review process before being
      included. For this reason, snap packages may be  more likely to introduce security and/or
      stability concerns.

      One of its main advantages though is that Snap packages are fully self-contained, which means
      they do not have to mess with any of the dependency issues that can sometimes plague apt
      packages. In this way, snap packages are fully sandboxed from the main system, which is pretty
      cool. HOWEVER! That said, evidently there are some rather 'easy' ways to get around this if
      someone really wanted to. So the sandboxing feature may not be all it's cracked up to be.

      $ sudo apt install snapd
      $ sudo snap install <package_name>
      $ sudo snap list
      $ sudo snap find <package_name>
      $ sudo snap refresh --list # lists packages with available updates
      $ sudo snap refresh # updates all packages
      $ sudo snap refresh <package_name> # updates named package
      $ sudo snap remove <package_name>

    }}}

    synaptic apt {{{
      NOTE: What's interesting about apt is that evidently it's a wrapper-like program around the
      dpkg package installer/manager

      https://help.ubuntu.com/community/AptGet/Howto?action=show&redirect=AptGet

      $ sudo apt -u -V upgrade
        # list upgradeable packages with their versions

      $ sudo apt --only-upgrade install <package>
        # upgrading only named packages

      Listing packages {{{
        $ sudo apt list '<package name>'

      }}}

      Updating packages {{{
        NOTE: All of the below has been condensed into a .bashrc alias

          `$ update`

        $ sudo apt update {{{

        $ sudo apt upgrade
          does not do a dependency check on what is installed, just runs a "raw/dumb" upgrade to
          latest versions

        $ sudo apt dist-upgrade
          "smart" upgrade that may add or remove packages based on new dependency requirements

          NOTE: If, after running apt upgrade you see the message that some things have been
          held-back, that's a good indication that you should then run apt dist-upgrade. Some users
          always run dist-upgrade in place of apt upgrade, but note that while it is generally very
          safe in how it handles things, the fact that packages may be removed can -- in some off
          circumstances -- make the system unstable until they are resolved. Others, however, claim
          that dist-upgrade is MORE secure since it also checks for dependencies... So who knows?

        $ sudo do-release-upgrade
          If you need to do a major upgrade, say from Lubuntu 16.04 to 18.04, this command may help

        $ sudo apt install gtkorphan
          useful for finding and removing orphaned packages

          $ sudo gtkorphan

        NOTE: The following two commands can be used from time to time. You should receive a notice
        after an upgrade to run autoremove to get rid of now outdated stuff. The autoclean one,
        though, that one is a little different and should be ran from time to time to purge your
        repo of files that are no longer available.

            $ sudo apt autoremove
            $ sudo apt autoclean
            $ sudo apt clean

        The above steps can be condensed into a single command

          $ sudo apt update && apt dist-upgrade && apt autoremove && apt autoclean
            NOTE: Aliased to `$ sudo update`

        NOTE: If you ever want to reinstall something, use this syntax

          $ sudo apt install --reinstall <package_name>

        $ sudo apt install apt-rdepends # lists dependencies for applications

        }}}

      }}}

      Uninstalling packages {{{
        $ sudo apt --purge autoremove <name>

        In the GUI mode, when you right click a package, here what the different removal options
        mean

          Removal = remove package
          Complete Removal = remove package along with any associated configuration files

      }}}

      Troubleshooting and Issues {{{
        Broken packages {{{
          First, try it through the GUI. There's an actual option in one of the drop-down menus to
          fix broken packages. Give that whirl... If it's still not acting right, try these steps

          Remove the package altogether then try installing it again. If a ppa, use one of the
          following:

            $ ls /etc/apt/sources.list.d
            $ sudo rm -i /etc/ap/sources.list.d/<ppa file name>
            $ sudo apt update

            $ sudo add-apt-repository <ppa address> --remove

            $ sudo apt install ppa-purge
            $ sudo apt update
            $ sudo ppa-purge <ppa address>
              $ sudo ppa-purge ppa:me-davidansome/clementine

          Another set...
            $ sudo apt clean
            $ sudn apt install -f
            $ sudo dpkg --configure -a
            $ sudo apt update
            $ sudo rm /var/lib/apt/lists/* -vf
            $ sudo apt update

          Other things you can try (these are each separate commands to try, not necessarily meant
          to be linked together):

            $ sudo apt update --fix-missing
            $ sudo apt -o Debug::pkgProblemResolver=yes dist-upgrade
            $ sudo dpkg --configure -a --force-all
            $ sudo dpkg -i --force-overwrite <file_path>
              Not so drastic, though it involves file directories
            $ sudo dpkg --remove --force-remove-reinstreq <package_name>
              Be extremely careful with this one, it's known as the nuclear option, and evidently it
              can really bork your system if you're not careful or don't know what you're doing

          If none of these options work, then the only recommendation I really saw was to restart
          from scratch by reinstalling the OS. Since packages are at the heart of how Linux runs as
          an OS, if you're unable to get them to work properly, it's definitely a bum deal. In this
          instance, I feel that this may have happened in how I tried to uninstall the other desktop
          environments... But that's just a wild guess!

        }}}

        To remove repos that cannot be found or are no longer compatible, or whatever... {{{
          There's probably a command-line way of doing this, but the only way I'm aware of for now
          is through the GUI interface.

          Here's what you do:

            Synaptic Package Manager -> Settings -> Repositories -> Other Software -> Click on what
            you want to remove -> Select "Remove" at the bottom

        }}}

      Package manager not starting? {{{
        $ sudo dpkg --configure -a
        $ sudo apt autoclean
        $ sudo apt clean
        $ sudo synaptic
          This should launch it from the terminal
        $ xhost +si:localuser:root
          This may fix it too. Run this command, then log out and log back in
        Another thing you can try is reinstalling the LXDE environment and or reinstalling the
        actual synaptic package manager itself

      }}}

        Concerned you may have dependency problems? {{{
          $ sudo apt check
            NOTE: For some reason, this may return "E: Invalid operation check".
            If it does, then try running

              $ sudo apt-get check

          $ sudo dpkg --audit

        }}}

      }}}

    }}}

  }}}

  Partial upgrade errors  {{{
    sudo apt dist-upgrade
      May walk you through additional steps to correct the issue
    gksudo synaptic
    sudo apt install -f

  }}}

  PATH {{{

    http://www.linfo.org/path_env_var.html {{{

      PATH is an environmental variable in Linux and other Unix-like operating systems that tells
      the shell which directories to search for executable files (i.e., ready-to-run programs) in
      response to commands issued by a user. It increases both the convenience and the safety of
      such operating systems and is widely considered to be the single most important environmental
      variable.

      Environmental variables are a class of variables (i.e., items whose values can be changed)
      that tell the shell how to behave as the user works at the command line (i.e., in a text-only
      mode) or with shell scripts (i.e., short programs written in a shell programming language).
      A shell is a program that provides the traditional, text-only user interface for Unix-like
      operating systems; its primary function is to read commands that are typed in at the command
      line and then execute (i.e., run) them.

      PATH (which is written with all upper case letters) should not be confused with the term path
      (lower case letters). The latter is a file's or directory's address on a filesystem (i.e., the
      hierarchy of directories and files that is used to organize information stored on a computer).
      A relative path is an address relative to the current directory (i.e., the directory in which
      a user is currently working). An absolute path (also called a full path) is an address
      relative to the root directory (i.e., the directory at the very top of the filesystem and
      which contains all other directories and files).

      A user's PATH consists of a series of colon-separated absolute paths that are stored in plain
      text files. Whenever a user types in a command at the command line that is not built into the
      shell or that does not include its absolute path and then presses the Enter key, the shell
      searches through those directories, which constitute the user's search path, until it finds an
      executable file with that name.

      The concentrating by default of most executable files in just a few directories rather than
      spread all over the filesystem and the use of the PATH variable to find them eliminates the
      need for users to remember which directories they are in and to type their absolute path
      names. That is, any such program can be run by merely typing its name, such as ls instead of
      /bin/ls and head instead of /usr/bin/head, regardless of where the user is currently working
      on the filesystem. This also greatly reduces the possibility of damage to data or even to the
      system as a whole from the accidental running of a script that has the same name as a standard
      command.

      A list of all the current environmental variables and their values for the current user,
      including all the directories in the PATH variable, can be seen by running the env command
      without any options or arguments (i.e., input data), i.e.,

        env

      As there can be considerable output, it can be convenient to modify this command so that it
      displays just the PATH environmental variable and its value. This can be accomplished by using
      a pipe (represented by the vertical bar character) to transfer the output of env to the grep
      filter and use PATH as an argument to grep, i.e.,

        env | grep PATH

      Another way to view the contents of just PATH alone is by using the echo command with $PATH as
      an argument:

        echo $PATH

      echo repeats on the display screen whatever follows it on the command line. The dollar sign
      immediately preceding PATH tells echo to repeat the value of the variable PATH rather than its
      name.

      Each user on a system can have a different PATH variable. When an operating system is
      installed, one default PATH variable is created for the root (i.e., administrative) account
      and another default is created that will be applied to all ordinary user accounts as they are
      added to the system. The PATH variable for the root user contains more directories than for
      ordinary users because it includes directories, such as /sbin and /usr/sbin, that contain
      programs that are normally used only by that user.

      PATH variables can be changed relatively easily. They can be changed just for the current
      login session, or they can be changed permanently (i.e., so that the changes will persist
      through future sessions).

      It is a simple matter to add a directory to a user's PATH variable (and thereby add it to the
      user's default search path). It can be accomplished for the current session by using the
      following command, in which directory is the full path of the directory to be entered:

        PATH="directory:$PATH"

      For example, to add the directory /usr/sbin, the following would be used:

        PATH="/usr/sbin:$PATH"

      An alternative is to employ the export command, which is used to change aspects of the
      environment. Thus, the above absolute path could be added with the following two commands in
      sequence

        PATH=$PATH:/usr/sbin export PATH

      or its single-line equivalent

        export PATH=$PATH:/usr/sbin

      That the directory has been added can be easily confirmed by again using the echo command with
      $PATH as its argument.

      An addition to a user's PATH variable can be made permanent by adding it to that user's
      .bash_profile file. .bash_profile is a hidden file in each user's home directory that defines
      any specific environmental variables and startup programs for that user. A hidden file is
      a file whose name begins with a dot (i.e., a period) and which is normally not visible;
      however, it can be seen by using the ls (i.e., list) command with its -a (i.e., all) option.

      Thus, for example, to add a directory named /usr/test to a user's PATH variable, it should be
      appended with a text editor to the line that begins with PATH so that the line reads something
      like PATH=$PATH:$HOME/bin:/usr/test. It is important that each absolute path be directly
      (i.e., with no intervening spaces) preceded by a colon.

      It is sometimes desired to run a script or program which has been installed in a user's home
      directory or some other location that is not in the user's default search path. Such script or
      program can, of course, be run by typing in its absolute path. But an often more convenient
      alternative when the script or program is in the current directory is to merely precede the
      command name with a dot slash (i.e., a dot followed by a forward slash and with no intervening
      spaces). The dot is used in paths to represent the current directory and the slash is used as
      a directory separator and to separate directory names from file names.

      MS-DOS also uses a PATH variable. However, it differs from Unix-like operating systems in that
      it searches the user's current directory before it searches in any directories in that
      variable.

      1 An extreme example would be the situation in which an ordinary user created a shell script
        such as rm -r /, which would delete all files and directories in the system for which the
        user had writing permission, and named this script ls. Were the system administrator to
        navigate to the directory in which this script was located and attempt to run the standard
        ls command in order to view the contents of that directory, the shell would instead run the
        script with the same name and thereby remove the contents of all currently mounted
        partitions on the computer!

    }}}

    $ echo $PATH
      This displays the directories currently "known" by the PATH variable, separated by colons. If
      you ever want to change this in any way, say to add or remove a directory, there are two ways
      to go about it...

    $ export PATH="$PATH":/directory/to/add
      This adds the directory for a single bash shell session. To make it permanent, you'd add the
      same line to one of the following system files:

        ~/.bashrc
        ~/.bash_profile

  }}}

  Pomodoro Stuff {{{
    $ sudo apt install gnome-shell-pomodoro
      This is the one you normally use, that covers the screen when time is reached
      $ gsettings set org.gnome.pomodoro.plugins.gnome indicator-type text
        text, short-text, or icon (default)
        NOTE: Okay so here's the low-down on this setting... Turns out the text feature only works
        if you're using the gnome desktop environment, which is different from the Lubuntu desktop
        environment. It is possible to run Ubuntu in several other desktop environments, including
        gnome.

    If you want a minimalist approach without having to use an app

        $ sleep 1500 $$ notify-send "Break Time!"
          This results in a small popup

      You can even use something called zenity to have a sticky type notification, like this:

        $ sleep 1500 && zenity --warning --text="25 minutes passed" # for more info,'$ man zenity'

      For more info on how desktop notifications work on Linux, check out

        http://www.thegeekstuff.com/2010/12/ubuntu-notify-send/

    $ pip3 install termdown
    $ sudo snap install termdown
      https://github.com/trehn/termdown
        $ termdown <seconds> -b
          adds blinking when finished
        $ termdown <seconds> -v ENGLISH
          adds count-down voice. Or, at least it's supposed to... I can't seem to get it to work
          consistently. I do know I've had it work a few times before, but I can't remember what
          I did exactly...

          $ termdown 1500 -b -v ENGLISH

      $ sudo apt install espeak
        This adds the ability for termdown to count down the final critical seconds in a spoken
        voice. To test that espeak was installed properly, pass it a phrase and see if it is spoken

          $ espeak "Testing testing one two three"

    $ sudo apt install tomate-gtk
      may want to use the following plugins
      $ sudo apt install tomate-alarm-plugin
                            tomate-notify-plugin
                            tomate-indicator-plugin
                            tomate-statusicon-plugin
                            tomate-launcher-plugin
                            tomate-exec-plugin
    www.pomodoneapp.com
    www.tomato-timer.com
    www.tomatoid.com
    https://bitbucket.org/dvtomas/osd-pomodoro

  }}}

  Prompts {{{
    I'm not entirely sure what these do... It seems like the prompt is the meat and potatoes that
    goes on behind the scenes, whereas the terminal is what you use to actually interact
    with/display the prompt stuff. So whenever I installed the starship prompt, I noticed the
    terminal output looked a little different. What's the advantage of this? I have no idea...

    Here are some great resources you can use to quickly and easily build out a custom prompt line:

      https://scriptim.github.io/bash-prompt-generator/
      http://ezprompt.net/
      https://terminal.sexy/

    spaceship-prompt {{{
      https://github.com/denysdovhan/spaceship-prompt

    }}}

    starship {{{
      https://github.com/starship/starship

      Install
        $ curl -fsSL https://starship.rs/install.sh | bash
        $ cargo install starship

      Uninstall
        $ carg uninstall starship
        $ sudo rm "$(which starship)"
        disable the eval statement in .bashrc

    }}}

  }}}

  processes currently running, how to kill etc {{{
    $ ps aux
    $ ps aux | grep <name>
    $ sudo kill -15 <PID>

    $ pidof <options> <process name>
      options:
        -c only PIDs within a single root dir
        -o omit certain PIDs listed after flag
        -s only return single pid
        -x also return pids of shells running scripts

    If sending the "terminate" signal doesn't work, then pull out the big guns and use

      $ sudo kill -9 <PID>

    To see all available signal commands

      $ kill -l

    To kill ALL related to a named process, but be sure the name is exact!

      $ killall <process_name>

    You ran into an "issue" with a particular part of `mono-develop` that would always have
    a process start on boot. This led you discover some other interesting ways to control actively
    running processes along with setting up which ones run at startup, etc.

      $ ps aux | grep <all or part of name>
        As always, good to confirm the name just to be sure
      $ sudo systemctl stop <precise service name>
      $ sudo systemctl disable <precise service name>
        NOTE: `stop` attempts to kill actively running but does NOT prevent running at next boot
              `disable` does NOT kill actively running, but does prevent running at next boot
              Which means you typically run BOTH commands back to back

      $ update-rc.d <precise service name> disable
        This is another command that disables automatic startup on boot

    To see a list of programs that automatically start on boot

      Upstart
        $ sudo initctl list
      System V
        $ sudo service --status-all
        $ ls /etc/init.d/  (for all init scripts)
        $ ls /etc/rc*.d/  (for runlevel symlinks)
      SystemD
        $ sudo systemctl --all list-unit-files --type=service
        $ ls /lib/systemd/system/*.service /etc/systemd/system/*.service

  }}}

  Resource and boot managers  {{{
    $ ps aux
    $ ps -eF
    $ ps -fU

    $ pstree

    https://github.com/brendangregg/perf-tools#contents

    Current faves
      htop
      btop, with TTY enforced if you want to go more minimal
      glances

    $ sudo apt install top # There's a good chance this is already preloaded with the OS
      https://bencane.com/2012/08/06/troubleshooting-high-io-wait-in-linux/
    $ sudo apt install iotop
      If you're seeing a lot of IO usage by "spring app" chances are its your server running in the
      background. Often, simply restarting the server brings it back in line `$ service postgresql
      stop` then `$ service postgresql start`
    $ sudo apt install htop
      $ sudo snap install htop
      Similar to vtop, different layout and feature set, more up to date
    $ sudo snap install bpytop
      You then need to run the following commands to get it work properly
        $ sudo snap connect bpytop:mount-observe
        $ sudo snap connect bpytop:network-control
        $ sudo snap connect bpytop:hardware-observe
        $ sudo snap connect bpytop:system-observe
        $ sudo snap connect bpytop:process-control
        $ sudo snap connect bpytop:physical-memory-observe
    $ sudo apt install atop
    $ nvm install -g vtop # Really cool, but it is a bit more resource intensive
      UPDATE: if you're getting Syntax errors when running the above version of vtop, try the
      following

        $ pnpm uninstall -g vtop
        $ pnpm install -g vtop-node10

        https://github.com/MrRio/vtop/issues/121

      If, when installing vtop, nmp complains that it runs in an older version of minimatch... Not
      sure how to fix...

      $ npm update -g minimatch@3.0.2
      $ npm update minimatch
        This may also work
      $ npm install -g npm
        Another way, will update npm itself
      $ npm view minimatch version
      # Change the themes with `$ vtop -t <theme>`
        acid, becca, brew, dark, monokai, parallax, seti, wizard
        seti is my fav, I created an alias in .bashrc so it is called by default
    $ sudo apt install conky-all
      run with $ conky &
        NOTE: The ampersand runs linux apps in background mode.
        Similar to other apps, conky also uses a dotfile for its configuration. And believe me, this
        thing is HIGHLY configurable! There's supposedly a gui version available here, but it said
        it can't find the package:

          $ sudo apt-add-repository -y ppa:teejee2008/ppa
            UPDATE: Do NOT go this route, try to get it another way, maybe through git?
          $ sudo apt update
          $ sudo apt install conky-manager

    $ tload
      comes baked right to linux, shows a simple graph
      ^C to quit
    $ top
      similar to htop, but comes with linux out the of the box
    $ tcpdump
      network packet analysis
    $ netstat
      network stats
    $ sudo apt install nmon
      <q> to quit
    $ sudo apt install glances
      Or, if you want a more recent version, you can probably get it through pip3 or snapd
      $ pip3 install glances
      $ sudo snap install glances
    $ npm install gtop -g
      similar to vtop in resource usage
    $ free -m
      available column shows how much free ram you "really" have, without swap files etc being taken
      into account
    $ sudo apt install wireshark
      network stats
    $ sudo apt install pybootchartgui
      UPDATE: Has since merged into https://github.com/xrmx/bootchart

}}}

  rsync {{{
    great option for managing backups and file transfers

  }}}

  Screensavers {{{
    $ sudo apt install xscreensaver

    https://www.linuxbabe.com/ubuntu/install-autostart-xscreensaver-ubuntu-16-04-16-10
    To get Xscreensaver to autostart, use the following:

      $ sudo gedit /etc/xdg/autostart/screensaver.desktop

      Then save the following into the opened file:
        [Desktop Entry]
        Name=Screensaver
        Type=Application
        Exec=xscreensaver -nosplash

      From here, I ran the following command to see WHERE the xscreensaver stuff is kept
        $ which xscreensaver

      Then I went into the Startup Applications Preferences menu, and edited the Screensaver entry,
      Command --> Browse, then clicked the xscreensaver file found in that same directory.

      Some say that removing the gnome-screensaver can help avoid any conflicts too
        $ sudo apt remove gnome-screensaver

}}}

  Screenshot tools  {{{
    $ sudo apt install gnome-screenshot

    $ sudo apt install xfce4-screenshooter

  }}}

  screen recording tools {{{
    asciinema
      $ sudo apt install asciinema

    kazam
      $ sudo apt install kazam

    simple screen recorder
      $ sudo apt install simplescreenrecorder

  }}}

  Slack  {{{
    $ sudo snap install slack --classic

  }}}

  power management and sleep {{{
    auto-cpufreq
      https://github.com/AdnanHodzic/auto-cpufreq

      $ git clone https://github.com/AdnanHodzic/auto-cpufreq.git
      $ cd auto-cpufreq && sudo ./auto-cpufreq-installer
      $ sudo auto-cpufreq --monitor
        NOTE: This shows you what auto-cpufreq could change when ran
      $ sudo auto-cpufreq --live
        NOTE: This allows you to test it out to make sure it works
        If everything checks out, then proceed with the install
      $ sudo auto-cpufreq --install
      <reboot system>
      $ systemctl status auto-cpufreq
        NOTE: This lets you confirm that it IS in fact running quietly in the background
      $ auto-cpufreq --stats
        NOTE: When it's running, allows you to check on its status

    powertop
      $ sudo apt install powertop

      First calibrate, restart, calibrate again, then you should be able to run auto-tune
        $ sudo powertop --calibrate
        $ sudo powertop --auto-tune

      $ sudo powertop --html
        Supposedly this creates a sort of editable config file that you can then use to
        automatically start at system boot with those settings
          https://askubuntu.com/questions/112705/how-do-i-make-powertop-changes-permanent

    $ sudo pm-suspend

    $ systemctl suspend -i

  }}}

  Sound manager {{{
    $ alsamixer

    $ \nmvim /etc/pulse/daemon.conf
    Make sure the default-sample-rate is set properly, something like 48000

  }}}

  Stow  {{{
    This is the tool you're currently using to manage your dotfile repos
    $ man stow

    Some quick notes... You've set up ~/dotfiles as a Stow enabled directory. This directory then
    acts as a sort of holding-pen for all files and directories you want Stow to manage. This means
    that on a fresh install, after you pull down your dotfiles repo, you'll have to `$ stow <xyz>`
    each of your dotfiles directories in order to bring them into your ~ directory so you can rock
    and roll.

    Since you save Stow and its packages within your $HOME/dotfiles directory, Stow will target
    the $HOME directory by default. In those instances where you want Stow to target a different
    directory (for example, your logrotate stuff needs to go in the /etc/logrotate.d directory),
    you include the `-t /` option as part of the stow command. And since it's going into the
    /etc directory, you also need to preface it with `sudo`

      $ sudo stow logrotate -t /

    Here's the overall gist as I understand it so far.
      You put the files themselves within the Stow enabled directory and then "disperse" them into
      your normal file system.
        -- For example, I wanted my dotfiles git repo (which uses Stow) to also include a .vifm
        directory which itself contained a .vifmrc file. So what I did was go into the ~/dotfiles
        directory and create vifm/.vifm/.vifmrc The catch here is to create a "parent" folder so you
        have something for stow to reference.
        -- Then I ran `$ stow vifm` This grabbed the "parent" vifm folder in the stow enabled
        directory and output its contents into the ~/ directory with symbolic links. Now,
        technically I ran this command while I was in the dotfiles directory but I don't know if
        that's required in order for stow to run.

      Once the symlinks are created, you can interact with the symlink files as you would any other
      file. So if you want to edit the .reference file, for example, you don't have to use the
      dotfiles/reference/.reference file explicitly, you can simply use the ~/.reference file and it
      knows what to do.

      If you ever want to remove a symbolic link

        $ cd ~/dotfiles
        $ stow --delete <directory>

      Note this does not delete the files from the stow-enabled directory, it only removes their
      symbolic links (which is a good thing!).

      So for example, if I wanted to remove the vifm stuff I added in the above example, I'd use

        $ stow --delete vifm


  }}}

  system info {{{
    $ lsb_release -a

    neofetch {{{
      $ sudo apt install neofetch

    }}}

  }}}

  Tasksh  {{{
    $ tasksh
      start tasksh session, view command list. Once you're in the tasksh environment your prompt
      will be
        $ tasksh>

    Here are some of the commands you can run from within the $ tasksh> prompt
    $ tasksh> ...
        review N
          start reviewing your available tasks, with N being the number of tasks you'd like to
          review this session (leave blank if you want to call your own stopping point). You can
          quit and resume at any time. The purpose of the task review session is to interactively
          look over your tasks and either "okay" them or make any needed edits.

        projects
          view project list

  }}}

  Taskwarrior    {{{
    https://taskwarrior.org/

    NOTE: Just as an FYI, there are some alternatives out there that might pique your interest

      Calcurse {{{
        Doesn't seem as robust and feature-rich as TW, but putting it here just in case

        $ sudo apt install calcurse

      }}}

    ETM {{{
      https://github.com/dagraham/etm-dgraham

    }}}

    Tasklite {{{
      It's much more transparent and open source, built by someone who also LOVED taskwarrior, but
      found himself either not agreeing with some of their decisions or wanting a slightly different
      workflow. It also sounds like the author simply liked Haskel as a programming language and
      wanted to 'rebuild' a better taskwarrior in a language that he considers to be far
      superior. But overall it is very similar in how it functions and in how it looks, but
      missing a few features at the moment so don't expect parity just yet :)

          https://tasklite.org

      Can easily transfer/migrate your taskwarrior stuff into tasklite with the following:

          $ task export rc.json.array=off | while read -r task; \
            do echo $task | tasklite importjson; \
            done

    }}}

    Now, back to your regularly scheduled hullabaloo with Taskwarrior :)

    Install (now with git) {{{
      $ sudo apt install taskwarrior

      $ sudo apt install timewarrior
        adds a time tracking component to TW

        After initial sudo install, you then need to run the following commands to have Timewarrior
        and Taskwarrior sync up:
          UPDATE: The below steps have since been consolidated into the alias

            $ initialinstallstow

          $ mkdir ~/.task/hooks
            if the directory doesn't already exist
          $ sudo cp /usr/share/doc/timewarrior/ext/on-modify.timewarrior ~/.task/hooks/
          $ sudo chmod +x ~/.task/hooks/on-modify.timewarrior
          $ task diagnostics
            will show it the timwarrior hook is properly installed and recognized

        Timewarrior useage: Timewarrior integrates with Taskwarrior's start and stop commands as
        well as offering its own suite of commands that start with `$ timew` (check the Timewarrior
        section for more info on any commands you've found useful)

      $ sudo apt install tasksh
        this is a really cool taskwarrior shell that let's you interactively review your tasks
        https://taskwarrior.org/docs/review.html

      It can also have various scripts/plugins installed for it too to give it greater functionality
        taskopen
          allow a task, when accessed, to also open a related file
        bugwarrior
          allow taskwarrior to sync with lots of other platforms, like Trello, etc
        patata
          integrate timekeeping with pomodoro cycles

          https://www.reddit.com/r/pomodoro/comments/a0897q/a_pomodoro_timer_for_the_shell_works_with/

      There are even ways to integrate it into your tmux and/or vim sessions (even with the cool
      vimwiki plugin you like). These plugins have been added into your .vimrc, though may not be
      enabled. The ~/.taskrc config file and resulting ~/.task directory have been added to your
      .dotfiles repo

      Git
        UPDATE: This has been aliased to

          $ gitinstalltaskwarrior <version> # defaults to 2.6.0

        $ sudo apt install cmake uuid-dev
        $ cd /usr/share
        $ sudo git clone --recursive -b 2.6.0 https://github.com/GothenburgBitFactory/taskwarrior.git taskwarrior260
        $ cd taskwarrior260
        $ sudo cmake -DCMAKE_BUILD_TYPE=release .
        $ sudo make
        $ sudo make install

    }}}

    Resources {{{
      Official documentation
        https://taskwarrior.org/docs/introduction.html

      Video series
        https://www.youtube.com/playlist?list=PLI4gKGOkRTj37BHpZnFBVHDsa3J-dAUVF

      Playground for tools old and new
        https://gothenburgbitfactory.org/

    }}}

    How-To's {{{
      $ task help
      $ man task
      $ man taskrc
      $ man task-color
      $ man task-sync

      You can customize some things through ~/.taskrc, including choosing a color theme

      Basic structure of a TW command

        $ task <filter> <command> <modifications> <miscellaneous>

      Searches can be made with a regex command followed by the desired output of results. For
      example:

        $ task /<search_term>/ list

      Output from help 20180531: {{{
        task                                                   Runs rc.default.command, if specified.
        task <filter> active                                   Active tasks
        task          add <mods>                               Adds a new task
        task <filter> all                                      All tasks
        task <filter> annotate <mods>                          Adds an annotation to an existing task
        task <filter> append <mods>                            Appends text to an existing task description
        task <filter> blocked                                  Blocked tasks
        task <filter> blocking                                 Blocking tasks
          NOTE: Filters can be chained together too. So if you want a list of tasks that are
          blocking other tasks, but that they themselves are not blocked by any other task, you can
          use something like:

            task +BLOCKING -BLOCKED
            UPDATE: I'm wanting to say that `$ task ready` accomplishes the same thing

        task <filter> burndown.daily                           Shows a graphical burndown chart, by day
        task <filter> burndown.monthly                         Shows a graphical burndown chart, by month
        task <filter> burndown.weekly                          Shows a graphical burndown chart, by week
        task          calc <expression>                        Calculator
        task          calendar [due|<month> <year>|<year>] [y] Shows a calendar, with due tasks marked
        task          colors [sample | legend]                 All colors, a sample, or a legend
        task          columns [substring]                      All supported columns and formatting styles
        task          commands                                 Generates a list of all commands, with behavior details
        task <filter> completed                                Completed tasks
        task          config [name [value | '']]               Change settings in the task configuration
        task          context [<name> | <subcommand>]          Set and define contexts (default filters)
        task <filter> count                                    Counts matching tasks
        task <filter> delete <mods>                            Deletes the specified task
        task <filter> denotate <pattern>                       Deletes an annotation
        task          diagnostics                              Platform, build and environment details
        task <filter> done <mods>                              Marks the specified task as completed
        task <filter> duplicate <mods>                         Duplicates the specified tasks
        task <filter> edit                                     Launches an editor to modify a task directly
        task          execute <external command>               Executes external commands and scripts
        task <filter> export                                   Exports tasks in JSON format
        task <filter> ghistory.annual                          Shows a graphical report of task history, by year
        task <filter> ghistory.monthly                         Shows a graphical report of task history, by month
        task          help ['usage']                           Displays this usage help text
        task <filter> history.annual                           Shows a report of task history, by year
        task <filter> history.monthly                          Shows a report of task history, by month
        task <filter> ids                                      Shows the IDs of matching tasks, as a range
        task          import [<file> ...]                      Imports JSON files
        task <filter> information                              Shows all data and metadata
        task <filter> list                                     Most details of tasks
        task          log <mods>                               Adds a new task that is already completed
        task          logo                                     Displays the Taskwarrior logo
        task <filter> long                                     All details of tasks
        task <filter> ls                                       Few details of tasks
        task <filter> minimal                                  Minimal details of tasks
        task <filter> modify <mods>                            Modifies the existing task with provided arguments.
        task <filter> newest                                   Newest tasks
        task <filter> next                                     Most urgent tasks
        task <filter> oldest                                   Oldest tasks
        task <filter> overdue                                  Overdue tasks
        task <filter> prepend <mods>                           Prepends text to an existing task description
        task <filter> projects                                 Shows all project names used
        task <filter> ready                                    Most urgent actionable tasks
          # This one is really handy! It pulls out tasks that are not blocked, sorted by urgency
        task <filter> recurring                                Recurring Tasks
        task          reports                                  Lists all supported reports
        task          show [all | substring]                   Shows all configuration variables or subset
        task <filter> start <mods>                             Marks specified task as started
        task <filter> stats                                    Shows task database statistics
        task <filter> stop <mods>                              Removes the 'start' time from a task
        task <filter> summary                                  Shows a report of task status by project
        task          synchronize [initialize]                 Synchronizes data with the Taskserver
        task <filter> tags                                     Shows a list of all tags used
        task          timesheet [weeks]                        Weekly summary of completed and started tasks
        task          udas                                     Shows all the defined UDA details
        task <filter> unblocked                                Unblocked tasks
        task          undo                                     Reverts the most recent change to a task
        task <filter> uuids                                    Shows the UUIDs of matching tasks, as a comma-separated list
        task          version                                  Shows the Taskwarrior version number
        task <filter> waiting                                  Waiting (hidden) tasks
        task          _aliases                                 Generates a list of all aliases, for autocompletion purposes
        task          _columns                                 Displays only a list of supported columns
        task          _commands                                Generates a list of all commands, for autocompletion purposes
        task          _config                                  Lists all supported configuration variables, for completion purposes
        task          _context                                 Lists all supported contexts, for completion purposes
        task          _get <DOM> [<DOM> ...]                   DOM Accessor
        task <filter> _ids                                     Shows the IDs of matching tasks, in the form of a list
        task <filter> _projects                                Shows only a list of all project names used
        task          _show                                    Shows all configuration settings in a machine-readable format
        task <filter> _tags                                    Shows only a list of all tags used, for autocompletion purposes
        task          _udas                                    Shows the defined UDAs for completion purposes
        task <filter> _unique <attribute>                      Generates lists of unique attribute values
        task <filter> _urgency                                 Displays the urgency measure of a task
        task <filter> _uuids                                   Shows the UUIDs of matching tasks, as a list
        task          _version                                 Shows only the Taskwarrior version number
        task          _zshattributes                           Generates a list of all attributes, for zsh autocompletion purposes
        task          _zshcommands                             Generates a list of all commands, for zsh autocompletion purposes
        task <filter> _zshids                                  Shows the IDs and descriptions of matching tasks
        task <filter> _zshuuids                                Shows the UUIDs and descriptions of matching tasks
        burndown                                               Aliased to 'burndown.weekly'
        ghistory                                               Aliased to 'ghistory.monthly'
        history                                                Aliased to 'history.monthly'
        rm                                                     Aliased to 'delete'
        shell                                                  Aliased to 'exec tasksh'

        Documentation for Taskwarrior can be found using 'man task', 'man taskrc', 'man task-color', 'man task-sync' or at
        http://taskwarrior.org

        The general form of commands is:
          task [<filter>] <command> [<mods>]

        The <filter> consists of zero or more restrictions on which tasks to select, such as:
          task                                      <command> <mods>
          task 28                                   <command> <mods>
          task +weekend                             <command> <mods>
          task project:Home due.before:today        <command> <mods>
          task ebeeab00-ccf8-464b-8b58-f7f2d606edfb <command> <mods>

        By default, filter elements are combined with an implicit 'and' operator, but 'or' and 'xor'
        may also be used, provided parentheses are included:

          task '(/[Cc]at|[Dd]og/ or /[0-9]+/)'      <command> <mods>

        A filter may target specific tasks using ID or UUID numbers.  To specify multiple tasks use
        one of these forms:

          task 1,2,3                                    delete
          task 1-3                                      info
          task 1,2-5,19                                 modify pri:H
          task 4-7 ebeeab00-ccf8-464b-8b58-f7f2d606edfb info

        The <mods> consist of zero or more changes to apply to the selected tasks, such as:
          task <filter> <command> project:Home
          task <filter> <command> +weekend +garden due:tomorrow
          task <filter> <command> Description/annotation text
          task <filter> <command> /from/to/     <- replace first match
          task <filter> <command> /from/to/g    <- replace all matches

        Tags are arbitrary words, any quantity:
          +tag       The + means add the tag
          -tag       The - means remove the tag

        Built-in attributes are:
          description:    Task description text
          status:         Status of task - pending, completed, deleted, waiting
          project:        Project name
          priority:       Priority
          due:            Due date
            yyyy-mm-dd
            yyyy-mm-ddThh:mm:ss
            monday/etc # can NOT add a time to a named_date like mondayT12:00:00
            eod, eow # end of day, end of week
            sod, sow # start of day, start of week
            <month name> <date>, like "november 1"
              NOTE: There is a glitch with inputting 31st, for this you need to use yyyy-mm-dd
              format
            2days, 2hours, 5th, etc
              While these work, I have no idea where I discovered them; they're not listed in the
              manual or on the website documentation...

            More info at: https://taskwarrior.org/docs/named_dates.html
            More info at: https://taskwarrior.org/docs/dates.html#dateformat

          recur:          Recurrence frequency
            recur:daily/weekly/monthly/yearly/etc
            NOTE: There are a lot of other possibilities available too.
              bimonthly, quarterly, semiannual, 1 quarter, 2 quarters, 3 quarters, etc
              https://taskwarrior.org/docs/durations.html
          until:          Expiration date of a task
          limit:          Desired number of rows in report, or 'page'
          wait:           Date until task becomes pending
          entry:          Date task was created
          end:            Date task was completed/deleted
          start:          Date task was started
          scheduled:      Date task is scheduled to start
            NOTE: Can use this for recurring items too, like scheduled:15th
          modified:       Date task was last modified
          depends:        Other tasks that this task depends upon
            Example: If task 2 must be completed before task 1 can be completed
              task 1 modify depends:2
          blocks:         Other tasks that depend upon this task
            NOTE: This command is made available per one of the abovenamed plugins
            Example: If task 2 must be completed before task 1, you can establish this while
            creating task 2

              task add example blocks:1

        Attribute modifiers make filters more precise.  Supported modifiers are:

          Modifiers         Example            Equivalent           Meaning
          ----------------  -----------------  -------------------  -------------------------
                            due:today          due = today          Fuzzy match
          not               due.not:today      due != today         Fuzzy non-match
          before, below     due.before:today   due < tomorrow       Exact date comparison
          after, above      due.after:today    due > tomorrow       Exact date comparison
          none              project.none:      project == ''        Empty
          any               project.any:       project !== ''       Not empty
          is, equals        project.is:x       project == x         Exact match
          isnt              project.isnt:x     project !== x        Exact non-match
          has, contains     desc.has:Hello     desc ~ Hello         Pattern match
          hasnt,            desc.hasnt:Hello   desc !~ Hello        Pattern non-match
          startswith, left  desc.left:Hel      desc ~ '^Hel'        Beginning match
          endswith, right   desc.right:llo     desc ~ 'llo$'        End match
          word              desc.word:Hello    desc ~ '\bHello\b'   Boundaried word match
          noword            desc.noword:Hello  desc !~ '\bHello\b'  Boundaried word non-match

        Alternately algebraic expressions support:
          and  or  xor            Logical operators
          <  <=  =  !=  >=  >     Relational operators
          (  )                    Precedence

          task due.before:eom priority.not:L   list
          task '(due < eom or priority != L)'  list

        The default .taskrc file can be overridden with:
          task ... rc:<alternate file> ...
          task ... rc:~/.alt_taskrc ...

        The values in .taskrc (or alternate) can be overridden with:
          task ... rc.<name>=<value> ...
          task rc.color=off list

        Any command or attribute name may be abbreviated if still unique:
          task list project:Home
          task li       pro:Home

        Some task descriptions need to be escaped because of the shell:
          task add "quoted ' quote"
          task add escaped \' quote

        The argument -- tells Taskwarrior to treat all other args as description, even if they would
        otherwise be attributes or tags:

          task add -- project:Home needs scheduling

        Many characters have special meaning to the shell, including:
          $ ! ' " ( ) ; \ ` * ? { } [ ] < > | & % # ~

      }}}

    }}}

    Misc {{{
      ID numbering {{{
        In the past, you've been annoyed by how ID's are renumbered after any list removals (either
        done or complete). There is a way to disable this renumbering but it's strongly discouraged
        since it's part of the active "garbage collection" process baked into TW

          UPDATE: Also, be aware that the id's only seem to be renumbered after you 're-call' the
          report. So, for example, say you run `task due`, and out comes the list of tasks with due
          dates. And for simplicity, let's say they're number 1-10. If you were to mark task 6 as
          done, task 7 per the output that is already shown will still be task 7. The only time task
          7 'moves' to fill in the gap left by the now removed task 6 is whenever you generate a new
          report. At least that's how it seems to be and it hasn't come back to bite me yet.

      }}}

      Stopped notices {{{
        If you're in the middle of editing a task and you accidently close vim/whatever (when you
        abandon the session or something and you see something like `[1]+ Stopped`), whenever you go
        back into Taskwarrior to try and finish your edits, you may be presented with the glorious
        message:

          "Task is already being edited."

        The fix for this is pretty easy thanks to how Taskwarrior works. All you have to do is open
        the related file in vim

          $ vim ~/.task

        This should open a directory of sorts, and there you should see a listing for the file you
        previously had open. Hop back in to that bad boy then do a :wq and you're good to go!

      }}}

      Colorthemes {{{

        If you ever want to set up your own colortheme, this is generally what the no-color
        colortheme is for, to serve as a blank slate for you to start from

          http://manpages.ubuntu.com/manpages/trusty/man5/task-color.5.html
          $ task color # shows available colors
          $ task color legend # lets you see what you're currently rocking

      }}}

      User-Defined Attributes {{{
        https://taskwarrior.org/docs/udas.html
        Think of these as custom bling you can add into Taskwarrior. You can basically create your
        own attributes and then specify how they should be treated/handled/calculated by Taskwarrior

      }}}

      Interesting plugins/hooks to consider for taskwarrior and timewarrior {{{
        (https://taskwarrior.org/tools)

        GTD # different look and feel for taskwarrior review sessions
          https://github.com/abesto/gtd

        on-modify.blocks_attr.py
          adds blocks:<id> command to add a dependency from the "other" side
          https://gist.github.com/wbsch/a2f7264c6302918dfb30

        tasktime
          keeps track of start/stop times for all tasks and sums for related projects.
          https://github.com/svenhertle/tasktime

        taskwarrior-notifications
          various popups and reminders
          https://github.com/flickerfly/taskwarrior-notifications

        taskwarrior-time-tracking-hook
          similar to the above, slightly different take
          https://github.com/kostajh/taskwarrior-time-tracking-hook

        taskwarrior-web
          web interface
          https://github.com/theunraveler/taskwarrior-web

        taskwhisperer
          if you ever got back to the gnome environment
          https://github.com/cinatic/taskwhisperer

        taskwiki
          if you ever want to integrate it into the vimwiki plugin
          https://github.com/tbabej/taskwiki

        timewarrior-indicator
          if you ever go back to using the gnome-shell environment, integrates with the command bar
          https://github.com/tassos/timewarrior-indicator

        warriors
          various scripts and reports for Timewarrior
          https://github.com/liloman/warriors

        VIT
          a fullscreen curses based rendition of Taskwarrior. From what I can tell it's basically
          the same thing but with a different set of keybindings/shortcuts to mimic vim. Though
          I don't understand... Why not just use TW within vim?

          $ sudo apt install vit

        taskw_gcal_sync
          bidirectional sync of taskwarrior and google calendar
          https://github.com/bergercookie/taskw_gcal_sync

        taskwarrior-tui
          https://github.com/kdheepak/taskwarrior-tui

          This actually looks REALLY slick. And since it's basically a supercharged version of the
          terminal view, I can see myself using this more than any sort of vim integration

        taskopen
          https://github.com/jschlatow/taskopen

    }}}

      vim-taskwarrior  {{{
        :TW # start taskwarrior inside current vim buffer

        NORMAL MODE
          a       # create new task
          M       # modify task
          +       # start task
          -       # stop task
          d       # mark highlighted task as done
          p       # duplicate task
          <space> # select/remove current task to selected list
          c       # execute command for selected task
          D       # delete task
          <CR>    # show task info

          A       # add annotation
          x       # delete annotation
          o       # open annotation as file
          <del>   # delete field/annotation/task

          r       # change report
            active all blocked blocking burndown.daily/weekly/monthly completed
            ghistory.annual/monthly history.annual/monthly information list long ls minimal newest
            next oldest overdue projects ready recurring summary tags unblocked waiting
          m       # modify field
          f       # change filter
          x       # clear completed tasks
          R       # refresh report
          s       # sort by highlighted column
          <       # sort ascending
          >       # sort descending
          H       # cycle column format left
          L       # cycle column format right
          J       # next historical entry
          K       # previous historical entry

          u       # undo last change
          S       # sync with taskd server
          B       # create bookmark for current combination
          <F1>    # view documents
          <TAB>   # jump to next column
          <S-TAB> # jump to previous column
          <right> # jump to next non-empty column
          <left>  # jump to previous non-empty column

          q       # quit taskwarrior interface

        VISUAL MODE
          d       # mark done all highlighted tasks
          D       # delete all highlighted tasks
          <CR>    # show info on highlighted tasks
          <Space> # add to selected list highlighted tasks

      }}}

    }}}

  }}}

  Terminals {{{
    bash shell  {{{
      https://www.gnu.org/software/bash/manual/bash.pdf

      writing scripts {{{
        https://askubuntu.com/questions/195652/is-there-a-standard-place-for-placing-custom-linux-scripts

        Generally, scripts are saved to /usr/local/bin. This way you don't have to add any new PATHS
        or anything. There may be instances where you want a script to be more isolated for security
        or accessibility reasons. If you feel like you ever need to go down those paths, do some
        more research on it muchacho :)

        Learning resources {{{
          https://www.linux.com/learn/writing-simple-bash-script
          http://tldp.org/LDP/Bash-Beginners-Guide/html/
          https://bash.cyberciti.biz/guide/Hello,_World!_Tutorial

        }}}

        Troubleshooting {{{

          One thing I ran into whenever I created my first scripts is this... In order to create
          a file in the /usr/local/bin directory, I had to start vim as sudo (ie `sudo vim
          reisub.sh`). However, launching vim in this way ensures the file is only given permissions
          to 'sudo'. So when it came time to try and run the script (as the default linux user), I'd
          receive notice that the operation was not permitted. To fix this, I had to expand the file
          permissions to the default linux user by going into the directory and typing:

            $ sudo chmod u+x <script_file>
            $ sudo chown <user> <script_file>
              So an example is something like `$ sudo chown linux rsub.sh`

          https://stackoverflow.com/questions/38676437/chmod-changing-permissions-of-my-script-sh-operation-not-permitted

        }}}

      }}}

      Reload bash shell without having to close down and restart the whole thing with one of
      these. This is useful if you make some edits to your .bashrc or .bash_aliases file

        $ exec bash
        $ exec $SHELL


    }}}

    To set a new default terminal emulator (ie, to say which one should open with ^M-T), run:

      $ update-alternatives --get-selections
      $ sudo update-alternatives --all
      $ sudo update-alternatives --config x-terminal-emulator

      UPDATE: For openbox qt (used in Lubuntu 19.04), you gotta go through the GUI
        Start -> Preferences -> LXQt settings -> Session settings -> Default Applications

      UPDATE: Or, use'a this'a

        $ galternatives

    Favorites:
      lxterminal
        Since it does not allow for customized line spacing, I've found the M+ series of fonts to
        have a line spacing comparable to the 1.35-1.50 height settings I was running in xfce4
      rxvt / rxvt-unicode / rxvt-unicode-256color
        UPDATE: I really really want to like uxterm or rxvt, but I've tried uxterm and I can't get
        it to play well with the vim colorschemes. I suspect it has something to do with the 256 or
        "true" color settings conflicting in some way, but I haven't been able to figure it out yet.
      sakura
      xfce4
        NOTE: allows the user to set linespacing, very handy!
      xterm / uxterm

      And when it comes to st, I can't even get the darn thing to compile without errors! Which is
      a must for customizing it

    NOTE: Something you can do to test the color spectrum of whatever terminal you're using is to
    run this command within the terminal:

      $ colors # this is an alias you setup in your .bashrc
        -- OR --
      $ wget http://www.vim.org/scripts/download_script.php?src_id=4568 -O colortest
      $ perl colortest -w

    NOTE: Some of these terminal are GPU-accelerated and therefore may not work on your laptop.

    alacritty {{{
      GPU based rendering, can make this very fast and responsive on certain systems

      $ sudo apt install alacritty
        NOTE: This package may not be available in the Lubuntu environment, seems like at the time
        of this writing, it was only hosted through the Debian environment.

      $ sudo snap install alacritty --classic

      https://github.com/jwilm/alacritty/releases
        The other way to get it would be to install from the git files. The amd64.deb version should
        do the trick, but if you run into issues, you may need to clone it directly and proceed that
        way (see your notes on installing through git).

    }}}

        edex-ui {{{
          NOTE: This is completely for shits and giggles, not meant to be a serious terminal. But
          boy does it look cool! https://github.com/GitSquared/edex-ui

        }}}

    gnome-terminal {{{
      users also talk about pairing this with tmux

    }}}

    kitty {{{
      GPU based rendering, can make this very fast and responsive on certain systems

      https://github.com/kovidgoyal/kitty/releases

      NOTE: Seems like one of those that you'll need to pull down from git and install manually.
        https://sw.kovidgoyal.net/kitty/build.html

      NOTE: Creator has started offering binary files for installing through Linux. Give this a try:
        $ curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin

        If you ever want to delete kitty, everything is contained within the directory, so all you
        need to do is delete:

          ~/.local/kitty.app

    }}}

    konsole {{{
      supposed to be really fast too
        It doesn't handle non-true-type fonts very well, so pair with something like one of the
        Proggy fonts at 12 pts

      $ sudo apt install konsole

    }}}

    lxterminal {{{
      the lubuntu default, still gets the job done, nice and slim

    }}}

    st {{{
      by suckless community, search package manager for "stterm"
      UPDATE: there are some forks you may want to check out too if you're looking at st

          https://github.com/gnotclub/xst

      https://manpages.debian.org/buster/stterm

      $ sudo apt install stterm

      HOWEVER!!! If you want to do any customization of st, you're going to need to grab the source
      files because you'll need to re-build/make st with your edits in place. Though, from what
      I can tell it seems that in order for the system to recognize that you have st installed, you
      need to add in in apt as well, similar to what you do with vim.

      This requires a few more packages that you may not already have installed

        $ sudo apt install libxft-dev libxinerama-dev libx11-dev

      $ cd /usr/share
      $ sudo git clone git://git.suckless.org/st
        $ sudo apt source stterm # I don't think this is the same as git clone...
      $ cd st
      $ sudo make
      $ sudo rm -rf config.h
      $ sudo ln -s ~/dotfiles/st/config.h /usr/share/st/config.h
      $ sudo make clean install

      patches {{{
        anysize {{{
          The default install of dwm has an annoying gap at the bottom and right of the terminal
          window. Supposedly one of the patches fixes it, called the anysize patch. Here's how you
          apply it

          $ cd /usr/share/st
          $ sudo wget https://st.suckless.org/patches/anysize/st-anysize-0.8.1.diff
          $ patch -Np1 -i st-anysize-0.8.1.diff
          $ sudo make
          $ sudo make install

        }}}

      }}}

    }}}

    terminator {{{
      It's claim to fame is being able to easily manage multiple terminal windows at the same time

      $ sudo apt install terminator

      Preferences -> Profiles -> Colors -> Foreground & Background Scheme: Solarized Dark, Palette
        Scheme: Linux or Rxvt
      Preferences -> Keybindings ->   go_down : Alt +J
                                      go_left : Alt +H
                                     go_right : Alt +L
                                        go_up : Alt +K
                              layout_launcher : Disabled (click into cell then press <backspace>)
      NOTE: Preferences -> Profile -> Command -> Check "Run command as a login shell"
      Here's how you can set up pre-defined layouts that supposedly launch when you star the
      terminal. In practice, I could never get this to work, so I simply cd'd into whatever
      directory I wanted in the initial window, then split my panes manually. But here's a little
      rundown nonetheless:

        $ terminator -l <layout_name>
          launch w the desired layout
        $ terminator -s
          launches a GUI layout interface
        <CTRL+SHIFT+X>
          maximize current window toggle

        NOTE: In order to have terminator run a set of commands after starting up, you need to edit
        your bashrc file to include the following in order to keep the bash session open when
        starting with a command (this has been included in your dotfiles bashrc
        file, presently commented out):

        UPDATE: I cannot get this to work, the terminal always closes every window that lists
        a startup command! Even startup directory settings are not working! What a mess...

          [[ $startup_cmd ]] && { declare +x $startup_cmd; eval "startup_cmd"; }

        And then you style your ~/.config/terminator/config file "command" lines as follows:

          command = env startup_cmd="<command>" bash

        Another thing you can try is to set the following:

          Terminator --> Profiles --> Command --> When command exits: Exit the terminal

        Another styling for the command, without the custom bashrc settings, involves ending
        commands w '; bash'

          command = <command>; bash

        config files are saved in this directory:

          ~/.config/terminator/config

      If you're up for giving the whole custom layout thing another go, check out this webpage's
      breakdown:

        https://askubuntu.com/questions/158159/how-do-i-get-terminator-to-start-up-with-my-custom-layout

}}}

    terminology {{{
      Very stylish, but a bit more resource intensive and prone to crashing...
      $ sudo add-apt-repository ppa:enlightenment-git/ppa
        NOTE: This repo doesn't play well with default permissions. Skip this step and run the
        normal install command below if you'd rather not mess with it
        UPDATE: Do NOT go this route! What this essentially does is give sudo rights to the package,
        allowing any and all updates to repo to hit right to your root system, which is not safe at
        all! Better to simply use the latest package in apt, or if there's a more recent version
        you'd like to have, then manually install using git
      $ sudo apt update
      $ sudo apt install terminology

    }}}

    termite {{{
      Supports 24-bit colors, lightweight
      Installation can be a little tricky on ubuntu-like systems due to all of the required
      dependencies, but here goes :)
        https://askubuntu.com/questions/739163/how-to-install-termite

        $ sudo apt install -y g++ libgtk-3-dev gtk-doc-tools gnutls-bin valac intltool libpcre2-dev
        libglib3.0-cil-dev libgnutls28-dev libgirepository1.0-dev libxml2-utils gperf
        build-essential

        $ git clone https://github.com/thestinger/vte-ng.git
        echo export LIBRARY_PATH="/usr/include/gtk-3.0:$LIBRARY_PATH"
        cd vte-ng && ./autogen.sh && make && sudo make install
        cd ..

        git clone --recursive https://github.com/thestinger/termite.git
        cd termite && make && sudo make install
        sudo ldconfig
        sudo mkdir -p /lib/terminfo/x
        sudo ln -s /usr/local/share/terminfo/x/xterm-termite /lib/terminfo/x/xterm-termite

    }}}

    tilda {{{

    }}}

    tilix {{{
      # compatible with gnome environment

    }}}

    tmux {{{
      This allows you to run multiple terminal instances in a single terminal window, similar to
      Terminator, except that tmux is not itself a terminal, but a terminal "multiplexer" that
      allows you to run mulitple sessions within a single terminal instance. It operates like
      a server in that you can close out of the terminal and it will continue to run in the
      background until you kill the active "server" terminal session. Some users advocate using this
      in a lightweight terminal app like st or xfce4.

    }}}

    rxvt || rxvt-unicode || rxvt-unicode-256color {{{
      NOTE: you might here "urxvt" being mentioned everywhere, that's actually a reference to
      rxvt-unicode
      UPDATE: So long story short... Once again, I tried to setup the rxvt environment but I kept
      getting absolutely awful colorscheme displays with vim. Plus things like hexokianse wouldn't
      display the correct overlay colors. Did a lot of searching and browsing on the web and could
      not find an comprehensive way to fix the issues. What a mess! There's gotta be a way though,
      I'm just not sure what that is and by this point, I don't really care anymore, it's already
      taken up too much time already...
      I don't know for sure, but I suspect it has something to do with the echo $TERM output...
      Though from what I read on that topic, MOST recommend not manually setting the $TERM variable
      and instead let the system do it automatically. And even then, most advise to look for the
      output xterm-256color which is what I get when I run it on my end.
      Maybe it has something to do with the t_Co value? In vim, check with
        :verbose set t_Co?
      Hmm... Nope! Comes up as 256, which is what you want

      https://www.youtube.com/watch?v=_kjbj-Ez1vU

      very lightweight, often used in combo with tmux. One of the cons though is that is requires
      a lot of configuration up front, but once you have it setup the way you like, the config file
      can be saved and reused with subsequent installs, etc

      http://www.wiki.afterstep.org/index.php?title=Rxvt-Unicode_Configuration_Tutorial

      rxvt*lineSpace: 1

      After loading or updating the ~/.Xresources file, you need to run the following for it to take
      effect on the next load:

        $ xrdb ~/.Xresources

    }}}

    xfce4-terminal {{{
      $ sudo apt install xfce4-terminal
      Very lightweight

      Color presets {{{
        Xubuntu Dark

        White on Black w the following edits:
          Text color:        # D3D7CF
          Background color:  # 1D181E
          Palette white:     # D3D7CF
          Palette red:       # FC4B4B
          Palette light-red: # FE8686

        XTerm
          Change background to black and text to light grey

        Tango
          With white text color toned down

      }}}

      Themes {{{
        http://www.xfce-look.org
        https://github.com/arcticicestudio/nord-xfce-terminal/releases

      }}}

      If you ever want to save them, put'em in one of these locations and they should show up in the
      menus

        /usr/share/themes/xfce4
        ~/.local/share/xfce4/terminal/colorschemes

    }}}

    xterm / uxterm {{{
      https://wiki.archlinux.org/index.php/Xterm

      $ sudo apt install xterm
        NOTE: This also installs 'uxterm' as well, which is what will read your .Xresources config,
        just launch it with

          $ uxterm

      <^<left-click>>   bring up menu
      <^<middle-click>> bring up another menu
      <^<right-click>>  bring up yet another menu

      ~/.Xresources
      $ xrdb -merge ~/.Xresources
        run this command after updating .Xresources
        NOTE: aliased to `xres`

      Evidently another place to change the settings is at

        /etc/X11/app-defaults/XTerm

    }}}

  }}}

  Text editors and IDE's  {{{
    Kakoune {{{
      Heavily vim-inspired editor, aims to improve upon vim's keybindings and functionality. Reading
      about it, I couldn't help but wonder though, after the significant time investment I've made
      into the vim though, what makes this worth retooling my habits and learning something new?

      See subsection for more info

    }}}

    Sublime Text  {{{
      $ wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -
      $ sudo apt install apt-transport-https
      $ echo "deb https://download.sublimetext.com/ apt/stable/" | sudo tee /etc/apt/sources.list.d/sublime-text.list
      $ sudo apt update
      $ sudo apt install sublime-text

    }}}

    Vim  {{{
      $ sudo apt install vim

      Reigning champ as far as I'm concerned! See detailed notes in its own main section below

    }}}

    Visual Studio Code  {{{
      aliased to installvscode

      $ sudo apt install software-properties-common apt-transport-https
      $ sudo wget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add -
      $ sudo add-apt-repository "deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main"
      $ sudo apt update
      $ sudo apt install code

    }}}

  }}}

  Timewarrior   {{{
    https://taskwarrior.org/docs/timewarrior/

    NOTE: See Taskwarrior section on how to properly install Timewarrior and link it to Taskwarrior

    $ timew help
    $ man timew

    $ timew summary # gives running time totals by tags
    $ timew day/week/month

    $ timew extensions # list of extensions
    $ timew diagnostics # full output of timewarrior config and various settings

    To edit the time lengths of certain tasks that you've worked, say, the past week...
      $ timew week :ids
        You can then reference the listed id's to make various edits. For example

          $ timew @1 shorten 8hrs

      Commands include:
        :fill
        move
        split
        lengthen
        split
        join
        tag
        untag
        shorten
        cancel
        delete

    Exclusions are typically used to represent time-off
      $ timew config exlusions.days.2018_06_25 off :yes

  }}}

  tmux  {{{
    https://github.com/rothgar/awesome-tmux

    $ sudo apt install tmux

    NOTE: To source the .tmux.conf file, use the following:
      <prefix> :source-file ~/.tmux.conf
        <prefix> r

    $ man tmux # display tmux manual
    $ tmux # starts tmux
    $ exit # exits out of tmux. NOTE this does not close an active tmux session, it will continue in
      the background!
    $ tmux new -s <session_name> # creates a named tmux session
    $ tmux new -s <session_name> -d # creates a new named tmux session in the background
    $ tmux new -s <session_name> -n <window_name> # same as above, also names the window
    $ tmux list-sessions || $ tmux ls
    $ tmux attach # will attach only available session, otherwise you have to specify the session
      name with...
    $ tmux attach -t <session_number>
    $ tmux kill-session # kills available session, if only one is active
    $ tmux kill-session -t <session_number>
    $ tmux kill-session -a # kills all other sessions, leaves the active session open
    $ tmux kill-sever # kills all active tmux sessions along with the server that is hosting them

    $ tmux list-keys # displays all keybinds
    $ tmux list-commands # displays all tmux commands
    $ tmux info # displays full session info

    # See the full color list available in tmux with

      $ for i in {0..255}; do printf "\x1b[38;5;${i}mcolor%-5i\x1b[0m" $i ; if ! (( ($i + 1 ) % 8 )); then echo ; fi ; done
      UPDATE: Added something similar to this to bashrc: `$ colors`

    To see how much memory tmux is currently using, first get the PID of each tmux related instance

      $ pgrep tmux

    Then run the following to see a total memory usage figure

      $ top -p <id>,<id>, ...
      <q> # exit

    Toggle statusbar (for example, when using :Goyo)
      prefix :set -g status off
      prefix :set -g status on

    Keys  {{{
      Notes on how this is formatted:
        M denotes the Alt key
        C denotes the Ctrl key
        <Default>
          <Modified in .tmux.conf>

        ^B # tmux prefix, will be abbreviated here as 'prefix'
          ^A

        prefix ? # list all tmux commands # Oddly, I find that not all commands are listed here...

        prefix f # search for text in all open windows
        prefix # # list all paste buffers

        prefix s # list sessions
        prefix d # detach from active session

        Windows  {{{
          prefix w # list windows
          prefix c # create new window
          prefix , # rename current window
          prefix ' # prompt for window index
          prefix . # prompt for index to move current window
          prefix 0..9 # switch to window of that number
          prefix p || n # move to the previous and next window respectively
            prefix h || prefix l
          prefix <number> # move to window by number
          prefix l # move to previously selected window
          prefix M-n # move to next window with bell or activity marker
          prefix M-p # move to previous window with bell or activity marker
          prefix & # close active window
          prefix i # display info about window

        }}}

        Panes  {{{
          prefix q          # diplay pane numbers
          prefix q 0..9     # move to pane by number
          prefix %          # create new pane in vertical stack
            prefix -
          prefix "          # create new pane in horizontal stack
            prefix \
          prefix o          # move to next pane
          prefix ;          # move to previous pane
          prefix <arrow>    # move to pane in direction
            prefix ^HJKL
            ^HJKL           # Modified with vim-tmux-navigator plugin, does not require prefix
          prefix <HJKL>     # resize pane in given direction
          prefix {          # swap contents with next pane
          prefix }          # swap contents with previous pane
          prefix ;          # move to last-pane
          prefix x          # close pane if multiple, closes window if only one pane
          prefix <spacebar> # cycle through various pane layouts
          prefix M-1        # select layout even-horizontal
          prefix M-2        # select layout even-vertical
          prefix M-3        # select layout main-horizontal
          prefix M-4        # select layout main-vertical
          prefix M-5        # select layout tiled
          prefix z          # full screen current pane
          prefix !          # break-out active pane into a new separate window
          prefix ^O         # rotate pane contents clockwise
          prefix M-o        # rotate pane contents counter-clockwise

        }}}

        Copying text  {{{
          NOTE: copy mode is also how cursor movement and scroll controls are enabled for tmux
          prefix [ # enter copy mode, press <enter> to get out of copy mode with
                     highlighted text copied
                v  # begin highlighting, move cursor with hjkl to select all desired text
                y  # copy selection and exit copy mode
            ^V     # paste selection to another application (ie Libre Writer, Leafpad, etc)
            ^V     # paste selection within Tmux/Vim - must first be in INSERT mode

        }}}

        prefix ^Z  # suspend tmux client session
        prefix R   # restore tmux session
        prefix d   # detach current client
        prefix $   # rename current session
        prefix s   # display a list of active sessions, switch by selection

        prefix t   # show time
        prefix ~   # show previous tmux message, if any

    }}}

    Command mode  {{{
        <prefix> : # enter command mode
          new-window -n <window_name> "<command>"
            opens new named window and then runs the quoted command new-window -n processes "top"
            NOTE: Whenever you start a window with a command, whenever the command is stopped (in
            this case, 'q' to stop top), then the window will also be closed
          capture-pane
            copies entire visible contents of active pane
          show-buffer
            shows the copied buffer contents
          save-buffer <filename.ext>
            saves current buffer to a file. Now, these commands can be strung together like this:

              $ tmux capture-pane && save-buffer buffer.txt

            capture-pane; save-buffer buffer.txt
              from within tmux in command mode
          list-buffers
          choose-buffer
          break-pane
            move pane content to new separate window, see prefix !
          join-pane -s <source_window> -t <target_window>
            moves content from window to pane
          move-window -t <window_number>
          swap-window -t -1/+1
            move window one space to the left/right
          swap-window -s <window_number> -t <window_number>
          setw synhronize-panes
            toggle; typing in active pane is duplicated to all other panes
          clear-history
            clears scrollback history

    }}}

    Plugins {{{
        Tmux Plugin Manager {{{
          https://github.com/tmux-plugins

            Strongly advise using the Tmux Plugin Manager, similar to Vim's Vim-Plug
              $ git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm

              Then put this at the very bottom of your .tmux.conf:
                List of plugins
                set -g @plugin 'tmux-plugins/tpm'

                Other format examples:
                set -g @plugin 'github_username/plugin_name'
                set -g @plugin 'git@github.com/user/plugin'
                set -g @plugin 'git@bitbucket.com/user/plugin'

                Initialize tmux plugin manager
                run '~/.tmux/plugins/tpm/tpm'

              How to use?
                First, add the new plugin to your .tmux.conf
                Start tmux and press
                  <prefix> I to install the plugin
                  <prefix> U updates plugins
                  <prefix> alt + u remove/uninstalls plugins no longer on .tmux.conf plugin list

      }}}

    }}}

    Scripting and automation  {{{
        This is very powerful! Just remember that ^M stands for carriage return, ie pressing
        <enter>

        A script looks something like this:
          tmux send-keys -t 1 "vim" "^M" "i" "<content here>"

        Save your current window and pane layout? There are a few ways for doing this...

          Tmux way
            <layout_name>=$( tmux display-message -p "#{window_layout}" )
            And then from here, you can call up that same layout at launch with
              $ tmux select-layout <layout_name>

          Plugin way with resurrect
            See notes in your .tmux.conf

          Tmuxinator
            This is a Ruby gem... As a matter of principle, I think it's odd to have a ruby gem
            manage tmux, so for the time being I'm avoiding this one. I'd rather keep the two stream
            separate and learn how to manage tmux within its own environment

    }}}

    Troubleshooting  {{{
      Error: "Tmux resurrect file not found!"
        UPDATE 20180627: For some reason, it looks as though newly created tmux_resurrect_xyz.txt
        files are not being pushed up to the dotfiles repo... Is it possible the use of `$ dotfiles
        add -u` is not adding them in for some reason?
        UPDATE: Yes, that's exactly it. When I tried using git add -u, the newly created .txt files
        were not being included while the last symlink, which was part of the original repo, was
        updated accordingly.
        ---------------------------------------------------------------------------------------------------------------------
        In my case, whenever this happened, I noticed the last shortcut file was significantly out
        of date compared to the lastest tmux-resurrect save file. In order to fix the issue, I had
        to recreate the symbolic link to the last file so that it pointed to the most recent tmux
        save file. Evidently this can happen if you happen to close out of tmux precisely while
        tmux-resurrect is making its rounds to create a new save.

          $ cd ~/.tmux-resurrect
          $ sudo ln -sf <use desired tmux_resurrect_YYYY-MM-DDTHH:MM:SS.txt> last
            This creates a new symbolic link for the last file

      WARNING: terminal is not fully functional
        From what I've read online, it seems to come from the TERM environment/?? not being set up
        properly either in the bash settings or in the .tmux.conf settings. I honestly didn't
        understand much of what I was reading... One suggestion that ended up working was to go into
        a "blank" tmux pane (like top left of window 1) and simply typed

          `$ export TERM=xterm`

        For whatever reason, after that the error message went away... How odd!

        As a sidenote, I did mess with some of the term settings in .tmux.conf prior to this
        happening, but what's confusing me about that, is I was using the same setting I was using
        before.

      Restoring session comes up "empty"
        UPDATE: Turns out I never uninstalled the continuum plugin on the virtual machine. Once
        I got rid of it, all the weird conflicting .last files quit happening!

        Something I've noticed recently is that if I try to restore a session and the terminal
        window I'm using is in, say, the ~/dotfiles directory, it will come up "blank". However, if
        I cd into the root directory (ie `cd ~`), and do a restore, most of the time it will "find"
        what it needs and restore the session properly. For a while there I was convinced the
        tmux-resurrect files were either being corrupted or overwritten somehow and that's why they
        weren't working, but after discovering this little thing, that thankfully may not be the
        case after all.

        UPDATE: So there are still times when I need to do a `$ git reset --hard` for it to take...
        Happens most often on the VM. How weird... Either way, the good news is that 99% of the
        time, using one or both of these approaches can get it back in action without having to
        rebuild the symlink or anything like that

    }}}

  }}}

  tmuxp {{{
    $ sudo apt install tmuxp
      NOTE: Somewhat of an alternative to the tmux-resurrect plugin, in case it's still giving you
      some headaches. This enhances the session management capabilities of tmux, specifically
      allowing sessions to be managed in JSON or YAML syntax.

      https://github.com/tmux-python/tmuxp

      You have a configuration saved as part of your dotfiles, to launch it, start tmux with the
      following:
        UPDATE: It now launches with the alias `$ tmuxp`

        $ tmuxp load landapp
        $ tmuxp load ~/.tmuxp/landapp.yaml

      The version hosted on apt is only 1.3.5, the most recent version is 1.5.0+. To get the most
      recent version, use the python installer:

        $ pip3 install tmuxp

  }}}

  TTY  {{{
    http://www.linusakesson.net/programming/tty/

  }}}

  UML Viewer/Editor  {{{
    NOTE: Something interesting... UML is not widely used for Ruby/Rails, at least not to the extent
    its utilized in languages like Java. From what I gather, this has to do with a difference in how
    Ruby is written. It is not a statically typed language, but dynamically typed. It uses
    duck-typing, which allows for a wide range of flexibility. I'm not sure I completely understand
    what this is referring to (yet), but it's something to keep in mind. Regardless, at least for
    myself personally at this juncture, I do see value and use in sequence diagrams

    PlantUML
      Search the PackageManager for PlantUML. You also have some plugins in your vimrc that allow
      you to create UML diagrams through the use of PlantUML syntax.

    Modelio

    BOUML
      Might be able to install it the "normal" way via
        $ sudo apt install bouml
      But if not, you've gotta do it a more manual way as follows...
      For 17.10 aka Artful release
      $ wget -q http://www.bouml.fr/bouml_key.asc -O- | sudo apt-key add -
      $ echo "deb http://www.bouml.fr/apt/artful artful free" | sudo tee -a /etc/apt/sources.list
      $ sudo apt update
    Violet
    yEd Graph Editor
    staruml
    umbrello
    app.diagrams.net
    jgraph/drawio-desktop

  }}}

  virtualbox - shared directory, install, setup, and update {{{
    *vbox-guest-additions*

    For whatever reason, when VirtualBox always seems to have resolution display issues. The most
    reliable "fix" I've used so far is to do two things:

      1. Make sure you have the vbox-guest-additions stuff good to go
      2. In the virtual drive settings, set the graphics controller to VBoxSVGA. It will probably
         complain that it's not fully supported or that the "default" graphics controller is
         recommended, but just ignore that for now. Unless and until they get the issues resolved
         for your particular setup, there's a good chance this edit is the way to go muchacho!

    In this case, you were wanting to share the Google Drive folder that was on your host windows
    machine so that it is accessible by your Linux virtual machine. It's really easy to do!

      1. Oracle VM VirtualBox Manager -> Settings -> Shared Folders -> Machine Folders -> Right
         Click 'Add Shared Folder' -> Browse to directory you'd like to share -> In the checklist
         options, I went ahead and selected "automount"
        - Machine Folder is a directory that is 'always' shared and accessible
        - Transient Folder is one that is available only for the "next" virtual machine session,
          after which it will no longer be shared.
        - At this early stage in the process, you might only see the Machine Folder as an option,
          but don't worry. You'll get access to the Transient Folders option once we're done getting
          this whole thing set up and running
      2. Now, that takes care of the VirtualBox itself as a system, next you need to go into the
         virtual machine itself and configure it so that it can "get" this shared directory.

          UPDATE: The following steps have been condensed into the alias
            $ mountvbox

          If your Guest Additions stuff is setup properly, then it should do the trick, otherwise
          continue through these remaining steps to get'er done

      3. Log into your VM as usual, then click into the User Interface toolbar.
        Devices -> Insert Guest Additions CD Image

        Explorer -> autorun.sh (double click) -> "Execute"

        - What this does is mounts an virtual installer CD that will let you install the files
          needed for file sharing. Will likely ask you to restart the system once it's completed its
          song and dance.

          UPDATE 20200527: I had trouble getting this to do anything with the GUI on Lubuntu 20.04.
          Here's how I got around the issue by running it through the terminal. You might need to
          have the Guest Additions CD mounted beforehand per the above

            $ sudo apt install gcc
            $ sudo apt install make
            $ cd /media/<username>/
            In this directory there was a directory named something like VBox* blah blah blah. Go
            into that directory and within this directory, there are a bunch of files. We're after
            one in particular...
            $ sudo ./VBoxLinuxAdditions.run

          Another thing you can try if the doesn't work, is

            $ sudo apt install virtualbox-guest-utils

      3b. If you're just updating the program, after #3 is done, you can remove the virtual disc,
          otherwise continue on to #4 Devices -> Optical Drives -> IDE X -> Remove disc from virtual
          drive.
      4. At this point, things were almost done. I could see the the google drive-like mount icon on
         the desktop, but whenever I tried to get into it, I received a "Permission Denied" message.
        - $ echo $USER # confirm your username, this is what you'll reference below
        - $ sudo adduser <user> vboxsf
        - $ sudo usermod -a -G vboxsf <user>
        - Log out and log back in and voila, you should be home free :)

        UPDATE 20201024: Okay, so step 4 didn't do the trick... For some reason the directory was
        not being mounted after login. Here's what I had to do instead, this mounts it manually

          $ sudo mkdir ~/Desktop/0_Books (or whatever you want to call it on the linux vm)
          $ sudo mount -t vboxsf 0_Books ~/Desktop/0_Books
            first argument should match the directory name you set up in VBox settings and the
            second argument is where you'd like the directory to be located on the VM

            UPDATE: Some of this has been added to the following alias

              $ mountvbox

        NOTE: The default automount directory is
          /media/sf_*

          Which is well and good, except whenever I try to go into that directory, I receive
          'permission denied'

          How can permission be granted? How can the directory be changed?

          $ sudo VBoxControl guestproperty enumerate
            Not sure if this is what you're after, but this could be a way to customize
            https://www.virtualbox.org/manual/ch08.html#vboxmanage-guestproperty

          UPDATE: From what I've read, it seems like while you can change this stuff around, it's
          advised to leave it as is

          Now, when it comes to setting up the proper permissions in order to access the directory,
          try this:

            $ sudo adduser $USER vboxsf
            $ sudo usermod -a -G vboxsf $USER
            <Log-out and log back in>

          $ sudo \vim /etc/fstab
            add one of the following lines (most recently, I found the third one to work)
            0_Books /home/linux/Desktop/0_Books vboxsf defaults 0 0
            0_Books /home/linux/Desktop/0_Books vboxsf
            0_Books /home/linux/Desktop/0_Books vboxsf rw,uid=1000,gid=1000 0 0

          $ sudo \vim /etc/modules # add the following line
            vboxsf

          Then reboot, and voila! You should be good to go :)

  }}}

  weather {{{
    wttr.in {{{
      https://github.com/chubin/wttr.in
        $ curl wttr.in
        $ curl v2.wttr.in

    }}}

  }}}

  Web Browsers {{{
    NOTE: There's some interesting history behind text-based browsers. Finding the "right one" today
    often involves making sure it's still being developed, etc. Because there are some security
    concerns here and there (for example, some text-browsers don't handle SSL very well).

    https://en.m.wikipedia.org/wiki/Comparison_of_lightweight_web_browsers
    https://phoenixts.com/blog/linux-text-based-web-browsers/

    There's also some history there. IIRC, Lynx is the oldest of the bunch and may no longer be in
    active development. Links came along soon after to build upon some of the shortcoming of
    Lynx. And even later still, the introduction of links2 and elinks came along as an answer to
    some of links's short-comings. w3m, from what I can tell, is another "long running" project with
    some different approaches here and there.

    Long story short? Again, you're mostly fighting against what is actively being maintained. Aside
    from that, many applaud elinks, for example, as being very easy to configure and customize.

    From what I can tell, the heavy-hitters that seem to gain the most traction are elinks and w3m

      Lynx
        Links
          elinks
          links2

    brave {{{
      install {{{
        apt {{{
          $ sudo apt install apt-transport-https curl
          $ curl -s https://brave-browser-apt-release.s3.brave.com/brave-core.asc | sudo apt-key --keyring /etc/apt/trusted.gpg.d/brave-browser-release.gpg add -
          $ echo "deb [arch=amd64] https://brave-browser-apt-release.s3.brave.com/ stable main" | sudo tee /etc/apt/sources.list.d/brave-browser-release.list
          $ sudo apt update
          $ sudo apt install brave-browser

        }}}

      snap {{{
        $ sudo snap install brave

      }}}

    }}}

      Set Brave as default browser {{{
        UPDATE: Another thing to try is to use `update-alternatives --set`, but I'm not sure how
        that works, the syntax is a little odd

        UPDATE: Yay, found one that works for opening links through xdg! BUT! That said, it seems to
        be working only with the NON-snap installed versions of Brave. I haven't yet figured out how
        to get it to reference that version.

          $ xdg-settings set default-web-browser brave-browser.desktop

        https://forum.mxlinux.org/viewtopic.php?f=108&t=57348

          $ exo-preferred-applications
          $ set -x;  D=brave-browser.desktop; A=/usr/share/applications ; xdg-mime default $D  $(sed -n
          '/^MimeType=/{s/MimeType=//; s/;/\n/g;p}' $A/$D ); set +x

        $ sudo galternatives
          $ sudo update-alternatives --config x-www-browser
          $ sudo update-alternatives --config gnome-www-browser
          $ xdg-settings set default-web-browser brave-browser.desktop

        https://forums.linuxmint.com/viewtopic.php?p=1784824
          Terminal -> update-alternatives --config x-www-browser set to
            "0 /usr/bin/brave-browser-stable 200 auto mode"

          Start -> Settings -> Preferred Application -> "Web Browser" is set to Brave Web Browser

          Start -> Settings -> MIME Type Editor -> "application/xhtml+xml", "text/html" and
            "text/htmlh" are all set to Brave Web Browser

          Brave menu -> settings -> Get started -> The browser self-test is set to "Brave is your
            default browser"

          Firefox menu -> Preferences -> The browser self-test is set to "Firefox is currently your
            default browser"

      }}}

      Vimium C
        There are times in a tiling window manager when you switch over to the browser, only to have
        your cursor "stuck" within the address bar. Rather than having to reach over and click into
        the page body with the mouse or mash <Tab> several times to get to the body, you can create
        a custom hotkey of sorts that tells Brave to move into the page

          Settings -> Search engine -> Manage search engines and site search -> Site search -> Add
            Search engine: leaveAddressBar
            Shortcut: u
            URL with %s in place of query: javascript:

    }}}

    browsh {{{
      While it may be an interesting project, from what I can tell, it seems to fulfill a very
      narrow use-case. It literally is a platform that is lazer focused on low bandwidth
      communities. BUT! Since it's basically a front-end representation, this means it has to "get"
      a lot of the same stuff as a normal browser session then "convert" it to the ASCII like
      output. Which means it's still downloading all of the images, etc. And this is what makes it
      a solution for remote sessions, where a server renders things out, then distributes it to
      others. Again, as a means to cutting down the bandwidth usage over the wider userbase, etc.

      But it's actual browsing experience and featureset leaves a LOT to be desired, even you're
      'sold' on its CLI terminal stuff. If all you're wanting is a CLI type experience, then check
      out things like w3m or one from the links-related family.

    }}}

    chrome {{{
      Devtools {{{
        Resources {{{
          Official documentation
            https://developers.google.com/web/tools/chrome-devtools/

          https://github.com/ChromeDevTools/awesome-chrome-devtools

          http://anti-code.com/devtools-cheatsheet/
            https://github.com/jaredwilli/devtools-cheatsheet/

          https://umaar.com/dev-tips/

          https://youtube.com/watch?v=wcFnnxfA70g
          https://youtube.com/watch?v=Z3HGJsNLQ1E

        }}}

        See violations in console
          https://umaar.com/dev-tips/192-console-violations/

        Long task indicators to root out performance/page-load issues
          https://umaar.com/dev-tips/194-long-tasks/

        Test css coverage to root out any superflous or unused css tidbits you might have floating
        around in code

          ^P to bring up the command menu, 'coverage', select Show Coverage
          https://umaar.com/dev-tips/121-css-coverage/

          Export the coverage results with
            https://umaar.com/dev-tips/187-code-coverage-export/

      }}}

    }}}

    chromium {{{
      $ sudo snap install chromium

    }}}

    elinks {{{
      $ sudo apt install elinks

      https://linux.die.net/man/5/elinkskeys
      https://wiki.archlinux.org/title/ELinks

      Default config file is ~/.elinks/elinks.conf
        NOTE: Use config file in dotfiles dir with

          $ stow elinks

      $ elinks -config-dump
      $ elinks -config-help
      $ elinks -help
      $ elinks -long-help

      <ESC>     # display menu
      o         # options
      <CR>      # insert mode when on an input line
      <up/down> # navigate links on page
      <right>   # follow link
      <left>    # previous in history
      g         # URL dialogue box
      h         # home
      :         # ex-mode (ie command line)
      < >       # switch tabs
      ^N ^P     # scroll
      <tab>     # next frame
      S-<tab>   # previous frame

      Useful commandline arguments
        --long-help

    }}}

    epiphany

    firefox {{{
      To change the zoom level of the developer tools area
        In the address bar type: about:config -> devtools.toolbox.zoomValue

    }}}

    links {{{
      $ sudo apt install links

    }}}

    links2 {{{
      http://links.twibright.com/download.php
      https://linux.die.net/man/1/links2

      $ sudo apt install links2

    }}}

    lynx {{{
      $ sudo apt install lynx

    }}}

    midori {{{
      NOTE: DO NOT USE THIS BROWSER! It hasn't been maintained since 2015 so, for security reasons,
      strongly not recommended

      About as lightweight as you can get while still be (mostly) fully-featured

        $ sudo apt-add-repository ppa:midori/ppa && sudo apt update -qq && sudo apt install midori
          UPDATE: Do NOT go this route, try to get it another way, maybe through git?

    }}}

    nyxt {{{
      Built from the ground-up with vim keys and productivity in mind
      https://github.com/atlas-engineer/nyxt

    }}}

    qutebrowser {{{
      built around vim shortcut keys

      $ sudo apt install qutebrowser

      Commands
        f F  # link overlay open in current/new
        hjkl # scroll
        H L  # back forward history
        J K  # next previous tab
        gg G # scroll top, bottom
        d    # close tab
        o O  # :open, kind of like a quick-menu, in current new tab

      :config-edit # open config file
      :bind/:unbind <command string> # update keybindings
        TODO: Need to swap out the tab movement commands, they're backwards by default. J should
        move left and K should move right. Add to edited config and stow

    }}}

    surf {{{
      https://git.suckless.org/surf/

      From the folks over at suckless, the dwm people

    }}}

    vimb {{{
      https://github.com/fanglingsu/vimb

    }}}

    vivaldi {{{
      $ wget -qO- https://repo.vivaldi.com/archive/linux_signing_key.pub | sudo apt-key add -
      $ sudo add-apt-repository 'deb https://repo.vivaldi.com/archive/deb/ stable main'
      $ sudo apt update
      $ sudo apt install vivaldi-stable

      In order for Vivaldi to work properly with certain video codecs, you need to install the
      following linux package:

        $ sudo apt install chromium-codecs-ffmpeg-extra

      To confirm that it is now supported, open vivaldi and go to:

        www.youtube.com/html5

      If you run into issues with errors or something, there's a chance you need to install gdb

        $ sudo apt install gdb
    }}}

    w3m {{{
      $ sudo apt install w3m w3m-img

      H # view keybindings

        <tab>
        b B
        s S

    }}}

    waterfox {{{
      a more barebones version of firefox

    }}}

  }}}

  webdrivers {{{
    mozilla geckodriver {{{
      download latest *.tar.gz release from https://github.com/mozilla/geckodriver/releases
      cd to download directory
      $ tar -xvzf geckodriver*
      $ chmod +x geckodriver
      $ sudo mv geckodriver /usr/local/bin

    }}}

    google chromedriver {{{
      $ sudo apt install unzip
      https://sites.google.com/a/chromium.org/chromedriver/downloads and download through the browser
        -- OR --
      $ wget -N http://chromedriver.storage.googleapis.com/2.10/chromedriver_linux64.zip -P ~/Downloads
        NOTE: I'm not entirely certain the above wget address is correct, the site I got it from may
        be outdated. Best to either get it through the browser or look to see if you can update the
        address accordingly
      cd to the download directory
      $ unzip chromedriver_linux64.zip
      $ chmod +x chromedriver
      $ sudo mv chromedriver /usr/local/bin

    }}}

  }}}

  webp image format {{{
    https://developers.google.com/speed/webp/docs/compiling
    https://storage.googleapis.com/downloads.webmproject.org/releases/webp/index.html

    install {{{
      UPDATE: These steps are aliased to

        $ installwebp

      But if you want to go through it manually, or run into any issues, by all means, check'er out

      cd into the ~/Downloads directory # this is where the file will save

        $ wget https://storage.googleapis.com/downloads.webmproject.org/releases/webp/libwebp-1.1.0.tar.gz

      Download the latest tar.gz file from the above. cd into the parent directory that holds the
      downloaded file, then

        $ tar xvzf libwebp-1.1.0.tar.gz
        $ cd libwebp-1.1.0

        $ ./autogen.sh
        $ ./configure
        $ ./configure --enable-libwebpmux --enable-libwebpdemux --enable-libwebpdecoder
          NOTE: jcupitt used the above configure steps to incorporate webp (https://github.com/libvips/libvips/issues/637)
            This is different than the 'normal' empty `$ ./configure`
        $ make
        $ sudo make install
        $ dwebp -version
        $ cwebp -version
        $ vwebp -version
          NOTE: For some reason, when I recently went through the above steps, vwebp -version did
          not come up with anything and complained that webp was not installed. I went ahead and
          installed the apt package

            $ sudo apt install webp

          and after that, the vweb -version command returned the expected output

    }}}

  }}}

  wget {{{
    http://man7.org/linux/man-pages/man1/wget.1.html
    https://linux.die.net/man/1/wget
    https://www.lifewire.com/uses-of-command-wget-2201085

    $ wget [options] [URL...]

    Use example:
      Someone had posted links to a bunch of Berkshire Hathaway related speeches, etc. Whenever
      you'd follow the link within a browser, it'd take you to one of those "play the mp3 here" type
      pages where you'd then have to physically click somewhere to download the file. Rather than do
      that one by one by one, this is where wget came to the rescue!

      First of all, I could just use `$ wget <link>` and that worked! Except for one thing, I could
      not then paste in the whole lot of <CR> separated links like `$ wget <paste links>`.

      To download them one by one right after the other I had to save the link list to a .txt file
      and then load that file with wget. It looked like this:

        $ wget -i <link_to_file>

      Once that kicked off, it worked like a charm! Moving through each file, one after the other,
      without me having to babysit it any way. Very friggin cool!

      And as a bonus, the person was somehow able to "get" all of the links with the following
        $ grep -oPi 'http.+mp3' feed.xml

  }}}

  Window Managers  {{{
    NOTE: In addition to fully-featured DE's noted above, you can also fine-grain customize
    individual components of those DE's, including what window manager you want to use. By default,
    Lubuntu uses something called Openbox, which is a very minimalistic window manager that is very
    much in line with Lubuntu's mantra on keeping things fast and lightweight. However, some
    features -- like transparency -- are not supported. So, in the event you want to keep Lubuntu
    but jazz up the capabilities (ie have things like transparency), give these various window
    managers a try.

    You select them from the login window just like you do a desktop environment

    My personal faves for now are dwm and awesome because they include dynamic layouts, whereas with
    something like i3 you have to "tell it" what you want it to do.

      awesome {{{
        https://awesomewm.org/index.html

        $ sudo apt install awesome

        $ sudo find  / -name rc.lua
          this will show you where the default config is installed.

          /etc/xdg/awesome/rc.lua
          NOTE: A custom rc.lua has since been added to your dotfiles repo, use stow to install onto
          system

        mod4 is the "super" key (ie windows key)
        mod1 is alt

        <mod> + ...
          s         # show key bindings
          r         # run prompt, I think this is the dmenu equivalent, though you may want to
                      install and use dmenu
          <cr>      # start terminal
          w         # open main menu
          m         # maximize
          n         # minimize
          ^N        # restore
          f         # fullscreen
          S-c       # kill focused
          t         # set on-top
          j         # focus next
          k         # focus previous
          u         # focus urgent
          left      # previous tag
          right     # next tag
          <n>       # switch to tag n
          ^J        # next screen
          ^K        # previous screen
          <esc>     # previously selected tag set
          h         # decrease master
          l         # increase master
          S-h       # decrease number of master windows
          S-l       # increase number of master windows
          <space>   # next layout
          S-<space> # previous layout
          ^<space>  # toggle floating
          ^<cr>     # swap with master
          ^N        # toggle tag view
          S-<n>     # tag with n
          S-^N      # toggle tag
          S-q       # quit awesome

      }}}

      bspwm {{{
        Note that bspwm is literally JUST the window manager, meaning you need something to handle
        keybindings

        $ sudo apt install bspwm sxhkd
        $ sudo apt install lemonbar # or whatever xbar variant you'd like to use
          Even if you're not using Arch, the documentation there is excellent for packages
            https://wiki.archlinux.org/index.php/Lemonbar

      }}}

      DWM {{{
        built by the "suckless" crew, have a good rep for making quality stuff

        https://wiki.debian.org/Dwm
        https://wiki.archlinux.org/index.php/Dwm
        http://forums.debian.net/viewtopic.php?f=16&t=65110
        https://unix.stackexchange.com/questions/18697/configure-dwm-to-automatically-open-applications

      install {{{
        UPDATE: The below has since been updated into an alias

          $ installdwm

        Since you'll want to do any customization of dwm, you're going to need to grab the source
        files because you'll need to re-build/make dwm with your edits in place.

        NOW! That said, for whatever reason if you just go through and install it from source, dwm
        is not listed anywhere in your login screen. The "fix" for this is to install it through apt
        AND build from source so you can get the latest version
          TODO: But why is this? How can I build from source and also have it show up properly? What
          is happening with an apt install that's not taking place with a source compile? Maybe
          there are missing dependencies?

          $ sudo apt install dwm

        This requires a few more packages that you may not already have installed

          $ sudo apt install libxft-dev libxinerama-dev libx11-dev

          $ cd /usr/share
          $ sudo git clone git://git.suckless.org/dwm
            $ sudo apt source dwm # I don't think this is the same thing as git source...
          $ cd dwm
          $ sudo make
            NOTE: If you're on a fresh install and receive something about the make command not
            being found, run the following

              $ sudo apt install build-essential

        And then, if you have your dotfiles repo ready to go, you can go ahead and move in your own
        custom config.h file with the following commands
          UPDATE: The Below should now be part of your `$ stowdwmrelated` alias

          UPDATE: You ran into an issue on your most recent install because your dotfiles config.h
          is for dwm 6.2 and git was at 6.3+, which made it somewhat incompatible.

          $ sudo rm -rf /usr/share/dwm/config.h
          $ sudo ln -s ~/dotfiles/dwm/config.h /usr/share/dwm/config.h

        Lastly, wrap up the install and put a bow on it with

          $ sudo make clean install

        Double check the version with

          $ dwm -v

      }}}

      Update config.h {{{
        UPDATE: This has since been added as its own alias

          $ dwmupdate

        Every time you update the config.h contents, you must "re-make" dwm. Here's how...

          $ cd /usr/share/dwmgit
          $ sudo make clean install

        Log out and log back in and BOOM! There ya have it :)

      }}}

        keybinds {{{
          The default mod1 key is alt
          Tags can be thought of as separate workspaces, these are the numbers listed across the top
          Screens are monitors

          <mod1> + ...
            S-<cr>      # open terminal
            p           # open dmenu
            t           # tiled layout
            m           # monocle layout (seems like fullscreen equivalent)
            f           # floating layout
            <space>     # toggle layouts
            S-<space>   # toggle window layout
            j           # focus next window
            k           # focus previous window
            i           # increment number of split windows in master area
            d           # decrement number of split windows in master area
            l           # increase master area size
            h           # decrease master area size
            <cr>        # zoom focused window into master area
            S-<n>       # apply tag to currently focused window
            <n>         # view tag
            <tab>       # toggle tag (like switching between two most recent desktops)
            ^S-<n>      # toggle tag n
            0           # view all windows
            ^N          # banish tags, add remove all windows with nth tag to/from the view
            b           # toggle top bar
            ,           # focus previous screen
            .           # focus next screen
            S-,         # send window to previous screen
            S-.         # send window to next screen
            S-c         # close focused window
            S-q         # quit dwm
            left-click  # move
            right-click # resize

          :sort

        }}}

        Patches {{{
          How to install and apply dwm patches {{{
            https://superuser.com/questions/658038/how-to-add-patches-to-dwm

            https://dwm.suckless.org/customisation/patches_in_git/
            NOTE: Since you sourced the dwm stuff into the usr/share directory there's a good chance
            you'll need to preface the below commands with `sudo`.

            NOTE: Also, just to clarify... What you're essentially doing by creating a separate
            branch is making an alternate environment for you to test out the patch. If it works
            like you want it to, you can then merge that branch into the master branch. What
            confused me with that, though, is that you're creating a sort of separate local master
            state since none of your changes are pushed up to the repo. But that's okay, it
            shouldn't break anything down the line whenever you try to update dwm with new pulls,
            etc

            Make a temporary branch, this is where you'll apply the patch then merge into master

              $ cd /usr/share/dwmgit
              $ sudo git checkout -b <branch_name>

            Save the desired .diff patch file in the root dwm directory (dwmgit), then apply the
            patch with:

              $ sudo git apply <patch.diff>

            NOTE: If you receive errors about the patch failing, give these a try:

              $ git apply --ignore-space-change --ignore-whitespace <patch.diff>
              $ git apply --3way <patch.diff>
              $ git am --3way --ignore-space-change <patch.diff>

            Another approach would be to go into the files and make the changes manually. Might be
            able to even workup a vim diff view and get/put the changes accordingly

              $ git add .
              $ git commit -m "<message>"

            Then, if all works well, you can go ahead and merge into the master branch and rebuild

              $ git checkout master
              $ git merge <branch_name>
              $ sudo make clean install

          }}}

          Useful patches {{{
            NOTE: Pertab is a MUST HAVE

            Actual full screen
              https://dwm.suckless.org/patches/actualfullscreen/

            Full gaps
              https://dwm.suckless.org/patches/fullgaps/

            Centered master
              https://dwm.suckless.org/patches/centeredmaster/

            Resizing stack window sizes
              https://dwm.suckless.org/patches/cfacts/

            Per tab layouts
              https://dwm.suckless.org/patches/pertag/

            Fibonacci layouts
              https://dwm.suckless.org/patches/fibonacci/

            Swallowing
              https://dwm.suckless.org/patches/dynamicswallow/

            Indicators
              https://dwm.suckless.org/patches/clientindicators/

          }}}

          Troubleshooting {{{
            Patch not working {{{
              If things look to be in order and it's still not taking the patch, it may be that
              you're operating with a "base" dwm fileset that is different from what the patch
              expects. It could be that the stowed files are out of date from those in your
              dotfiles. In which case, you may be better off doing the following:

                * Delete the temp dwm install directory altogether
                * Install dwm again from scratch
                * Re-stow all the dwm related files
                * Attempt to run the patch through again
                * If it takes, then finish'er off with `$ sudo make clean install`

            }}}

          }}}

        }}}

      }}}

      i3 {{{
        Think a desktop wide incarnation of tmux.
        https://i3wm.org

        $ sudo apt install i3
          some sites recommend the following extras be installed as well
            $ sudo apt install i3status dmenu i3lock xbacklight feh conky

            https://github.com/davatorium/rofi
              alternative to the 'default' dmenu, adds more fuctionality. Though I'm not entirely
              sure how to get it up and running...

              $ sudo apt install rofi

        <mod> d     # opens the quicksearch toolbar thingy
        <mod> <cr>  # opens terminal
        <mod> <S-q> # logout

      }}}

      ice-wm {{{
        https://ice-wm.org

        $ cd /usr/share
        $ wget https://github.com/ice-wm/icewm/releases/download/2.0.0/icewm-2.0.0.tar.lz
        $ cd icewm-2.0.0
        $ ./configure --prefix=/usr
        $ make
        $ sudo make install

      }}}

      qtile {{{
        http://www.qtile.org/
        https://github.com/qtile/qtile

      }}}

      xmonad {{{
        $ sudo apt install xmonad suckless-tools

      }}}

  }}}

  zoom {{{
    $ sudo snap install zoom-client

  }}}

}}}

Low-code No-code tools{{{
  airtable
  budibase
  ifttt
  webflow
  wix
  zapier

}}}

Markup languages {{{
  https://dillinger.io

  asciidoc {{{
    https://asciidoc.org/
    https://asciidoclive.com/

    There is a highly recommended Ruby gem called asciidoctor that converts asciidoc to HTML5, etc
      https://asciidoctor.org/docs/user-manual/
        $ rvm @global do gem install asciidoctor
        $ asciidoctor --version

        $ rvm @global do gem install asciidoctor-pdf
        $ asciidoctor-pdf --version

    Install
      $ cd /usr/share
      $ sudo git clone https://github.com/asciidoc/asciidoc-py3 asciidoc-9.0.4
      $ cd asciidoc-9.0.4
      $ sudo autoconf
      $ sudo ./configure
      $ sudo make
      $ sudo make install

      $ asciidoc --version
      $ make test

  }}}

  fountain {{{
    Syntax for screenplay formatted documents!
    https://fountain.io/syntax

    Export to pdf {{{
      NOTE: In my experience, screenplain has the smallest output file size and gets the job done
      just fine :)

      Afterwriting {{{
        https://afterwriting.com

          $ pnpm install afterwriting -g
          $ afterwriting --help
          $ afterwriting --source <input_file>.fountain --pdf <output_file>.pdf

      }}}

      Fountain.js {{{
        https://mattdaly.github.io/Fountain.js/

      }}}

      Kit Scenaristi {{{
        https://kitscenarist.ru/en/download.html

      }}}

      Screenplain {{{
        https://github.com/vilcans/screenplain
          http://www.screenplain.com/

          $ pip3 install 'screenplain[PDF]'

            $ screenplain <input_filename> --format pdf >> <output_filename>.pdf

      }}}

      Textplay {{{
        https://github.com/olivertaylor/Textplay

      }}}

      Trelby {{{
        https://www.trelby.org/

      }}}

      Wrap {{{
        https://github.com/Wraparound/wrap

          $ sudo snap install wrap

          $ wrap -h
          $ wrap pdf <input_file>
          $ wrap pdf <input_file> --out <output_file>.pdf

      }}}

    }}}

    Syntax
      NOTE: newline characters are respected indiscriminately. This means your editor needs to
      allow "run-on" sentences extend beyond the desired character width. In vim, this
      accomplished by enable word-wrapping and disabling the column width settings.

      Title Page
        <key>: <value>

        Title:
        Credit: written by
        Author:
        Source:
        Draft date:
        Contact:

      Sections
        # Act 1
        ## Scene 1
        ## Scene 2

      Synopsis
        = Summary text goes here

      Scene Headings
        <Camera vantage> <Location> - <Time>
          EXT. FOREST - DAY

        Keep'em simple, save details for the scene description

        INT.     # Interior
        EXT.     # Exterior
        EST.     # Establish, not used very often, highlights an entirely NEW introduced scene
        INT/EXT. # Camera changes from interior to exterior
          I/E.   # abbreviated
        EXT/INT. # Camera changes from exterior to interior
          E/I.   # abbreviated

        Force with .
          .THE CAR

      Action
        Line that does not have any syntax leaders.
        All carriage returns are interpreted as being intentional, so line gaps are respected.
        Tabs and indentations are also respected and treated "literally".

        Force with !
          !Loud bang

      CHARACTER
        All caps, with an empty line above a non-blank line below it
        Denotes who is speaking/doing whatever immediately follows

        Force with @
          @Name

      Dialogue
        "Normal" text proceeding a character designation

        If you want to add a space between some lines of dialogue, use the linebreak syntax

          First line of talking...
          {two spaces}
          Second line of talking...

      Dual Dialogue
        NOTE: This is not commonly used, mostly because it is limited to ONLY two characters. What
        about when three or more characters are involved? This is why any sort of "dual speaking"
        syntax/structure is typically avoided.

        A second character involved in the dialogue should be followed by ` ^` (ie space caret)

        JUDY ^
        You can do it!

      Parentheticals
        One line immediately following a character designation, encapsulated in parens

          BOB
          (angry)
          Where's my coffee?

        Can also be used to specify dialogue

          (V.O.) # voiceover
            When we hear the voice of someone who is not on screen AND the characters on screen also
            do not hear the voice
          (O.S.) # off-screen
            When the voice is heard by the other characters on screen but the speaker is not
            onscreen himself

      Lyrics
        ~Tra la la

      Centered text
        >CENTER THIS<

      Notes
        [[enclose notes in double brackets]]

      Omit from output
        /*
        This stuff will not appear in the output
        */

      Transitions
        Uppercase line, ends in TO:
        Has a blank line both before and after

          CUT TO:

        Force with >
          > FADE TO BLACK

        NOTE: Transitions are justified to the right margin. The exception to this rule is any
        transition that ends with "IN:". These "IN:" transitions, such as "FADE IN:", are placed at
        the left action margin and optionally in BOLD (**text**) to further distinguish.
        NOTE: Every transition must end with a colon. Except any transition that ends with "OUT". In
        those instances, the transition ends in a period, like, "FADE OUT."
        NOTE: When doing a FADE OUT. followed immediately by FADE IN: it's customary to combine
        these two into a single transition of FADE TO:

      Text
        **bold**
        *italic*
        _underlined_

      Line break
        NOTE: This does NOT seem to work as expected. From what I read online, this seems to be an
        issue with the exporters (tested on both screenplain and afterwriting).
        {<spelled count> spaces}

          {two spaces}

      Page break
        ===

  }}}

  markdown {{{
    https://www.markdownguide.org/

  }}}

}}}

Math {{{
  Trig stuffs: angles, circles, and all that jazz {{{
    http://setosa.io/ev/sine-and-cosine/
    https://en.wikipedia.org/wiki/Polar_coordinate_system
    https://www.geogebra.org/m/cNEtsbvC

    There are some great visual animations for sin, cos, tan that will really help you understand
    what they're meant to represent. At the heart of it, just keep in mind that it's all about
    a ratio of either x or y values as they pertain to certain angles around an origin.

      sin(theta) is all about the Y value
      cos(theta) is all about the X value

    These resulting ratios can then be multiplied by the given radius value to determine the end
    point.

    Now, I will say that whenever I was working through some of this stuff, I was confused at how
    angles greater than or less than 0-90* seemed to fubar things. Turns out, that's a GOOD thing!
    Because trig is all about using right triangles to make calculations. Which means that, if
    you're working with, say, a 120* angle, since that stretches beyond 90*, it naturally "breaks"
    the right triangle requirement for the trig functions. This is precisely why, in the examples
    you've seen, several right triangles may be cobbled together to "get to" the desired angle, etc.
    Looking back, that's kind of a DUH! moment, but it's all good :) Just means I'll know by a bit
    more experience, eh?

    Rotation is generally COUNTER-clockwise. So a 45* rotation rotates to the left, -45* to the
    right

    SOH CAH TOA
    sin(theta) = opposite / hypotenuse
      r*sin(theta) = y value
    cos(theta) = adjacent / hypotenuse
      r*cos(theta) = x value
    tan(theta) = opposite / adjacent
      this deals with the tangent value, which where the hypotenuse intersects the line that runs
      tangent to the circle, at the point where the circle meets the 'flat' radius line. For
      a visual, think of it as being the edge of the circle, running straight up and down, that
      meets the 0* radius line aka typically the "adjacent" line in TOA

    r (ie the hyoptenuse) = Math.sqrt(x**2 + y**2)

    To solve for theta in each, you use something called inverse trig functions
      arc<x> (arcsin, arccos, arctan)

      theta = (opposite / hypotenuse) / sin
        OR, to phrase it with arcsin, it looks something like this
      theta = (opposite / hypotenuse) * sin^-1
      theta = (opposite / hypotenuse) * arcsin
        Here, sin power to the negative 1, is what is referring to the INVERSE of sin, aka the whole
        thing divided by sin

    Point coords after rotation
      radians = degree_angle * (PI / 180)
      degree_angle = radians / (PI / 180)
        OR
      degree_angle = radians * (180 / PI)
        NOTE: In mathematical notation, the radian angle is referred to as 'theta' and looks like an
        O with a wavy-ish line through it, like a small loop at the top

      x' =  x*cos(theta) + y*sin(theta)
      y' = -x*sin(theta) + y*cos(theta)
        OR!
      y' =  y*cos(theta) - x*sin(theta)

    Converting between polar and cartesian
      Polar coordinates are noted by (radius length, theta value) or (r, theta)
        NOTE: Recall that theta is the radian angle value

        x     = r*cos(theta)
        y     = r*sin(theta)
        theta = (y/x) * arctan
          OR
        theta = (y/x) * tan^-1
          OR
        theta = (y/x)/tan

    A full circle is
      2*PI*radians

    A radian is
      The resulting angle from taking the radius length, and "traveling" that distance along the
      circle's edge. If you're looking for a more mathy term, this kind of has to do with a tangent
      line.

  }}}

}}}

metadata cleaners{{{
  exiftool{{{
    https://exiftool.org/
    https://github.com/exiftool/exiftool

    Install{{{
      TODO: Move these methods into some sort of a bash script
      Apt
        $ sudo apt install exiftool

      wget
        $ wget https://exiftool.org/Image-ExifTool-12.43.tar.gz
        $ tar xvf Image-ExifTool-12.43.tar.gz
        $ cd Image-ExifTool-12.43/
        $ ./exiftool t/images/ExifTool.jpg

      Git source
        $ git clone https://github.com/exiftool/exiftool.git

    }}}

    view all metadata
      cmd.exe /k exiftool -all

    remove all metadata
      WARNING: Be SURE you create a backup first! If exif crashes or glitches you may lose original
      -all=

  }}}

}}}

Neovim {{{
  install and setup {{{
    $ installneovim

    UPDATE: Not gonna lie, the LunarVim setup is pretty sweet and it was painless to get it up and
    running. Be sure to rename any existing ~/.config/nvim directory so you don't have to mess with
    anything accidentally overwriting or being goofy

      $ git clone https://github.com/LunarVim/Neovim-from-scratch.git ~/.config/nvim

    git install {{{
      # use alias `$ gitinstallneovim`

    }}}

    Apt {{{
      $ sudo apt install neovim
        NOTE: This version may not be up to date. For something more current, consider snap or flatpak
        packages instead, or use a PPA

        https://launchpad.net/~neovim-ppa/+archive/ubuntu/stable
        $ sudo add-apt-repository ppa:neovim-ppa/stable
        $ sudo apt update
        $ sudo apt install neovim

    }}}

    Snap, flatpak, others {{{
      $ sudo snap install nvim --classic
      $ flatpak install flathub io.neovim.nvim
        $ flatpak run io.neovim.nvim

    }}}

    NOTE: At this point, you can use your neovim related config files from your dotfiles repo you're
    managing with Stow. So it should be nothing more than

      $ stow neovim

    How to do it manually, without using Stow with your dotfiles repo {{{
      $ touch ~/.config/nvim/init.vim # this is the neovim equivalent of vim's .vimrc file

      Now, when it comes to configuring neovim, you have two basic options.
        1) Link neovim's config file to vim's config file, meaning they'd both use vim's setup file
           as a reference
        2) Run a completely separate config file for neovim

        Option 1: {{{
          <cd into the nvim directory>
          <paste the following into the init.vim file>
          set runtimepath^=~/.vim runtimepath+=~/.vim/after
          let &packpath = &runtimepath
          source ~/.vimrc

        }}}

        Option 2: {{{
          $ mkdir -p ~/.config/nvim/plugged
          <cd into nvim directory>
          $ cp -r ~/.vim/plugged ~/.config/nvim
          $ cp ~/.vimrc ~/.config/nvim/init.vim

        }}}

    }}}

    And since you had to do some of the install with sudo priveleges, there's a chance neovim will
    complain about not being able to write to a ShaDa file (neovim's version of a .viminfo file). In
    order to fix that, you need to give permission to your current user/self with

      $ sudo chown -R $(whoami) <path to the .local/blah blah/nvim directory>
      $ sudo chown -R $(whoami) $HOME/.local/state/nvim/shada/main.shada

      UPDATE: Doing a recent install with the script, on the laptop I had to first run

        $ sudo chown -R $(whoami) $HOME/.local/state/nvim

      ...and then, after successfully starting nvim afterward, the additional directories would be
      present, so I could then run the original

        $ sudo chown -R $(whoami) $HOME/.local/state/nvim/shada/main.shada

      Weird!

  }}}

  Uninstall {{{
    $ sudo rm /usr/local/bin/nvim
    $ sudo rm -r /usr/local/share/nvim

    NOTE: Modify the above if you installed to a different directory

  }}}

  $ nvim
  :so ~/.config/nvim/init.vim

  When it comes to using neovim, virtually everything should cross over seamlessly from vim. So for
  most of your questions and stuff, checkout the vim section. I'll only mention here those aspects
  or uses that differ from vim.

  :checkhealth

  Plugins {{{
    https://github.com/ggandor/lightspeed.nvim
    https://github.com/nvim-telescope/telescope.nvim
    https://github.com/mfussenegger/nvim-dap

    NvChad recommends
      https://github.com/NvChad/nvterm
      https://github.com/NvChad/extensions
      https://github.com/kyazdani42/nvim-tree.lua
      https://github.com/akinsho/bufferline.nvim
      https://github.com/kyazdani42/nvim-web-devicons
      https://github.com/lewis6991/gitsigns.nvim
      https://github.com/neovim/nvim-lspconfig
      https://github.com/williamboman/nvim-lsp-installer/
      https://github.com/hrsh7th/nvim-cmp
      https://github.com/nvim-telescope/telescope.nvim
      https://github.com/nvim-treesitter/nvim-treesitter
      https://github.com/windwp/nvim-autopairs
      https://github.com/lukas-reineke/indent-blankline.nvim
      https://github.com/rafamadriz/friendly-snippets
      https://github.com/L3MON4D3/LuaSnip
      https://github.com/folke/which-key.nvim

  }}}

  gx
    open web link under cursor

  troubleshooting {{{
    lsp related setup issues {{{
      https://alpha2phi.medium.com/neovim-for-beginners-lsp-part-1-b3a17ddbe611
      https://vonheikemen.github.io/devlog/tools/setup-nvim-lspconfig-plus-nvim-cmp/
      https://github.com/VonHeikemen/nvim-lsp-sans-plugins
      https://vonheikemen.github.io/devlog/tools/manage-neovim-lsp-client-without-plugins/

      c# c-sharp and Unity
        https://github.com/kabouzeid/nvim-lspinstall/issues/77
        https://neovim.discourse.group/t/setting-up-omnisharp-via-nvim-lspconfig-and-nvim-lspinstall/531/2
        https://neovim.discourse.group/t/lsp-setting-up-neovim-with-omnisharp-roslyn-server/836
        https://github.com/kabouzeid/nvim-lspinstall/issues/77
        https://www.jhonatandasilva.com/published/1623278444
        https://github.com/OmniSharp/omnisharp-roslyn/issues/1948

        https://old.reddit.com/r/neovim/comments/w44tiu/lspconfig_omnisharp_mono_vs_net_auto_detection/
        https://github.com/teejstroyer/nvim/blob/main/init.lua
        https://neovim.discourse.group/t/setting-up-omnisharp-via-nvim-lspconfig-and-nvim-lspinstall/531

        omnisharp (failed)
          mono was not found in path.
          See https://www.mono-project.com/download/stable/ for installation instructions.
          dotnet was not found in path.
          See https://dotnet.microsoft.com/download for installation instructions.
          Installation failed: system executable was not found.

        https://neovim.discourse.group/t/setting-up-omnisharp-via-nvim-lspconfig-and-nvim-lspinstall/531/2 {{{
          UPDATE: aliased to `installomnisharproslyn`
          NOTE: Change whatever path/install directory you'd like to use, to quote directly:
            curl --verbose --location --remote-name https://github.com/omnisharp/omnisharp-roslyn/releases/latest/download/omnisharp-linux-x64.tar.gz
            mkdir -p ~/.local/omnisharp
            mv omnisharp-linux-x64.tar.gz ~/.local/omnisharp
            cd ~/.local/omnisharp
            tar -xvf omnisharp-linux-x64.tar.gz

            And then, in your lsp config stuff, you set it up something like
              local omnisharp_bin = "/var/home/dudleyp/.local/omnisharp/run"
              require'lspconfig'.omnisharp.setup{
                cmd = { omnisharp_bin, "--languageserver" , "--hostPID", tostring(pid) };

        }}}

        https://github.com/kabouzeid/nvim-lspinstall/issues/127

    }}}

    packer corruption issues - E5108 "attempt to index a boolean value" {{{
      This one was a bit of a doozy. I suspect it came about due to a hard-drive lockup during
      :PackerSync. To fix it, I basically had to wipe out all of th existing packer stuff and start
      over all the plugin installs from scratch

        $ rm -rf ~/.local/share/nvim/site/pack/packer/start/*
          NOTE: This is a directory. I went in and cleaned it out manually with vifm, but perhaps
          this command would do the trick just as well?
        $ git clone --depth 1 https://github.com/wbthomason/packer.nvim ~/.local/share/nvim/site/pack/packer/start/packer.nvim
        <start nvim>
        <run :PackerSync>

    }}}

  }}}

}}}

PHP {{{
  frameworks {{{
    cakephp

    codeigniter

    fuelphp

    laminas

    laravel

    phalcon

    slim

    symfony

    yii

  }}}

}}}

*postgresql* {{{
  https://deveiate.org/code/pg/
  https://www.youtube.com/watch?v=qw--VYLpxG4

  $ man psql
    this is a command interface manual of sorts, lists out all available commands. Press q to quit.

  Initial setup {{{
    This is a two part thing, first involving your system itself, then doing some tinkering with the
    Rails app. Then setting up pgadmin to be able to interact with all the stuff. The order is
    important here!

    Part 1 {{{
      UPDATE: Use alias 'installpostgres...'

      $ sudo apt install ...
          postgresql
            NOTE: you may need to specify a version, ala `postgresql-11`
          pgadmin3
          libpq-dev
          postgresql-server-dev-all
            This one may not be needed since you don't intent to host your own servers

      NOTE: Just a word to the wise on this'n... Whenever you actually log into the server, the
      prompt symbol changes from $ to #, so I've used that here as well. This conflicts with my
      convention of using the # symbol for denoting a comment, but I'm sure you can sweat the
      difference just for this postgres write-up :)

      On initial install, postgres creates a user named 'postgres'. This means in order to interact
      with postgres, you'll need to add your logged in username to the users list. In order to get
      there, you need to do log in as the default admin user (postgres), as outlined below...

      $ ls /etc/postgresql
        Note the version number displayed here, this is the number you'll use if you ever need to
        reference a version number

      $ service postgresql status
        $ service postgresql start/stop
          Not needed for now, but this is how you can manually fire it off

      $ sudo su postgres
        this logs you in as the named user. Since the initial install of postgres includes the user
        postgres, that's what we're using here. If you're ever asked for a password, the initial
        default password is your OS login password

      type 'exit' to exit
      other users can gain access to the server by instead using `$ psql <database_name>` at the
      terminal prompt

      $ psql
        type \q to exit

      # \l # shows available databases

      # \du # shows available users

      Now, to create a set of users and databases for your application... Note the specific use of
      single quotes and ;'s

      You can skip this first step if you want and move right on to creating a new user...
      # ALTER USER postgres WITH PASSWORD '<password>'; # this updates the default user's password

      # CREATE USER <username> WITH PASSWORD '<password>'; # creates a new user, generally you want
      one with the same name as the login you're actively using for linux, so that user will have a
      corresponding database account

      # ALTER USER <newly_created_username_from_above> WITH SUPERUSER;
        grants the named user with the SUPERUSER role
        NOTE: I've also seen something similar accomplished with this command:
          # GRANT ALL PRIVELEGES ON DATABASE <database_name> TO <user_name>;
        If you ever want to get rid of a user, use
          # DROP USER <username>;
            deletes the specified user

      # CREATE DATABASE <server_name> [options...];
        Advised you make two databases for whatevever you need, one *_test and one *_development
        For land_app, you have this set up in the config/database.yml as
          land_app_test
          land_app_development

        NOTE: can be accomplished from the root command prompt with the following commands:

          $ createdb <database_name>;

      # \q

      $ exit

    }}}

    From here, you're done setting up the Postgresql side of things. Now, we've gotta hop on over to
    Rails for its portion...

    Part 2 {{{

      gem 'pg' # This should be for all environments, replacing the default "gem 'sqlite3'"
      NOTE: I had trouble getting the 'pg' gem to install, the associated log files were complaining
      about not having the proper headers and stuff... To fix it, I ran the following:

        $ sudo apt install libpq-dev
          NOTE: This is now included as part of your install notes

      edit the database.yml file to the following:
        development:
          adapter: postgresql
          encoding: unicode
          database: <app_name>_development
          pool: 5
          username: <username>

        test:
          adapter: postgresql
          encoding: unicode
          database: <app_name>_test
          pool: 5
          username: <username>

      rails db:setup

      rails db:migrate

      rails s
        Test'er out! Make sure she's blowing and going! And that's it, you're done :)

    }}}

    And for pgadmin...

    Part 3 {{{
      Name: whatever you want, this is to identify it within pgadmin, something like
        "land_app_development"
      Host: localhost (if you're rocking it locally)
      Port: 5432 (default)
      Password: input your server password so you can connect to the badboy

      And there ya go, the rest you can either leave blank/default

    }}}

  }}}

  Upgrading to a new version {{{
    NOTE: This write-up details the steps followed to upgrade from 9.6 to 10.x, consult
    documentation for subsequent upgrades

    $ pg_dumpall > <output_file>
      creates a backup
    $ sudo apt install postgresql-10
    $ pg_lsclusters
      this should show both versions installed with Status: Online
    $ sudo pg_dropcluster 10 main --stop
      the new version is installed for continuity purposes. But since we want to do a "fresh"
      install and transfer from the older version, we need to first drop the "new" cluster so we can
      rebuild it
    $ sudo systemctl stop postgresql
      stops everything that may be currently writing to the database
    $ sudo pg_upgradecluster -m upgrade 9.6 main
      This will likely cause some processes to write to the screen; just sit back and watch, you
      don't have to do anything...
    $ pg_lsclusters
      the old 9.6 should now be listed as
        Status: Down

      while the new 10 is listed
        Status: Online
        @ port 5432

    -- Take a moment to check that everythig is working fine with the new database version --
    $ sudo pg_dropcluster 9.6 main --stop
      this drops the old database completely.
    -- At this point, all that's left to do is remove the old 9.6 related packages from linux with
    Synaptic Package Manager and you should be good to go! Congratulations!

  }}}

  Using in Rails {{{
    https://edgeguides.rubyonrails.org/active_record_postgresql.html#full-text-search

  }}}

  Troubleshooting {{{
    PG::ConnectionBad - could not connect to server: no such file or directory. Is the server
    running locally and accepting connections on Unix domain socket
    "/var/run/postgresql/.s.PGSQL.5432"?

      Saw this error message after I had ran my normal `$ rails s` then navigated the browser to
      localhost:3000 I'm not quite sure how to fix it just yet, but here are some things I'm
      currently tracking down to figure it out. I suspect the easiest "fix" may be to simply drop
      and rebuild an entirely new server.

      UPDATE NUMERO DOS: Nope, on second thought, I went ahead and purged the whole thing from the
      system and REALLY start from scratch

        $ dpkg -l | grep postgres
        $ sudo apt purge <each of the packages named in the above command>
        $ sudo apt update && sudo apt autoclean && sudo apt autoremove -y

      UPDATE NUMERO UNO: At the end of the day, I ended up rebuilding the database with the
      following commands: WARNING!!!! This will result in losing all data!!! So if you're in
      a situation where you need to preserve things, this is NOT the approach you want to take
      muchacho :)

        $ sudo pg_dropcluster 10 main
        $ sudo pg_createcluster 10 main --start
        $ sudo services postgresql restart

      From here, you'll then need to run through the initial setup again to recreate the databases
      that you need. Otherwise, if you try to continue from here, you'll likely receive a message
      similar to the following:

        "FATAL: role 'linux' does not exist"

    ############################################################

      For posterity, here are the notes I took while sifting through the weeds. The good news is,
      navigating through this process made me a little more familiar with postgres :)

        $ which psql
          shows where the working directory is for psql
        $ pg_lsclusters
          shows which version/s you're currently rocking

        Looking at the logs (/var/log/postgresql/postgresql-10-main.log)

          "...aborting startup due to startup process failure terminated by signal 6: aborted
          pg_ctl: could not start server" Which is a bit odd given that lubuntu/linux doesn't work
          off of the pg_ctl program, at least from what I can tell, not directly.

        $ sudo systemctl <start/stop/restart/reload/status> postgresql
          Appears to work

        $ sudo pg_ctlcluster 10 main <start/stop/restart/reload/status/promote>
          Laptop: "Error: Config owner (linux:1000) and data owner (postgres:114) do not match, and
          config owner is not root" On the VM, it does not say this.
            What is the config file it's talking about?
            How do I set the config owner to postgres?

              $ sudo chown postgres:postgres /etc/postgresql/10/main/postgresql.conf

            How do I make it the root user?
            Better still, how can I set linux to be the root user?
              This is not advised in any way shape or form! Better way to go about it is to simply
              add whatever users you'd like to have root-like permissions as part of some sort of
              root group rather than create a user that directly duplicates/mirrors root permissions

          $ psql
            This is you trying to get access into the server directly, appropriately complains about
            the same .s.PGSQL.5432 file not existing

          $ dpkg -l | grep postgres
            This will show you what postgres related packages you have installed. The far left
            column shows the package "states". In the case of my laptop, it showed state "rc" for
            postgresql-9.6. The "rc" designation means it had been unintsalled. Which is correct. If
            it has showed the postgresql-10 as "rc", then that would be a problem since that's the
            version you were using at the time.

          $ service postgresql@10-main start
            "Job for postgresql@10-main.service failed because the service did not take the steps
            required by its unit configuration. See 'systemctl status postgresql@10-main.service'
            and 'journalctl -xe' for details."

            $ sudo passwd root
              this lets you set a password for the 'supser-user' account (aka root). From what
              I gather, this seems to be a temporary login of sorts. Like you're telling the system,
              "Hey, I want to quickly log in as root, here's the password I want to use for this
              while I have this 'session' open..."

            $ su -
              this is what you use to actually then log into the root account with the password you
              supplied earlier. You'll know you're using the root account because the prompt will
              end with # rather than the usual $ When you're done doing what you need to do, exit
              out of the root account by simply typing `# exit`. And then, you need to then 'close'
              the re-lock the root account with:

              $ sudo passwd -dl root #

        https://stackoverflow.com/questions/31645550/why-psql-cant-connect-to-server

        browse to the datadir and checkout the pg_hba.conf file. You may first need to give yourself
        access rights with

          $ sudo chown
          $ etc/postgresql/10/main/pg_hba.conf

        Evidently what you're looking for are the settings toward the bottom for "local". When
        I initially pulled up the file, the line read "local all all peer". According to the link
        above, changing "peer" to "trust" may resolve the issue.
          UPDATE: Nope, didn't fix anything! At least not with just this... Will keep a'looking :)

        https://askubuntu.com/questions/50621/cannot-connect-to-postgresql-on-port-5432

        $ sudo /etc/init.d/postgresql restart
          Seems like another way to restart the server... Still didn't fix anything...

        https://stackoverflow.com/questions/31645550/why-psql-cant-connect-to-server
          $ sudo pg_ctlcluster 10 main start
            Job for postgresql@10-main.service failed because the service did not take the steps
            required by its unit configuration. See "systemctl status postgresql@10-main.service"
            and "journalctl -xe" for details.

          $ sudo systemctl status postgresql@10-main.service
            ● postgresql@10-main.service - PostgreSQL Cluster 10-main
              Loaded: loaded (/lib/systemd/system/postgresql@.service; indirect; vendor preset: enabled)
              Active: failed (Result: protocol) since Fri 2018-08-10 16:26:48 CDT; 28s ago
              Process: 15144 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 10-main start (code=exited, status=1/FAILURE)

            Aug 10 16:26:47 linux systemd[1]: Starting PostgreSQL Cluster 10-main...
            Aug 10 16:26:47 linux postgresql@10-main[15144]: Use of uninitialized value in subroutine entry at /usr/bin/pg_ctlcluster line 209.
            Aug 10 16:26:47 linux postgresql@10-main[15144]: Use of uninitialized value in subroutine entry at /usr/bin/pg_ctlcluster line 210.
            Aug 10 16:26:48 linux postgresql@10-main[15144]: Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /v
            Aug 10 16:26:48 linux systemd[1]: postgresql@10-main.service: Can't open PID file /var/run/postgresql/10-main.pid (yet?) after start: N
            Aug 10 16:26:48 linux systemd[1]: postgresql@10-main.service: Failed with result 'protocol'.
            Aug 10 16:26:48 linux systemd[1]: Failed to start PostgreSQL Cluster 10-main.
            lines 1-12/12 (END)...skipping...
            ● postgresql@10-main.service - PostgreSQL Cluster 10-main
              Loaded: loaded (/lib/systemd/system/postgresql@.service; indirect; vendor preset: enabled)
              Active: failed (Result: protocol) since Fri 2018-08-10 16:26:48 CDT; 28s ago
              Process: 15144 ExecStart=/usr/bin/pg_ctlcluster --skip-systemctl-redirect 10-main start (code=exited, status=1/FAILURE)

            Aug 10 16:26:47 linux systemd[1]: Starting PostgreSQL Cluster 10-main...
            Aug 10 16:26:47 linux postgresql@10-main[15144]: Use of uninitialized value in subroutine entry at /usr/bin/pg_ctlcluster line 209.
            Aug 10 16:26:47 linux postgresql@10-main[15144]: Use of uninitialized value in subroutine entry at /usr/bin/pg_ctlcluster line 210.
            Aug 10 16:26:48 linux postgresql@10-main[15144]: Error: /usr/lib/postgresql/10/bin/pg_ctl /usr/lib/postgresql/10/bin/pg_ctl start -D /var/lib/postgresql/10/main -l /var/log/postgresql/postgresql-10-main.log -s -o  -c config_file="/etc/postgresql/10/main/postgresql.conf"
            Aug 10 16:26:48 linux systemd[1]: postgresql@10-main.service: Can't open PID file /var/run/postgresql/10-main.pid (yet?) after start: No such file or directory
            Aug 10 16:26:48 linux systemd[1]: postgresql@10-main.service: Failed with result 'protocol'.
            Aug 10 16:26:48 linux systemd[1]: Failed to start PostgreSQL Cluster 10-main.

        https://stackoverflow.com/a/39878580/5474201

        https://stackoverflow.com/a/28931794/5474201

  }}}

  Misc Notes {{{
    https://www.youtube.com/watch?v=aYqZXa2byrI
      This is an excellent video discussing postgresql and jsonb in the wild :)

    jsonb {{{
      Can be a very powerful and useful data storage solution for unstructured or semi-structured
      data, but it does come with some drawbacks

      In addition to query performance and stuff like that, by its very nature it also consumes more
      space since it is storing the "full" key value pair for every listing.
        There are ways to mitigate this. One of the most obvious is in converting commonly used
        jsonb data sets into more traditional sql column-based structures.

        Can be tricky to index if/when speed is key

      GIN function index
        with jsonb_ops
        with gin_trgm_ops

      jsonb_path_query
        introduced in pq12
        allows for some interesting and powerful "like" queries that span across one or more levels

      jsonb_path_<*>
        (There's a whole slew of these that are introduced, not going to cover each of them indepth)

    }}}

  }}}

}}}

Rails {{{
  Resources
    https://edgeguides.rubyonrails.org

    Misc tips and tricks {{{
      partial indexing {{{
        this allows rails to index a subset of a column. So, for example, let's say you have an
        attribute that is frequently used to locate a small group of records (say, by a column named
        "classification", for example), you can index only the rows that match the criteria you're
        looking for. In this case, let's say you only want to index those records classified as
        "hero". In the migration file, you'd include something like

          def change
            add_index :people, :classification, where: "(classification = hero)"
          end

      }}}

      .pluck() {{{
        Whenever you're pulling in a large query, consider using .pluck() to return an ARRAY of the
        needed attributes. Since it returns an array, rather than a collection of Active Record
        objects, it is much faster to execute and doesn't take as many resources to gather and
        stuff.

        You can do some really neat stuff with scopes, .join, and .merge, getting them to kind of
        chain off of each other to perform a single query, very quickly. You can even throw pluck in
        there too if you only need specific attributes rather than full ActiveRecord objects

        Here's another cool thing you can do with .pluck. You can have it output into a hash format.
        Combining with .zip() and .to_h results in an output that can be much faster than a "full"
        record pull

          attrs = %w(id parent_image_id crop_x crop_y crop_w crop_h crop_rotation_degrees)
          test = Image.joins(tags: [submission: [task: :task_group]]).where(task_groups: { id: tg }).pluck(*attrs)
            .map { |i| attrs.zip(i).to_h }

        And then, from there, you can access the results using typical hash notation

          test[0]["crop_x"]

        I initially saw this in a Stackoverflow answer, but there are some great articles out there
        for it too:

          https://www.rubyguides.com/2017/10/array-zip-method/
          https://www.geeksforgeeks.org/ruby-array-zip-function/

        What makes sense to me is to consider the phrase .zip() meaning that you're "connecting" too
        ends of a zipper together. Which is a good visual for what it does. It "zips" argument
        arrays with the self array such that each array's position gets combined. Here's an example:

          a1 = [1, 2, 3]
          a2 = %w(a b c)
          a3 = %w(x y z)

        If you "zip" these arrays together, you'll "zip" the 0-position values into one array, then
        the 1-position values, etc

          a1.zip(a2, a3) => [[1, "a", "x"], [2, "b", "y"], [3, "c", "z"]]

        Does that make sense?

        Now that you see how it works, can you make sense of how the initial attrs and test example
        works?

      }}}

      rails-observers gem {{{
        https://github.com/rails/rails-observers

        You may not use something like this, but prior to Rails 4-ish, Observers were built into
        Rails. Since then, however, they've split out the functionality into its own gem. So if you
        ever feel like you want to broadcast updates between models and stuff, definitely check this
        one out!

      }}}

    }}}

    WHEN UPDATING RAILS VERSIONS!!!
      There's more you need to do than simply bumping up your gemfile listing and `$ bundle
      update`ing. You also need to run a command that runs through your Rails files and does some
      housecleaning to bring things up to the new standard. Before you do this, though, you need to
      mosey on over to a new branch so you don't overwrite anything important.

        <go to new branch>
        <bump up your gemfile listing, then run the ole $ bundle update>
        $ rails app:update # <-- This is the new step you weren't doing before

      Now, when you run that last command, it'll walk you through a confirmation for each and every
      file it's going to modify. From what I've read, the recommended approach is to go ahead and
      hit `a` to accept'em all. And then, after it's done, merging the changes into your prior
      working branch one by one.

      Because there's a VERY STRONG CHANCE some or all of the files' contents will be completely
      wiped out and replaced with new text. For example, when I updated from 5 to 6, this process
      completely erased what I previously had in my routes.rb file and replaced it with the default
      boilerplate. Running through the $ git mergetool allowed me to step through this change and
      bring my active setup back into the picture.

      Make sense? Good luck muchacho!

  ActiveController {{{
    callbacks  {{{
      after_action
      append_after_action
      append_around_action
      append_before_action
      around_action
      before_action
      prepend_after_action
      prepend_around_action
      prepend_before_action
      skip_after_action
      skip_around_action
      skip_before_action

    }}}

  }}}

  ActiveRecord {{{
    Callbacks {{{
      Creating an object:
        before_validation
        after_validation
        before_save
        around_save
        before_create
        around_create
        after_create
        after_save
        after_commit

      Updating an object:
        before_validation
        after_validation
        before_save
        around_save
        before_update
        around_update
        after_update
        after_save
        after_commit

      Destroying an object:
        before_destroy
        around_destroy
        after_destroy
        after_commit

    }}}

    Associations {{{
      https://guides.rubyonrails.org/association_basics.html

      NOTE: there are a bunch of options too (not listed below), including some cool scoping things
      you can do to make thing more efficient.

      belongs_to  {{{
        Book
          belongs_to :author

        The books table will have an author_id column that contains the appropriate foreign_key from
        the author table.

        With this association, a book has exactly one author, the author_id field must exist for
        every book record.

        book instances gain the following five methods:
            * author
            * author=
            * build_author
            * create_author
            * create_author!

      }}}

      has_one  {{{
        Author
          has_one :book

        This means that an author record can only be used in one book record.
        Kind of like an additional stipulation for a belongs_to association that affects how the
        foreign key records are assigned author instances gain the following five methods:
          * book
          * book=
          * build_book
          * create_book
          * create_book!

      }}}

      has_one :through {{{
        Supplier
          has_one :account
          has_one :account_history, through: :account

        Account
          belongs_to :supplier
          has_one :account_history

        AccountHistory
          belongs_to :account

      }}}

      has_many  {{{
        Author
        has_many :books

        An author record can be used in zero or more book records, as a foreign key (author_id) in
        the books table

        Typically on the "other side" of a belongs_to association

        Author instances gain the following methods:
            * books
            * books<<(object, ...)
            * books.delete(object, ...)
            * books.destroy(object, ...)
            * books=(objects)
            * book_ids
            * book_ids=(ids)
            * books.clear
            * books.empty?
            * books.size
            * books.find(...)
            * books.where(...)
            * books.exists?(...)
            * books.build(attributes = {}, ...)
            * books.create(attributes = {}, ...)
            * books.create!(attributes = {}, ...)

      }}}

      has_many :through  {{{
        Pysician
        has_many :appointments
        has_many :patients, through: :appointments

        Appointment
        belongs_to :physician
        belongs_to :patient

        Patient
        has_many :appointments
        has_many : physicians, through: :appointments

        # you can also chain them together like this
        Document
        has_many :sections
        has_many :paragraphs, through: :sections

        Section
        belongs_to :document
        has_many :paragraphs

        Paragraph
        belongs_to :section"
        has_one :through
        # same as the above, but it has one instead of manually

      }}}

      polymorphic  {{{
        Picture
        belongs_to :imageable, polymorphic: true

        Employee
        has_many :pictures, as: :imageable

        Product
        has_many :pictures, as: :imageable

        Now, before you run off and throw in poly's everywhere, be aware that this table structure
        may throw some things off here and there, even within Rails. For example, evidently forms
        and other views may need some love because the Rails helpers are not able to glean what they
        need from the poly table. There are work arounds for this though! Also, there's a chance
        that some of these snags have since been worked out by the Rails devs, but you'll no doubt
        find no shortage of posts and solutions if you do find yourself running into issues,
        including:

          https://stackoverflow.com/questions/3969025/accepts-nested-attributes-for-with-belongs-to-polymorphic
          https://api.rubyonrails.org/classes/ActionDispatch/Routing/PolymorphicRoutes.html
          https://jarlowrey.com/blog/polymorphic-rails-5
          https://semaphoreci.com/blog/2017/08/16/polymorphic-associations-in-rails.html
          https://jeremiahalex.gitbooks.io/wdi-sg/content/06-ruby-rails/rails-polymorphism/readme.html

        Using poly with many to many
          https://stackoverflow.com/questions/1128308/rails-many-to-many-polymorphic-relationships
          https://www.sihui.io/has_many-through-and-polymorphic/
          https://aaronvb.com/articles/a-polymorphic-join-table.html

          Quite a few, if all, eager-loading methods are supported with poly, it all just depends on
          if you've setup your models and stuff properly

        For the method issues I'm running into, check out
          https://apidock.com/rails/ActiveRecord/Associations/ClassMethods/has_many

      }}}

    }}}

    CollectionProxy  {{{
      This is an interesting little creature. Basically, a collectionproxy is an array which itself
      contains one or more arrays. In order to iterate through each of the associated items, then,
      you must approach them as a sort of compound array object (I just made that term up), like
      arrays within arrays. This may first entail flattening out the array structure so you can "get
      to" each item individually.

      So for example... I had a thing where a Task has_many Documents and a Document has_many
      Images. I was trying to iterate through the Images themselves starting at a Document object.
      It ended up looking something like this:

        @task_documents = @task.documents
        @task_document_images = @task_documents.map { |task_document| task_document.images }
        @task_document_images.flatten.uniq.each do |tdi|
          ...
        end

    }}}

    Methods  {{{
      ActiveRecord::Relation  {{{

        first_or_create {{{
          This is used AFTER a search of some sort has already been found, usually with a where
          statement as shown below. This is different from find_or_create_by which DOES take in
          attributes. These two statements essentially perform the same action, notice how their
          ordering is different:

            Foo.where(attributes).first_or_create
            Foo.find_or_create_by(attributes)

          See the difference?

          first_or_create can also take a block, like this:

            Book.where(:title => 'Tale of Two Cities').first_or_create do |book|
              book.author = 'Charles Dickens'
              book.published_year = 1859
            end

        }}}

        find_each {{{
          If you use .each you will iterate over ALL available records and instantiate each of them
          into memory. If you'd rather avoid clogging up that much memory and risk causing your app
          to freeze, consider find_each. It returns results in batches of 1000. The only drawback
          here is that you may not be able to specify any type of ordering of output here...

        }}}

        .to_sql and .explain {{{
          Great for deciphering just what the heck ActiveRecord is passing into SQL :)

        }}}

        pluck {{{
          Pulls out an array of column values for certain records.
            published_book_titles = Book.published.map(&:title)
              A more common array return could be restated as...
            published_book_titles = Book.published.pluck(:title)

        }}}

        merge {{{
          This one does quite a few things, including do a join and filter by a named scope on the
          joined model, like

            class Book < ActiveRecord::Base
              scope :available, where(:available => true)
            end

            class Author < ActiveRecord::Base
              has_many :books
              scope :with_available_books, joins(:books).merge(Book.available)
            end

            And here's the cool part. It allows you to return all authors with at least one
            available book:

              Author.with_available_books

        }}}

        scoping {{{
          scope all queries to the current scope, like'a this'a
            Comment.where(post_id: 1).scoping do
              Comment.first
            end

        }}}

      }}}

      Dynamic finders {{{

        "For every attribute you define in your table, Active Record provides a finder method. If
        you have a field called 'first_name' on your Client model, for example, you get
        'find_by_first_name' for free from Active Record.... Multiple dynamic finders can also be
        combined. So let's say you also have a 'locked' field on the same Client model. If you want
        to find by both name and locked, you can use 'Client.find_by_first_name_and_locked'."

        Very cool!

      }}}

      Set methods {{{
        https://davidverhasselt.com/set-attributes-in-activerecord/

        method             | uses default | saved to | validations | callbacks | touches    | readonly check
                           | accessor     | database |             |           | updated_at |
        ___________________|______________|__________|_____________|___________|____________|_______________
        attribute=         | yes          | no       | n/a         | n/a       | n/a        | n/a
        write_attribute    | no           | no       | n/a         | n/a       | n/a        | n/a
        update_attribute   | yes          | yes      | no          | yes       | yes        | yes
        attributes=        | yes          | no       | n/a         | n/a       | n/a        | n/a
        update             | yes          | yes      | yes         | yes       | yes        | yes
        update_column      | no           | yes      | no          | no        | no         | yes
        update_columns     | no           | yes      | no          | no        | no         | yes
        User::update       | yes          | yes      | yes         | yes       | yes        | yes
        User::update_all   | no           | yes      | no          | no        | no         | no

      }}}

    }}}

    Eager loading {{{
      My understanding thusfar is that eager loading is all about pulling additional associations
      that will be needed later on down the pike into a single, initial query.

      So for example, if you're working with @image.tags and all you're ever going to be doing
      within that block/instance is deal with an image's tags, then eager loading is likely not
      something that you need. However, if you're instead working with @image.crops and later in the
      block/instance you go to pull out all of the tags associated with each crop, this is where
      eager loading may be a benefit because you'd be pulling those tags in with the initial
      database query to get the image crops.

    }}}

    Querying {{{
      https://guides.rubyonrails.org/active_record_querying.html
      https://pawelurbanek.com/rails-query-caching
      http://sequel.jeremyevans.net/rdoc/files/doc/querying_rdoc.html
        This gives an EXCELLENT rundown of the various query methods. Interesting to note that,
        sometimes, using the raw Sequel commands almost seem more intuitive and easy to compose. I'm
        including this here to remind you that you don't have to rely on ActiveRecord methods to get
        what you want :)

      NOTE: Any time you're dealing with querying a database, one thing you need to be aware of is
      how OFTEN your queries make a hit to the database. This is where N+1 problems start to pop up.
      The good news is these are often corrected or at least greatly improved, by using eager
      loading -- which is something ActiveRecord can handle quite well. Most often, this will
      involve use of the .includes() query method.

      .joins returns an array of the activerecord object/s
      .where returns activerecord_relation, which are arrays
        The point being that how to interact with the result differs. For example, you have to "pull
        things out of" activerecord_relation objects. So let's say you want a list of the ids, you
        could do <activerecord_relation>.ids

      scopes also always return activerecord_relation objects

    }}}

  }}}

  app/assets folder {{{
    This is where you should stick all your stylesheet and javascript files because rails will
    automatically check those folders and reference those files to create all the necessary HTML
    code and tags in the application layout file. Very coolio. See p 134 ish in the Learn Ruby on
    Rails pdf for more info

    FOR STYLESHEET ORGANIZATION, it may be handy to create separate css files for different
    themes/portions rather than one big ole huge css file. As long as they're each stored in the
    assets/stylesheets folder, all of the code will be pulled into the rails asset pipeline

  }}}

  asset pipeline  {{{
    $ rails c
    $ Rails.application.config.assets.paths
      This will output a list of loadpaths available to you based on how you've configured the asset
      pipeline with application.js and css/scss @imports, etc
      NOTE: After you make any changes or additions to the asset pipeline, you will likely need to
      restart your server before you can see them

    The below pertains specifically to vendor related fonts, but I imagine the same applies to all
    vendor assets, including images.

    https://stackoverflow.com/questions/27147844/rails-4-1-actioncontrollerroutingerror-no-route-matches-get-fonts-t

    So when it comes to including jQuery plugins through the vendor/assets directory, things can be
    a little weird in setting them up to work correctly with the asset pipline. From what I can tell
    so far, this mostly has to do with two things:

      1. how the plugins have their font file paths set up  in their related .css file.
      2. how the asset pipeline does not initially know to look in the vendor/assets/fonts directory

    There are some possible fixes, each approaching the issue from slightly different angles.

      - Move the fonts out of the vendor/assets/fonts directory and place them in app/assets/fonts
        directory. Then, you may need to go into the related .css file for the plugin and update the
        path accordingly. On this note, Rails expects something different than the usual `src:
        url('...')`. You need to instead use the rails helper font_url `src: font_url(...)`
      - Move the fonts out of the vendor directory altogether and instead place them in the
        public/fonts directory

      UPDATE: Moving the assets into the public/fonts directory worked when I let the .css file use
      its default src: url(...) syntax. Whenever I updated them to use the font_url helper, it quit
      working. How odd! Now, ideally, I'd rather have things in the vendor assets directory since
      that's more in line with where they belong. So maybe there's a way to add something to the
      application.rb file or something so that it will know to look in the right places?


  }}}

  backtrace silencer {{{
    Rails.backtrace_cleaner.add_silencer { |line| line =~ /rvm/ }

    http://www.rubydoc.info/docs/rails/4.1.7/ActiveSupport/BacktraceCleaner#add_silencer-instance_method

    UPDATE 2: It also seems like the equivalent of a backtrace_silencer has been included in Rails
    for quite a while now, which largely removes the need for any sort of backtrace_silencer gem
    altogether. To use it, it sounds like you add the following to the
    config/initializers/backtrace_silencers.rb file (or, make your own initializer if that doesn't
    work?)

        Rails.backtrace_cleaner.add_silencer { |line| line =~ /rvm/ }

    UPDATE 1: So in my recent apps with Rails 5, it looks like this no longer filters out the
    backtrace... Reading online, I found the following:

      https://stackoverflow.com/questions/28836767/backtrace-silencer-not-working#36456987

      It advocates creating your own log formatter as follows:
      In development.rb and/or test.rb
        config.log_formatter = SilentLogger.new
        config.log_formatter.add_silencer { |line| line =~ /rvm/ }

      And then create a new class model
        class SilentLogger
          def initialize
            @silencers = []
          end

          def add_silencer(&block)
            @silencers << block
          end

          def call(severity, timestamp, progname, msg)
            backtrace = (String == msg) ? "#{msg}\n" : "#{msg.inspect}\n"

            return backtrace if @silencers.empty?

            @silencers.each do |s|
              backtrace = backtrace.split("\n").delete_if { |line| s.call(line) }
            end

            backtrace.join("\n")
          end
        end

  }}}

  caching {{{
    rails dev:cache
      Turns on caching for the Rails development environment. Then you can go into your views and
      specify which portions of the page you want to have cached. Very cool! In the Agile Web Dev
      w Rails 5 depot app, caching was used on the main products index page. This way it wouldn't
      have to reload every single record every single time a user viewed the page. This was also an
      viewing area that wasn't expected to change very often so it was a prime candidate. Page 114
      if you want to review further. also http://guides.rubyonrails.org/caching_with_rails.html

        <% cache <items> do %>
          <% <items>.each do |item| %>
          < ... code here ... >
          <% end %>
        <% end %>

  }}}

  concerns {{{
    https://stackoverflow.com/questions/14541823/how-to-use-concerns-in-rails-4
    https://signalvnoise.com/posts/3372-put-chubby-models-on-a-diet-with-concerns

  }}}

  console {{{
    If you ever want to enter into sandbox mode so that anyting you do in the console gets deleted
    upon exit, use this'n:

      rails c --sandbox

    reset
      just type this into the console if it ever seems to freeze up and/or not show user input. You
      likely won't see the characters on-screen, but just type them and hit <enter> and the terminal
      environment should go through a reboot routine

    When `rails c` hangss and never brings up the console, give this a shot:
      ^C to stop the process. If that's not working for some reason, kill the window/pane
        altogether
      `$ spring stop`
        stop the spring gem that's likely running in the background
      `$ rails c` # try it again

      If it turns out that things are working as intended, when you eventually leave the console,
      restart the spring instance by typing:

        `$ spring restart`

  }}}

  controller  {{{
    The controller interacts with the model to retrieve and format its information and defines
    different actions to perform on the data. The views are rendered by the controller and display
    the information collected within them."

  }}}

  db commands {{{
    $ rails ...
      db # connects to the database itself
      db:create # creates the database for the current env
      db:create:all # creates the databases for all envs
      db:drop # drops the database for the current env
      db:drop:all # drops the databases for all envs
      db:migrate # runs migrations for the current env that have not been run yet
      db:migrate:up # runs one specific migration
      db:migrate:down # rolls back one specific migration
      db:migrate:redo # runs (db:migrate:down db:migrate:up) or (db:rollback db:migrate) depending
        on the specified migration
      db:migrate:reset # runs db:drop db:create db:migrate
      db:migrate:status # shows current migration status
      db:rollback # rolls back the last migration
      db:forward # advances the current schema version to the next one
      db:seed # runs the db/seed.rb file
      db:seed:replant
      db:schema:load # loads the schema into the current env's database
      db:schema:dump # dumps the current env's schema (and seeems to create the database as well)
      db:setup # runs db:schema:load, db:seed
      db:reset # runs db:drop, db:setup
        NOTE: In practice, I've found that whenever you want to do a fresh rebuild of the entire
        schema from scratch, you need to use db:migrate:reset
      db:test:prepare # This is a good command to run immediately after doing a migration because it
        pushes the updates through to be available to the test environment

    Whenever you create a migration that adds a boolean value, BE SURE  you go into the mgiration
    file and set :boolean, default: false otherwise it will default to true, which may not be what
    you're wanting

  }}}/

  flash messages{{{
    flash[<message_type>]
    flash can take the following message types:
      :alert
      :danger
      :info
      :notice
      :success
      :warning

    Standard flash[:danger] messages will persist until a new request has been made. This can be
    a little tricky because technically, RENDER-ing a new page does not count as a request.

    To get around this, you can either submit a redirect (which does count as a new request) or you
    can modify the flash to read flash.now[:danger]. NOW is specifically designed to check for
    rendering, not requests.

  }}}

  forms {{{
    accept through_tables and associated objects  {{{
      For the model: A document has many document entities. It also has many entities through
      document entities. So far so good. It accepts nested attributes for entities and it also
      validates the associated entities. So far so good again.

      Now, for the controller: On the new action, you've got yer

        @document = Document.new

      BUT YOU ALSO HAVE yer

        @document.entities.build. This creates the documents.entities record in memory.

      In the form: Now here's where things get interesting. The wording needs to explicity match
      here. So it's

        f.fields_for :entities do |entity_fields| blah blah blah.

      This is what tripped me up so long because I was initially stating it as `f.fields_for
      :entity` and I'd always get notified in the console that entity was not an accepted parameter.
      This is because TECHNICALLY entity is not accepted. It's entities that are accepted. Once you
      straightened that out, it was smooth sailing.

      accepts_nested_attributes_for appears to accept an association name

    }}}

    form objects   {{{
      Whenever you're setting up a form with Rails form tools, the variable form objec that you pipe
      in to the method call is an actual object that you can interact with. For example...

        <%= form_for @object do |object_form| %>
        ...
        <% end %>

      object_form, in this case, is something you can interact with if you were to put up
      a binding.pry.

      NOTE: copied from handwritten notes, not quite sure where some of these were going, give them
      a whirl if interested

        object_form.methods
        Object.methods
        task_form.object

    }}}

    using modals  {{{
      https://coderwall.com/p/ej0mhg/open-a-rails-form-with-twitter-bootstrap-modals

    }}}

  }}}

  .gitignore  {{{
    # Ignore Spring Files
    /spring/*.pid"
    # ignore config/database.yml if you add any passwords or other sensitive info to that file
    A general rule of thumb is to use environmental variables as much as possible so you don't have
    to worry about hackers gaining access to sensitive database

  }}}

  gems  {{{
    Misc notes regarding issues with Ruby gems{{{
      Whenever I tried to run bin/rspec, I got a notice saying, "Could not find mime-types-2.99.1 in
      any of the sources". To fix this, run:

          gem install mime-types
          bundle install

      I then got a notice saying that the uglifier gem wasn't able to run. To fix this, open the
      gemfile and look for a commented gem called rubyracer. Uncomment that badboy, do another
      "bundle install", then WHAM!  You should be go to go

      If that doesn't work, you make need to install nodejs with this lil command:

        $ sudo apt install nodejs

        Better yet, just install the whole shibang! Go to your package manager and search for "npm"
        then BLAM! Install it along with its depedancies

      I consistently ran into nothing but problems with my rails and gemset configurations upon
      system reboot. Rails -v would say nothing was installed while ruby -v claimed a different
      version was installed!

      Turns out one way that seemed to consistently fix the issue was to type this command:
        $ /bin/bash --login

        NOTE: I've since learned to add a similar command directly into the .bashrc file, this
        allows it to log in right when the terminal starts.

          [[ -s "$HOME/.rvm/scripts/rvm" ]] && source "$HOME/.rvm/scripts/rvm"

    }}}

    NOTE: When using the '~> 1.2.3' distinction, trail it out to however precise you want the
    version control to be. So in this case, this is the same as saying > 1.2.3 && < 1.3  So it will
    track allow all of the 1.2.4, 1.2.5 updates.

    flog
      a tool to help identify code that may be hard to maintain and/or test. The main thing to keep
      in mind here, as with all metrics based approaches, is that they should not be viewed as an
      end-all. You still MUST consider things with your noggin because there are often times where
      either the score does not adequately reflect the reality of the code and/or it does not
      account for poor levels of abstraction. The whole concept of abstraction is one you must
      consider from a human perspective, and that's where you knowledge and expertise come into
      play.

      Metz recommends comparing these results with other metrics, like SLOC and Cyclomatic
      Complexity. Putting all three of these together can give you a more nuanced indication as to
      how effective and well written the code may be.

    bootstrap-sass
      An alternative to bootstrap is something called Zurb Foundation (gem 'foundation-rails'). Zurb
      was used in the Learn Ruby on Rails tut, p149 ish

    sass-rails

    bcrypt

    faker

    formtastic
      Alternative to simple_form

    simple_form
      Most popular as of 20160524, went with it in my land_app project

    will_paginate

    rename
      allows you to rename applications. To use it, install the gem , then run the following command:

        $ rails g rename:app_to NEW_NAME

    bootstrap-will_paginate

    carrierwave
      recreate_versions! # Run this on the related attribute if you make a change to the uploader
      versions that are generated. So in my case, it looked something like this:

        Image.all.each { |i| i.file.recreate_versions! if i.file? }

    mini_magick

    fog

    net-ssh

    activemerchant
      This allows for the acceptance of various credit cards and stuff for payment! Very coolio!

    stripe
      This is a credit card payment service that seems to get good reviews and be fairly competitive
      on its rates. Ryan Bates had a railscast on this. Not sure if it fits into the activemerchant
      gem or if it's better to run the stripe gem separately/exclusively?

    chartkick
      For some sweet graphing and charting goodness!

    geocoder
      This is one sweet gem that can do a lot of stuff with Lat and Long coordinates, finds nearby
      places, etc etc. The google maps API is a great way to add visual maps to your view page too.
      So these two things combined together make a very formidable resource! To go one step further,
      the gmaps4rails gem can add some interactive capabilities to the map too!

    group :development, :test do
    byebug

    rails_layout
      This is a cool little gem I found out about in the Learn Ruby on Rails kindle book that helps
      with layouts and converting page layouts to mobile. Must be "called" into action through
      a "rails generate" command. P131 ish in Learn Ruby on Rails

    launchy
      This is an interesting gem. When you're running tests and you want to confirm what page the
      test is actually on at any given point in time, add a line that says "save_and_open_page" and
      it will display the page to you in a browser for you to see.

    guard-minitest
      See notes below on how to implement

    guard
      This goes along with guard-minitest and is required for newer versions of guard

    reek
      This is a great little app recommended by Sandi Metz to discover code smells you might have.

    group :development do
    web-console
      One way to use this is to go into application.html.erb and just before the </body> tag, add
      the following:

      <% console if Rails.env.development? %>
      This will show a console window that you can use to test params

    spring

    better_errors

    binding_of_caller
      Recommended per The Rails 4 Way

    group :production do
    pg
      If you're getting an error on this when running "bundle install", type this into the command
      line to install required library files:

        $ sudo apt install libpq-dev

    rails_12factor

    puma

    group :test do

    minitest-spec-rails
      only if you want spec type stuff

    minitest-rails-capybara
      only if you want capy and spec type stuff

    capybara
      only if you want capy

    factory_girl_rails

    minitest-reporters
      used to add color. Replaced by minitest-happy in Rails 5

    minitest-happy
      extension for minitest that adds colored output

    mini_backtrace

    rails-controller-testing
      adds funtionality to controller tests for assigns

    mocha
      Olsen recommends this be used alongside Test::Unit if you decide to go that route. It adds the
      mock functionality to the testing unit, resembling what rspec has

    shoulda
      Olsen also recommends this be used alongside Test::Unit, it allows the testing language format
      to resemble the should formats found in rspec EDIT!!  Oh no wait a second wait a second! He
      mentions briefly that Test::Unit has been completely rewritten as MiniTest and that it allows
      for a lot of the same functionality of Rspec. MiniTest is what Hartl used in his more recent
      text. So maybe a lot of Olsen's hangups about testing without rspec have been alleviated.
      Either way, I'm leaning toward MiniTest right now...

    simplecov
      gives you a report for how much of your program has been tested. There is some additional
      setup you have to do in the test_helper file, so read the documentation to be sure you get it
      all grooving correctly

  }}}

  gem: react_on_rails  {{{
    UPDATE: You know what? Something I just noticed is the Agile Web Dev w Rails 5.1 text did NOT
    include this gem. Only the webpack gem, as noted in the section above. Maybe that's where part
    of my trouble stemmed from? Going to try going it without this gem, see how that treats me :)

    <%= react_component("HelloWorld", props: @some_props) %>

    HelloWorld.js component might contain something like this:
      import React from 'react';

      export default (props, railsContext) => {
        return (
          <div>
            Your locale is {railsContext.il8nLocale}.</br>
            Hello, {props.name}!
          </div>
        );
      };

    I'm not sure what these are about, but they're mentioned in the documentation
      import ReactOnRails from 'react-on-rails';
      import HelloWorld from './HelloWorld';
      ReactOnRails.register({ HelloWorld });

    $ rails g react:component GreetUser name:string

  }}}

  gem: rubocop {{{
    `$ rubocop -h` # show all command-line options
    `$ rubocop --auto-gen-config`
    `$ rubocop`
    `$ rubocop -x` # auto-correct only layout offenses
    `$ rubocop -a` # auto-correct all offenses

}}}

  generate {{{
    $ rails generate # displays a list of all available generators
    $ rails generate -h # displays a list of all available generators

    rails g ...

  }}}

  global variables {{{
    If you require "English", you can then call up Ruby's global var's in a much more user friendly
    way. To see what the calls are, check out the english.rb file saved in the Ruby directory

  }}}

  guard and guardfile  {{{
    I noticed that after I began implemnting ActiveCable channels and stuff, that guard began
    complaining that it was unable to monitor the directory for changes. To fix this, run the
    following in terminal, per

    https://github.com/guard/listen/wiki/Increasing-the-amount-of-inotify-watchers

      $ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p

    <add to gemfile>
    $ bundle exec guard init
    Then you've can go into the guardfile and edit its contents. Recommend copypasting the guardfile
    used in the RoR Tut sample_app for starters, see Guardfile Notes file

    To actually start it up just go into the root project directory in the command line and type:

      $ guard

  }}}

  Javascript and ActiveRecord {{{
    There is hope! :) Your questions about HOW to get stuff into and out of js with ActiveRecord is
    a river you can cross and the good news is that there are a lot of great things built into Rails
    and ActiveRecord to help make it more than doable. Just jotting down a few notes here to get
    this party started! Got this first little bit from an old RailsCast episode:

      https://www.youtube.com/watch?v=NBk_zalHnac

    Watch it again and piece these together

      javascript_tag

      <%= j ... %>

      data-attributes

      <%= content_tag "div", id: "products", data: { key:value, key:value } do %>
        ...
      <% end %>

  }}}

  JSON {{{
    JSON.parse string

  }}}

  links{{{
    The rails link_to helper sounds like it's WAAAAAY more convenient and dynamics/maleable than
    using "traditional" html <a href> stuff. P136 of Learn Ruby on Rails. One of the reaons is
    because it's more dynamic in that the actual link itself can change with development but the
    link_to will always point to where it needs to go.

    If you ever use the Target="_blank" for links, be sure you also add this in:
      rel="noopener"

    _path
      used nearly all the time, except…

    _url
      is used for redirects because the HTTP standard requires a full URL after redirects.

    In most browsers, the _path will work, but using the _url is technically correct and thus, more
    reliable.

    Another difference between them is that the _path generates the "tail end" of the address, kind
    of a 'relative' path, whereas the _url generates the full shebang with the host information,
    kind of as an absolute path. So for example...

      edit_task_path(@task) => /tasks/2/edit
      edit_task_url(@task)  => http://tasks/2/edit

    This most recently came into play whenever I was trying to assert_current_path on a system test.
    I originally used the _url since I heard that's what you want to use for the 'visit xyz_url'
    commands, since it builds out an absolute route.

    And when I ran the assert_current_path(xyz_url), since it was being compared to an absolute url
    with the host info, that's what Capybara was pulling in too. And since Rails routes don't
    necessarily "know" about the port, it would NOT include the port number, and Capy would pull
    int the full host url detail with the port number, so the two wouldn't match (ie
    http://3000:/tasks/2/edit != http://tasks/2/edit). To remedy this, all I had to do was change it
    to a _path, as in assert_current_path(xyz_path) and BOOM! it worked. Special thanks to Thomas
    Walpole from Capybara Google Group for that knowledge bomb :)

  }}}

  log files {{{
    it can be very handy to have a separate terminal windows open with the active development log
    showing

      tail -f log/development.log

    Can do the same thing with the test.log so you can get a feel for what's firing under the hood
    there

      tail -f log/test.log

    Now, the log files themselves can actually grow to be VERY large, especially once you've moved
    to a production server and are getting regular traffic. You'll need to effectively manage these
    files to save space. (For example, after a couple years of development, land_apps's
    development.log file was well over 1.0 gigabytes in size!)

    There are generally two ways to do this. Within Linux proper or, if you're working with a Rails
    app, within Rails itself

      1) Linux {{{
        Linux's own logrotate.conf file is usually saved in /etc/logrotate.conf . To create new
        rotation configurations for other programs, create a new *.conf file somewhere that you then
        move within the

          /etc/logrotate.d/

        directory.

        The reason you don't create and write to the file within the destination folder is because
        everything in that directory is write-protected. Typically, you name the file something that
        is easy to recognize and associate with what it controls (you'll likely see other 'active'
        config files in that directory from other applications, for example).

        And in that file, you'll configure the setup. Here's a commonly recommended config
        NOTE: The first line is always the path to the log files you want to target

          /home/linux/Programming/Projects/land_app/log/*.log {
            compress
            copytruncate
            delaycompress
            missingok
            notifempty
            rotate 7
            weekly
            su root root
          }

      }}}

      2) Rails {{{

      }}}

      Then to test the rotate action

        $ logrotate -d /etc/logrotate.d/<file>
          NOTE: If you run into permission issues, running with sudo should do the trick

      If you're happy with the results, the log rotate action will run by itself as part of the cron
      job is automatically setup as part of the logrotate program. Or, you can run it manually with

        $ sudo logrotate /etc/logrotate.d/<file>

  }}}

  migrations {{{
    rails g migration ...
      NOTE: With all of these, you may still need to double check or edit the created migration file
      until you're more comfortable with the process. Further, you can always name a migration some
      generic name and then build your own desired commands in the created file itself.

      Create<TableName> <column_name>:<data_type>, <column_name:data_type>, ...
        Creates a similarly named table with the stated attributes.
        NOTE this is just for a migration! I generally prefer to do this with:
          'rails g model <ModelName> <attribute:data_type>'
        ...since it does both the model and the migration at the same time.

      Add<ColumnName>To<TableName> <column_name>:<type> (optional, add :index)
        Adding an index looks something like this
          $ rails g migration AddFooToBars foobar:string:index
        You can also give it a "generic" column name and then specify as many additions that you
        want in the second portion

      Remove<ColumnName>From<TableName> # removes a given column from a table
        Similar to the above, you can give a generic name, then specify multiple columns as needed.
        To help rails know what you're talking about, you may also need to include the data type as
        well, like part_number:string

      CreateJoinTable<ModelAName><ModelBName> <foreign_key_1_name> <foreign_key_2_name>
        Does what it sounds like, also creates indexes on those attributes too

      Now, when it comes to using Postgresql with Rails, whenever you have a model that references
      another model by way of a foreign_key, if you try to delete the parent model, it will
      correctly complain, "Ooops! Not so fast, hold up! That record you're trying to delete, it is
      referenced as a foreign_key in another table. So we can't do that..." What you have to do,
      then, is inform Postgres that whenever you delete a parent/whatever record, that you also want
      any record rows that reference the object as a foreign_key to also be deleted. Unfortunately,
      there's no way to set this up in the initial `rails g migration` call. Instead you need to go
      into the resulting migration file and add the following:

        add_foreign_key :<current_table>, :<related_table>, on_delete: :cascade

      So for example, take the TaskImages join table, it has a t.references :image and
      t.references :task, which means you need to direct it to those related tables. Here is how
      this would be added to the migration file, AFTER the create_table block:

        add_foreign_key :task_images, :images, on_delete: :cascade
        add_foreign_key :task_images, :tasks, on_delete: :cascade

      This tells Postgres that you want to ripple the delete, to cascade it through, to all other
      related records To check that this was setup properly in the database, whenever you've
      migrated those changes through, check out the resulting schema, toward the bottom it should
      list all the add_foreign_key stuff, and there you should also see your extra on_delete:
      parameters You can read about this and other on_delete settings in the Postgresql manual

    migration files, the change method allows for:
      add_column
      add_foreign_key
      add_index
      add_reference
      add_timestamps
      change_column_default (must supply a :from and :to option)
      change_column_null
      create_join_table
      create_table
      disable_extension
      drop_join_table
      drop_table :<table_name>
        NOTE: This migration affects ActiveRecord directly. If you have a lot of related tables, say
        a bunch of join tables that likewise rely upon the existence of the named table, you'll need
        to include those 'drop_table' commands prior to calling the final 'drop_table' command.
        Also, what about the various view, helper, and test related files tha directories that will
        still exist after the table is dropped? This is where, after the drop_table migration has
        been successfully passed through, you then run `$ rails d model <model_name>`, `$ rails
        d controller <controller_name>`. Rails will then mop up these "stragglers". HOWEVER...

        ...BE ADVISED! One of the things I do NOT like about the use of `$ rails d ...` is that it
        goes through and also removes all of the associated migrations. This may be an issue if you
        have subsequent or intervening migrations that rely on those now destroyed models/migrations
        being present. In practice, I found this served as a good "check" to make sure I took care
        of everything, but still, a part of me wishes that migration history was preserved to show
        what was previously created, HOW it was created, and that it was later removed.

      enable_extension
      remove_column :<table_name>, :<attribute>, :<data_type>
        remove_column :documents, :page, :text
      remove_foreign_key (must supply a second table)
      remove_index
      remove_reference
      remove_timestamps
      rename_column :table, :old_column, :new_column
      rename_index
      rename_table

  }}}

  minitest {{{
    def setup / end
      These run before the tests take place
    def teardown / end
      These run after the tests take place
    Minitest:Mock
      May be a way to create some filler objects to test against? Uses 'expect' and 'verify'
    assert | assert_not,  assert | refute
      Supposedly these do the same thing, just a different negative syntax

    To get colors in minitest, try this out:
      Open test/test_helper.rb and add the following to the top of the file:

        require “minitest/reporters”
        Minitest::Reporters.use!

      As of Rails 5, you can add color by using the minitest extension
        'minitest-happy' by adding it to your gemfile. Nothing else required.
        HOWEVER!!! That said, I still prefer the look of the minitest reporters. The built-in color
        thing for Rails 5 is pretty sparse.

    Testing options # These can be combined too ie -f -b
      -f # fail fast
      -d # combine all test results to show at the end of the test run, rather than as they occur
      -b # show complete stacktrace
      -v # verbose, includes run times

    Testing for errors
      assert_raises(<expression>)
        example: `assert_raises(ActiveRecord::InvalidForeignKey) do /.../ end`

    YAML files
      Since yml files rely on indentation, things can get a little weird if you ever need to extend
      a string or something onto a second line, since the usual ..." + "... syntax returns an error.
      Luckily, yaml's got ya covered with it's own bag-o-trix

        > # folded style. Will treat each subsequent line as a continuation of the first
          attribute: >
            And here is where the text
            goes on multiple lines ya
            ya ya
              equivalent to "And here is where the text goes on multiple lines ya ya ya\n"

        | # literal style. Will treat each subsequent line as it's own new-line within the string
          attributes: |
            And here is where the text
            goes on mutliple lines ya
            ya ya
              equivalent to "And here is where the text\ngoes on multiple lines ya\nya ya"

  }}}

  production web servers, Hartl p 327 {{{
    He talks about how the built-in WEBrick web server that's built into rails isn't quite up to
    snuff when it comes to handling the user traffic that comes with a large production web
    application. For this reaon, he recommends using Puma. This gets added to the gemfile in the
    production group group :production do / gem 'puma'

    Then you create a config/puma.rb file and add a bunch of stuff to it. This seems to be directed
    specifically at deplying with Heroku though...

  }}}

  pry {{{
    install to global rvm gemset with

      $ rvm gemset use global
      $ gem install pry pry-doc pry-rails pry-nav

    Then you need to set it up for the proper environment. Go into app's root directory and type:

      $ pry -r ./config/environment

    With that, pry will run anytime you type the usual 'rails c' or 'rails c --sandbox' commands and
    it will also be connected to the directory whenever you use 'pry'... Except when it doesn't! Had
    a hard time getting this to actually work. One way I found to fix it is to go into the gemfile
    and in the :development group add "gem 'pry-rails'". Now, essentially, this should have already
    been encompassed with the above gem install since you did include pry-rails, but maybe adding
    the environment specificity is what did the trick?

    If it still doesn't fire with "rails  c", check that the gem is running with '$ bundle list'. In
    one instance, I had the syntax wrong in an abovelisted gem, causing all subsequent gems to not
    be loaded, installed, or updated. Once I corrected the issue, everything was up and running
    normally again :)

    There is some greater functionality you can obtain too by including these into your gemfile
    under the development and test group.

    On debugging with pry...
      In practice, I've found debugging through pry to be very useful. To get there, just add

        binding.pry
        <%= binding.pry %> # to use in .*.erb files, like html.erb or js.erb, etc

      Then, we you encounter the call, you can use any one of the following:
        CTRL-D        # move forward from the pause.
        $ disable-pry # continue forward with the script, but disable any further pry calls
        $ exit!       # completely kill the script that triggered it


      WARNING: Just so you know, if you're ever running a binding.pry, having the developer console
      open interferes with the jQuery's .focus() method. Apparently, since your own focus/cursor
      gets moved to an alternate browser/window/pane, .focus() no longer has the proper context it
      needs to run properly on the active webpage. Interesting!

    If you ever want to save pry output to a file, you can do something like this

      pry()> x = <pry command>
      pry()> .echo '#{x}' > <filename>

  }}}

  routes  {{{
    https://guides.rubyonrails.org/routing.html#adding-more-restful-actions

    Put vs Patch can be a weird thing to tackle… Here's something I found on Stack:
      TLDR version: If I understand correctly, PATCH sends over just those bits the user submits,
      whereas PUT sends over the entire record, along with the changed elements. So using PATCH is
      a way to save on bandwidth.

    PUT => If user can update all or just a portion of the record, use PUT (user controls what gets
      updated)

    PUT /users/123/email
    new.email@example.org
    PATCH => If user can only update a partial record, say just an email address
      (application controls what can be updated), use PATCH.

    PATCH /users/123
    [description of changes]
    Why Patch

    PUT method need more bandwidth or handle full resources instead on partial. So
      PATCH was introduced to reduce the bandwidth.

    Explanation about PATCH

    PATCH is a method that is not safe, nor idempotent, and allows full and partial updates and
    side-effects on other resources.

    PATCH is a method which enclosed entity contains a set of instructions describing how a resource
    currently residing on the origin server should be modified to produce a new version."

    And here's another little wrinkle to throw into the mix: PUT is idempotent and implies putting
    a resource. Do it as many times as you like, and the result is the same. x=5 (the assignment) is
    idempotent.

    POST updates a resource, adds a subsidiary resource, or causes a change.
      A POST is not idempotent, in the way that x++ is not idempotent. Doing it repeatedly will
      produce different results. In the case of each of our "decrement", the resulting value changes
      each time.

  }}}

  scaffold  {{{
    rails generate scaffold <Uppercase resource name> <optional parameters for the model, like "name:string" etc>

    Final example would be:

      $ rails generate scaffold Purchase name:string cost:float

    Scaffolds create model, controller, view, and test areas for the named resource.

    Note that since scaffolds generate mostly the bare minimums required to display/connect the
    different areas together, there is usually a LOT of customization that needs to take place
    afterward. For this reason, some devs simply forgo the scaffold convention altogether and
    instead, create the pieces separately."

    If you ever have a model in place and you decide you want the rest of the trappings that comes
    with a scaffold migration, try this'n:

      $ rails g scaffold_controller <model_name>

      To get the other juicy bits like this that are available to you, run:

        $ rails g -h/

  }}}

  SessionsHelper module {{{
    include this lil bad boy in the application_controller.rb with "include SessionsHelper". It's
    a neato lil dude. It allows for TEMPORARY login sessions whose cookie expires upon browser
    close. Note that these are automatically encrypted by Rails :)

  }}}

  specifying app features on initial creation  {{{
    If you know what you want your "stack" to be like when creating your app, you can specify them
    up front like this

      $ rails new example_app --webpack=react --database=postgresql

  }}}

  SSL Protection, Hartl p 327 {{{
    to enable SSL site-wide, go into config/environments/production.rb and uncomment the line
    "config.force_ssl = true" Note that in order to set up SSL on the remote server, you'll need to
    purchase and configure an SSL certificate for your domain. Evidently, with Heroku there's an
    option for this to be included with the site, so that's cool!

  }}}

  stats {{{
    rails stats # Interesting per file breakdown of the app

  }}}

  strong parameters {{{
    https://edgeapi.rubyonrails.org/classes/ActionController/StrongParameters.html

    Strong parameters are a safeguard you can use whenever you have a controller action that
    involves any sort of mass assignment.

    For example, check out the update action from Hartl's users_controller. It includes

      if @user.update_attributes(user_params)

    The update_attributes method is one of those "mass assignment" type situations.

    This was confusing to me for a while because at first, I thought it was a way to streamline how
    params are allowed/used in normal operations, but that's not quite the case... My understanding
    is that if you step through the expected params and "put them to use" appropriately, then that's
    what you should be doing and strong_params never really enter the picture there.

    QUESTION: Is the reason mass assignment is sought in the first place is because it involves only
    one single database hit? Rather than accessing the database multiple times to change each
    attribute individually, perhaps mass assignment lets you do all that in one single round trip?

  }}}

  tests {{{
    $ rails test # runs all non-system tests
    $ rails test:system
      run all system tests, these must be called specifically, they are not run with the generic
      'test'
    $ rails test test/models/<filename>.rb
      you can specify the file directly if you only want to run a single test
    $ rails test test/models/<filename>.rb:<linenumber
      same as above, with only single test being run at linenumber

    System tests {{{
      It seems like changes to source files require the assets to be re-compiled in order to be
      available in the development environment.

        $ rails assets:clean
        $ rails assets:precompile

    }}}

  }}}

  turbolinks and javascript  {{{
    NOTE: After almost losing my mind over trying to get my .js files to work, it turns out that if
    you want to have both .coffee and .js files rocking and rolling, they have to be named
    differently from each other. The whole time I was trying to run tasks.js and tasks.coffee...
    I discovered it whenever I tried the .js code in the .coffee file and voila! It worked! Once
    I went back and renamed the .js file to tasks_jquery.js, that file started being properly
    captured by turbolinks/sprockets too. Whew, what a mess! Good to know though :)

    On a sidenote, I did see somewhere that running the following set of commands may allow
    similarly named files to coexist peacefully and both be ran:
    UPDATE: No, this did not work. Neither tasks.js nor tasks.coffee were ran, but tasks_jquery.js
    ran without a hitch.

      $ rails assets:clean
      $ rails assets:precompile
      $ rails s

    https://github.com/turbolinks/turbolinks/blob/master/src/turbolinks/compatibility.coffee

    In order to have you JS files wait for page loads, you need to use a slightly different command
    when you're using turbolinks. The magic words are 'turbolinks:load' instead of the normal .ready
    thing. Here are a few examples:

      $(document).on('turbolinks:load', function() {
        console.log("It works on each visit!");
      });

      document.addEventListener('turbolinks:load', function() {
        var element = document.getElementById("pay-type-component");
        ReactDOM.render(<PayTypeSelector />, element);
      });

      (coffeescript example)
      $(document).on "turbolinks:load", ->
        alert "page has loaded!"

    There's also a way to disable turbolinks on certain elements of a page by setting
    data-turbolinks to falise. I don't follow it completely, but here's what it looks like:

      <div data-turbolinks="false">
        <a href="/">Disabled</a>
      </div>

  }}}

  webpacker  {{{
    Upgrading Webpacker generally involves two steps. One with the gem, and the other on the
    javascript side of things

      $ bundle update webpacker
      $ rails webpacker:binstubs
      $ yarn upgrade @rails/webpacker --latest
      $ yarn upgrade webpack-dev-server --latest

    UPDATE 20200531: Okay, so the plot is quite a bit thicker than I anticipated. Still trying to
    figure out how to move land_app to wepback from sprockets... Here are some good sites I've found
    along the way

      https://rossta.net/blog/from-sprockets-to-webpack.html
      https://mariochavez.io/desarrollo/2020/05/19/from-the-asset-pipeline-to-webpack.html
      https://medium.com/statuscode/introducing-webpacker-7136d66cddfb
      https://www.dwightwatson.com/posts/replacing-rails-asset-pipeline-with-webpacker

    When it comes to the webpack side of things (no -er!) there are some tools that can help you
    diagnose and refine it

      https://webpack.jakoblind.no/optimize/
      $ yarn add -D webpack-bundle-analyzer
      https://chribateman.github.io/webpack-visualizer
        $ yarn add -D webpack-visualizer-plugin
      $ yarn add -D bundle-stats

    UPDATE 20180712pt2: Ahh so get this! This whole time I was using both the webpacker gem and the
    react_on_rails gem. The only gem that was used in the Rails5.1 text was webpacker! It will take
    care of all the ins-and-outs of getting react properly setup and running! Yay!

    UPDATE 20180712: From the look of it, setting up React for a Rails app is quite different from
    configuring it for a more traditional app. One of the main takeaways I just got done
    experiencing is that evidently the webpack.config.js file that was prevalent in the tymc
    tutorial is not needed when setting up React and Webpack and all that shiz for a Rails app. Is
    that completely right? I don't know at this point... All I know is that I just finished ripping
    my hair out over something that doesn't even appear to be a requirement at the end of the day.
    After putting up with all of this mess, I'm still on the fence as to whether or not React is
    worth all the hassle. Especially if all I'm wanting is some "basic" button integration for
    creating related model objects...

    This gave me a TON of grief when trying to get it all setup properly. Here are some initial
    notes on how I eventually got it up and running (I hope, knock on wood...)

      $ nvm install-latest-npm || $ sudo yarn install
        creates an npm or yarn related lockfile
        https://docs.npmjs.com/getting-started/fixing-npm-permissions

      $ npm init || $ yarn init
        runs a short setup wizard that creates the package.json file
        I was confused by what the "entry point" should be set to. Some say to put it as "index.js"
        or as "app.js"... Others say to remove that line altogether and set the "private" flag to
        true. For the record, I'm trying out the private option for now

      gem 'webpacker', '~>3.0' # Add to gemfile
        NOTE: In previous iterations I failed to mention the webpacker gem. This got me wondering...
        If the webpacker gem is what ultimately allow for the rails webpacker:install command to
        work, is the above stuff with npm necessary to run separately or will those steps somehow be
        taken care of by the gem?
        Further, the webpacker gem is what allows for use of the

          <%= javascript_pack_tag("<component_name>") %>

      $ rails webpacker:install
        This process may take some time so be patient. As long as the "waiting..." wheels keep
        spinning, you can be confident it's still doing its thing

      $ rails webpacker:install:react
        This installs everything you need for React! Yay!

    Now that that's done, you still have at least one issue to contend with... And that is, how do
    you have React interact with ActiveRecord objects? This is one of the additional aspects of
    React with Rails that react-rails and react_on_rails address, by introducing their own <%=
    react_component %> tag (or whatever it is...)

    What if you wanted to try to go without either of these gems? Well, it seems like you'll have to
    convert ActiveRecord objects into JSON before you can feed them into React.

      https://stackoverflow.com/questions/45692629/react-with-rails-5-1-calling-database-objects-into-react-components

    And even then the story is not quite done because how do you scale it? If there are, say, 700
    images to a document, how long would that take for all of that data to be parsed (?) into JSON?
    Maybe there's a way to lazy-load items in as they're needed?

    Past notes that may be outdated, keeping in case you need to reference them for any reason  {{{
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    UPDATE 20180712pt3: I do NOT think you need to run this next command any longer. This is
    something that came with the react_on_rails gem and, from what I can tell by looking at the
    Rails5.1 text again, is NOT required for setting up React. So toss this'n out. I'm keeping it
    here for posterity's sake and as a means of keeping my thoughts together :)

        DO NOT USE --> $ rails g react:install

    NOTE: Same as the above, I'm not so sure you need to run these next commands. From what I can
    tell, the various webpack gem related commands you ran earlier take care of this stuff. But in
    case there's some reason you need the knowledge, I'll leave the notes here.

      DO NOT USE -->
        $ ./bin/webpack # creates a manifest.json file
          NOTE: If this is giving you grief about modes, try using this instead:
            $ ./bin/webpack --mode=development
          Troubleshooting: At this stage I kept hitting an error that said, `TypeError:
          dep.getResourceIdentifier is not a function` followed by a long list of "at ... xyz"
          items. How to fix?
              Downgrade webpack to version 3.11.0
                $ yarn add webpack@3.11.0

          NOTE: At this point, you may receive a notice saying that the @rails/webpacker module
          cannot be found or is not installed or something along those lines... If that comes up,
          try explicitly installing the module with:

              $ sudo yarn add @rails/webpacker --save

        $ ./bin/webpack-dev-server

    And what do ya friggin know! After that, the files compiled properly and I'll be damned if the
    manifest.json didn't have the hello_react.js file listed :) BABOOM! For what it's worth, this
    file is located in app/javascripts/pack, so I anticipate that will become a popular directory
    while working with React.

    Another thing to note here too is that I didn't bother at all with launching any sort of
    webpack-dev-server terminal instance or anything. I only launched the standard $ rails s and
    that seemed to do the trick.

    ONE LAST THING! To confirm that everything was setup properly, you can do one of two things...
    First, you can look at your notes below regarding how the botree link handled the process, OR
    you can use the default created hello_react.jsx file that comes with the above install process.
    Here are the notes on how to use the defualt verification process...

      app/javascript/packs/hello_react.jsx
        This is the default created file we're going to reference in our app...
      Open up your desired view file and add into the following code, using react_on_rails's
      built-in javascript_pack_tag

        <%= javascript_pack_tag("hello_react") %>

      Note how the name argument name matches that of the pack/*.jsx pack itself

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    UPDATE 20180712: Some of the links below utilize other methods of incorporating React. For
    instance, Botree makes use of the react-rails gem which has it's own quirks and differences from
    running it as AgileWebDevWRails5.1 did, or even how the react_on_rails gem does it. So for all
    intents and purposes, I largely consider this approach to be "out of date" and/or incompatible
    with the working approach I laid out in the above section.

    NOTE: The below are notes from the the botree link, which got a react compliant app and running
    in no time. I took this same base and applied it to the land_app
      So here's what's weird... If I go in and ONLY do what's on botree's link, it works... So why
      the hell am I going through all that other trouble as listed below? Here's botree's good stuff

        https://blog.botreetechnologies.com/how-to-add-react-js-to-your-ruby-on-rails-app-with-webpacker-330d619d11ec

        add 'webpacker', '~> 3.0'
        add 'react-rails', '~> 2.4.4'
        $ bundle install
        $ rails webpacker:install
        $ rails webpacker:install:react
        $ rails g react:install

        Then he adds in some <%= javascript_pack_tag 'application' %> goodness into
        application.html.erb, as the last line in the <head> tag

        $ rails g react:component GreetUser name:string
        This creates app/javascript/components/GreetUser.js, which you can then edit something
        like'a this'a:

          // Path : app/javascript/components/GreetUser.js
            import React from "react";
            import PropTypes from "prop-types";

            class GreetUser extends React.Component {
              render () {
                return (
                  <div>
                    <h1>Hello {this.props.name}, </h1>

                    <h3>Wow! This is your first react component!</h3>
                  </div>
                );
              }
            }

            GreetUser.propTypes = {
              name: PropTypes.string
            };

            export default GreetUser;

        Throw in some <%= react_component 'GreetUser', name: "Merlin" %> goodness into the desired
        view file, then BOOM!!! The ma'fa' actually works!

        ---- Various sites and references I used early on in the initial learning process ----
          https://hackernoon.com/a-tale-of-webpack-4-and-how-to-finally-configure-it-in-the-right-way-4e94c8e7e5c1
          https://medium.com/justfrontendthings/using-webpack-4-a-really-quick-start-under-4-minutes-61ff3fa9a2c8
          https://www.valentinog.com/blog/webpack-4-tutorial/
          https://survivejs.com/webpack/developing/getting-started/
          https://blog.botreetechnologies.com/how-to-add-react-js-to-your-ruby-on-rails-app-with-webpacker-330d619d11ec

          Possible fix for the Error: Chunk.entrypoints that you were getting on ./bin/webpack-dev-server
            https://github.com/webpack/webpack/issues/6568

          package.json file is where you can set various scripts to run. From what I gather, if
          I want to run a developmenet server, I first run the named script to generate the
          appropriate "bundle.js" (ie `$ yarn run development`) so it will be ready to go (but for
          what? Still need to learn what this is all about)

          When you're ready to start the server itself, use something like this
            $ yarn run webpack-dev-server --mode development
            -- OR you can use on the scripts you defined in the package.json file --
            $ yarn run development # or whatever you have defined in there

  }}}

  }}}

}}}

Regular Expressions {{{
  NOTE: If you're looking for regex as it applies to Ruby, you'll want to check out the separate
  regex section you have there. Evidently there are a few quirks here and there that can make Ruby
  different from normal, but overall I've found the syntax to be very comparable to what I've noted
  here.

  NOTE: Now, if you're looking for vim regex, that one is definitely its own animal. Checkout its
  own section for more info

  https://regex101.com
  https://www.youtube.com/watch?v=sa-TUpSx1JA

  Much more available than listed here, but this is a great start!

  Tokens
    .       - Any Character Except New Line
    \d      - Digit (0-9)
    \D      - Not a Digit (0-9)
    \w      - Word Character (a-z, A-Z, 0-9, _)
    \W      - Not a Word Character
    \s      - Whitespace (space, tab, newline)
    \S      - Not Whitespace (space, tab, newline)
    \n      - Newline
    \r      - Carriage return
    \t      - Tab

    \b      - Word Boundary
    \B      - Not a Word Boundary
    ^       - Beginning of a String
    $       - End of a String
    \G      - Start of match

    []      - Matches Characters in brackets
    [^ ]    - Matches Characters NOT in brackets
    [a-z]   - Dash as a range indicator
    |       - Either Or

    ( )        - Match all enclosed, make capture group
    (?: )      - Match all enclosed, do NOT make capture group
    (?<name> ) - Match all enclosed, make named capture group

    (?=...)  - Positive Lookahead
    (?!...)  - Negative Lookahead
    (?<=...) - Positive Lookbehind
    (?<!...) - Negative Lookbehind

  Quantifiers
    ?       - Zero or one
    *       - Zero or more
    +       - One or More
    *?      - Lazy, matches as few as possible
    *+      - Possessive, matches as many as possible
    {3}     - Exact Number
    {3,}    - Number or more
    {3,4}   - Range of Numbers (Minimum, Maximum)

  Conditionals
    (?(1)true|false) - If capture group number is matched, then match the true pattern, otherwise
                       match the false pattern

}}}

Royalty free images {{{
  https://pixabay.com/
  https://www.pexels.com/
  https://unsplash.com/

}}}

Ruby {{{
  resources {{{
    https://www.ruby-lang.org/en/
    https://ruby-doc.org/
    https://rubyapi.org/
    https://github.com/bbatsov/ruby-style-guide

    https://www.ruby-toolbox.com/
    https://www.rubygems.org/

  }}}

  gems {{{
    Here's the lowdown on bundler, RVM, ruby and how all of that relates to gem files
      $ gem ...
        RVM related command, interacts with the gem in a more global/system-wide manner
      $ bundle add/update/etc
        Bundler related command that interacts directly with the gemfile of the active pwd/app
        So running `$ bundle add ...` adds a line directly to the gemfile
      <gemfile>
        Manually setting things up, subsequently carried out with some form of `$ bundle ...`

    nokogiri {{{
      UPDATE: So on the laptop, when I read through the error output, I saw it was complaining that
      some variation of the `mkdir` command could not execute properly. Which is crazy since mkdir
      is a staple command... Turns out all I had to do was create a symlink like-a dees-a:

        $ sudo ln -s /bin/mkdir /usr/bin/mkdir

        Thanks to
        https://stackoverflow.com/questions/62160017/error-installing-rails-error-failed-to-build-gem-native-extension-ubuntu-20-0
        https://stackoverflow.com/questions/61899655/installing-tiny-tds-gem-on-ubuntu-20-04-fails/62169578#62169578

      Now, it's likely this whole situation is caused by some sort of mixup with the system's PATH
      stuff. One way to fix that, evidently, is to do a reinstall of everything related to rvm so it
      will reconfigure all that stuff properly. From what I've read, it seems this type of thing can
      crop up immediately following a system upgrade/update. Which, now that I read that, the laptop
      did go through one of those "Your system has an upgrade available" prompts last night...

      NOTE: For whatever reason, there are times when this gem will have trouble installing. Seems
      to be related to something called libxml2. Here's a shmorgasboard of commands you can try:

        $ sudo apt install libxslt-dev libxml2-dev
        $ sudo apt install build-essential

        $ sudo apt-get install build-essential patch ruby-dev zlib1g-dev liblzma-dev
        $ sudo apt-get install libgmp-dev
        $ sudo apt-get install libxml2-dev libxslt1-dev

        $ sudo apt install libpng-dev
        $ sudo apt install zlibc zlib1g zlib1g-dev

        $ gem install nokogiri --use-system-libraries
          $ gem install nokogiri --use-system-libraries --with-xml2-include=/usr/local/opt/libxml2/include/libxml2
        $ bundle config build.nokogiri --use-system-libraries
          $ bundle config build.nokogiri --use-system-libraries --with-xml2-include=/usr/local/opt/libxml2/include/libxml2

    }}}

  }}}

  irb console {{{
    $ irb -r ./<file> # irb "require" named file, esssentially "opens" the file in irb mode
    $ irb -I . -r <file> # The -I . call essentially loads the entire specified directory into irb, then the subsequent -r
      call opens the desired file itself similar to the above
    NOTE: Okay, so the above commands never really seemed to the do the trick... Here's how
    I eventually got it going:

      cd into the directory
      $ irb -I .
      $ require '<file1>.rb'
      $ require '<file2>.rb'
      => true

  }}}

  metaprogramming {{{
    My understanding so far is that it's based on sending messages and manipulating those messages
    through the use of interpolation.

    #send is inherited from Object:

    http://ruby-doc.org/core/classes/Object.html#M000998

    These statements are equal:

    @person.age
    @person.send :age

    @person.age = 5
    @person.send :age=, 5

    @person.set_age(5)
    @person.send :set_age, 5"
    If you ever metaprogram with method_missing you have to remember to include an else call to
    super and you have also have to deal with respond_to as well. The two methods go hand in hand in
    practice. It can also be worthwhile to memoize the results as you go along to vastly speed up
    the search process

  }}}

  object-oriented language {{{
    When they talk about Ruby being an object oriented language and that everything is an object, in
    practice, this means that everything is available for method calls and stuff like that.  "This
    is a string".length is possible because "This is a string" is considered by Ruby to be a literal
    object that can have the method length called on it. If it wasn't an object, you'd have to do
    something like this length("This is a string")  Does that kind of make sense? This aha moment
    came from p 104/442 in Olsen's Eloquent Ruby. Since superclass and parent methods can be
    overwritten, if you're not familiar with the built-in class methods that come with Ruby, you
    could find yourself inadvertently redefining a core method simply by naming it the same thing.
    For example, say you were to make a method called send. Sounds harmless enough, it describes
    what you want it to do, it follows convention, etc... Except there's one small (read: BIG)
    problem. You just rewrote the Object method called send.

  }}}

  procs and lambdas {{{
    At the outset: I don't recall where I read this from, but to put it in Javascript terms, procs
    are kind of like anonymous functions that you can sprinkle in here and there, complete with
    a way to easily use the result of that function and keep on trucking.

    Also, this section is made of a few different "parts", let's say... When you were writing it,
    you kept adding to it as you came across articles that were ever-more useful at describing what
    these bad boys are. By and large, the latter bits tend to be better, but it's probably most
    useful to give yourself a few more minutes and simply read through this whole shebang at one
    time. Once you feel pretty sure of yourself here, maybe then you can consolidate some of these
    together, but for right now, here's the show. Enjoy :)

    Two essential differences between these little guys, otherwise, they operate the same.

    1) Lambdas are essentially argument-checked procs, meaning if the lamdba asks for three
       arguments but there are only two supplied, for example, it will return an error.

    2) There is also a difference in how return statements are handled. In a lambda, an embedded
       return statement will be overridden by subsequent return statements. Conversely, a proc will
       run its own return statement regardless of any subsequent return statements.

    I'm not 100% comfortable with how return statements are used, so obviously look into this more
    if you end up going this route at some point in time. <EDIT> It has to do with WHERE a proc
    expects to return to when it encounters a return statement versus WHERE a lamdbda expects to
    return to.

    A proc expects to return to where it was defined. In some instances, this happens to be within
    the scope of the main context, which you evidently can't return to/from sometimes, and thus it
    throws an error. This is different from a lambda which expects to return to where the program is
    currently running.

    https://www.youtube.com/watch?v=VBC-G6hahWA
    https://www.youtube.com/watch?v=Km9RlUfmvJc

    Okay let's give it another try! This time from Olsen's Ruby book! A proc is an object that holds
    code. Which, if you think of it, is very much like what a method does... Right? Right. The
    lambda method returns a new proc. So it is often used to make a proc, like'a dis'a

      hello = lambda do
        puts('Hello')
        puts('I am inside a proc')
      end

      NOTE: HOWEVER, I've gotta throw in a caveat here! And that is, that a lambda is specific TYPE
      of proc. Check out the first part of this section to get the lowdown!

        And this proc can then be "called" and used, like'a dis'a
          hello.call
          => Hello
          => I am inside a proc

      They routinely make use of the `yield` statement to output proc results
        def run_it_with_parameter
          puts('Before the yield')
          yield(24)
          puts('After the yield')
        end

        run_it_with_parameter do |x|
          puts('Hello from inside the proc')
          puts("The value of x is #{x}")
        end

        => Before the yield
          Hello from inside the proc
          The value of x is 24
          After the yield

        There's another way to write out the proc definition too, check this out
          def run_it_with_paramter(&block)
            puts 'Before'
            block.call(24)
            puts 'After'
          end

    https://www.skorks.com/2013/04/ruby-ampersand-parameter-demystified/
      Hmmm... So just what the heck is going on here with all this Proc and block business?
      It turns out that in Ruby, ALL methods can take a block whether it's explicitly stated or
      not. It's a given that a method implictly accepts a block (ie if one is given, it will not
      result in any sort of error or exception). So even a simple...

        def hello
        end

      ...can take a block when it is called. It just won't do anything with it.

        hello { puts 'Hello, I'm a block!' }
          Nothing will be output the screen, the empty hello method will be ran and do nothing.

      And THAT'S where yield comes in! It's how you explicitly allow a method to actually DO
      something with a block if one is given! The yield is a way of essentially saying, "Now, let's
      take a moment and yield control to the block, if one was included..."

        def hello
          yield if block_given?
        end

        hello { puts 'Hello, I'm a block!' }
          This call to hello will now know what to do with the block that was supplied!

      Now, here's something interesting... Ruby allows ANY object to be passed to a method and that
      method will, in turn, attempt to use that object as if it were a block. If you put an
      ampersand in front of the last parameter to a method, Ruby will attempt to treat this
      parameter as a block by calling .to_proc on it. If that parameter, when supplied, just so
      happens to already be a Proc object, well then fine and dandy, Ruby will simply use it as
      a block.

      To beat a dead horse, note that the bellow method definition does not EXPLICITLY take a block,
      but it will implictly take one per the underpinnings of good ole Ruby

        def hello
          yield if block_given?
        end

        blah = -> { puts "The arrow symbol is called the stabby lambda, shorthand for 'lambda' }

        hello(&blah)
        => That arrow symbol is called the stabby lambda...

      So... How, then do we define a method to let the user/developer know that it DOES expect to
      receive a block? Why, with the ampersand! Wait... Didn't we just use the ampersand above for
      something else? Yes. Yes we did. An ampersand used in the last argument of a method CALL tells
      Ruby to try to convert it to a Proc and use the object as a block for the method, and the
      method, if it explictly takes a block, will likely do something with it. Use an ampersand in
      front of the last parameter of a method DEFINITION and it indicates that the method expects to
      receive a block and likely will do something with it, if provided. I admit, it's a little
      confusing, but suck it up buttercup, let it sink in, and get over it :)

        def hello(&block)
          block.call if block_given?
        end

        hello { puts 'Hello!' }

      NOTE: Now, when it comes to the &block part in the method definition, you can use pretty much
      any word you want (ie you could also say &hippo_flaps_poo and then use hippo_flaps_poo.call),
      but most people prefer to use block so it's not quite as confusing, and lets everyone know
      what's going on. The same for block.call. It could also be `yield`, but again, for consistency
      purposes, if a &block is used in the method name, it's helpful to refer to it again as
      block.call. Capische?

      So here's an interesting example... See if you can follow it...

        def hello(&block)
          block.call if block_given?
        end

        def world
          puts 'WORLD!'
        end

        method_reference = method(:world)

        hello(&method_reference)

      What do you think happens here? It turns out, the Method object in Ruby implements the
      .to_proc thingy too. So when it goes off and runs method(:world), it does .to_proc, which
      allows it to be used as a block in the hello method call. This is a crafty way to give one
      method as a block to another method.

      He ends it with a great example that ties together all of the above. Notice the interplay
      between explicit and implicit and just regular old argument parameters...

        def blah(&block)
          yadda(block)
        end

        def yadda(block)
          foo(&block)
        end

        def foo(&block)
          block.call
        end

        blah do
          puts "Hello!"
        end

      This also gives us the foundation to understand how something like this works under the hood

        p ["a", "b"].map(&:upcase)

      What the heck is up with the &:upcase stuff? Why, that's the block stuff all over again! The
      ampersand on the last argument of a method CALL tells it to convert it to a Proc, if possible,
      and use it in the method. And lah de friggin dah, the map method's definition is set up to
      explictly take a block. Cool huh?

    From David Black's excellent Ruby for Rails
      A proc carries its context with it. This is very important when it comes to how its scope
      relates to variables. Here's a cool example he gives:

        def call_proc(pr)
          a = "'a' in method scope"
          puts a # 2a
          pr.call # 2b
        end

        a = "'a' in Proc block"
        pr = Proc.new { puts a }
        pr.call # 1
        call_proc(pr) # 2

      What do you think will result?

      1 returns the a defined within the "outer" scope
        => "'a' in Proc block"

      2 Ahh now this one is interesting!
      2a will output the a that was defined within the same method itself, so
        => "'a' in method scope"

      2b though... Since it is a proc, it will be executed within the context it was defined, which
      means it will use the "outer" scope a variable
        => "'a' in Proc block"

      "A piece of code that carries its creation context around with it is called a closure.
      Creating a closure is like packing a suitcase: Wherever you open the suitcase, it contains
      what you put in it when you packed it."

      UPDATE 20201103: Something else I recently found interesting was a little description about
      HOW the procs and lambda things actually work. It turns out it has to do with a little Ruby
      class called "Binding". Taken from https://rubyguides.com/2016/02/ruby-procs-and-lambdas/

        When you create a Binding object via the `binding` method, you are creating an "anchor" to
        this point in the code.

        Every variable, method, and class defined at this point will be availbale later via this
        object, even if you are in a completely different scope

          def return_binding
            foo = 100
            binding
          end

          puts foo
            This won't work since the foo variable is outside of the current scope. BUT, you can use
            the binding object for that :)

          puts return_binding.class
          puts return_binding.eval('foo')
            This will "get" the foo value stored within the method

        Cool, eh? So that's what allows for a lot of the scope magic going on behind the scenes with
        procs and lambdas

  }}}

  regex {{{
    http://www.rubular.com
    Just know at the outset that Ruby's REGEX stuff is structured a bit differently from pretty much
    any other language. So that's why ruby specific sites like Rubular are so important for testing.

    Here's a great valid email regex from Hartl:

      /\A[\w+\-.]+@[a-z\d\-]+(\.[a-z\d\-]+)*\.[a-z]+\z/i

    It can be very beneficial to literally talk through what you're trying to accomplish and then
    build the REGEX stuff from there. "The letter a, followed by a digit" "Any uppercase letter,
    followed by at least one lowercase letter", etc etc. Keep in mind that a REGEX is like a math
    formula. You may use string-like things to type out what you're instructing it to do, similar to
    how you might use variable names, etc, but at the heart of the matter, you're directly dealing
    with things that are very different from strings. You're creating "a set of predictions and
    constraints that you want to look for in a string."

    // is the literal regex constructor. So if you were to type `//.class` => Regexp. Between the
      slahes is where most of the magic happens.

    =~
    .match
      Two methods you can use. They differ in the content and format for how they return results.
      David Black is a HUGE proponent of using .match because it's just flat out waaaaaay more
      powerful and informative than =~ Also, on this point, the MatchData object itself returns
      a boolean value. It either returns "nil" (false) or true along with the information it stores
      about the match. So it's kind of like a two-for-one type thing, which comes in handy when you
      need to evaluate a yes/no type thing for whether a match is returned. How do you invoke
      a MatchData object? Simply save the .match call to a variable and that variable will serve as
      the object. So `m = phone_re.match(string)` would give the variable m the MatchData mojo.

    Literal Characters
      /a/
        matches the string "a" as well as any string containing the letter "a". Any literal
        character you put in a regex matches itself in the string. Some characters are used to
        express certain operations, so if you want to refer to them literally, you must first escape
        the character with a backslash
      /\?/
        matches the string "?" as well as any string containing "?"

    Wildcard
      .
        wildcard, matches any character except for a newline designation.

        /.ejected/ => rejected, dejected, %ejected, etc

    Character Classes
      /[dr]ejected/
        match "d" or "r" followed by "ejected" => dejected, rejected. But NOT %ejected or 8ejected
      /[a-z]/
        range of characters
      /[^a-z]/
        the ^ acts as a NEGATION of the given character class. So /[^A-Fa-f0-9]/ matches any
        character BUT a valid hexadecimal digit.
      /[0-9]/
        matches any digit. It's so common, it has a shorthand expression...
      /\d/
        matches any digit.
      /\D/
        negates any digit
      /\w/
        matches any digit, alphabetical character, or underscore
      /\W/
        negates any digit, alphabetical character, or underscore
      /\s/
        matches any whitespace character (space, tab, or newline)
      /\S/
        negates any whitespace character (space, tab, or newline)

    Captures and parenthetical groupings
        string = "Peel,Emma,Mrs.,talented amateur"

      Talking through it might sound something like this:

      "First some alphabetical characters, then a comma, then some alphabetical characters, then
      a comma, then either a 'Mr.' or 'Mrs.', then a comma, then some alphabetical characters which
      may contain a space..."

      If we want to pull out the person's last name and title, the Regex might look like this:

        regex = /[A-Za-z]+,[A-Za-z]+,Mrs?\./
        regex.match(string) => #<MatchData:0x1234a5b>

      This tells us that it's true, but what now? What if we wanted to have something returned that
      we could actually work with in some way? Well that's where parenthetical groupings come into
      play.

        regex = /([A-Za-z]+),[A-Za-z]+,(Mrs?\.)/

      These parentheses give .match some secret sauce to pull out each of the matches

        puts $1
          returns the match from the first subpattern
        puts $2
          returns the match from the second subpattern
        puts "Dear #{$2} #{$1}," => "Dear Mrs. Peel,"

      m[0]
        returns the full string that hit on the regex match
      m[1]
        returns the first capture, from the first parenthetical grouping
      m[2]
        returns the second capture, from the second parenthetical grouping
      ...  # and so on...

      .captures
        similar to the m[n] stuff above, except it returns all of the captured substrings in
        a single array. So the equivalent of m[0] would be .captures[0] to pull out the first array
        position

      There is also some other cool things you can do to return stuff that happened around the regex
      capture, can help to give you some context and stuff.

        .pre_match
        .post_match
        .begin(n)
          tells you where the nth capture began, on what character index position
        .end(n)
          tells you where the nth capture ended, on what character index position

      Quantifiers
        ?
          zero or one
        *
          zero or more. Greedy, it will keep gobbling up all matches rather that stopping at the
          first match
        +
          one or more. Greedy

      Repetitions
        {n}
          n number of times. /\d{3}-\d{4}/ would match a string of 555-1212
        {n,}
          n or more
        {n,n2}
          a range from n to n2. {2,10} would match 2-10, but not 0, 1, or >=11

        NOTE: These can apply to individual characters, classes, and any regex atom. So for
        example...

          /([A-Z]\d){5}/

        matches five consecutive occurences of an uppercase letter, followed by a digit

      Anchors
        ^  # beginning of line
        $  # end of line
        \A # beginning of string
        \z # end of string
        \Z # end of string, except for final newline
        \b # word boundary. I think each word effectively has two boundaries, one at its beginning
             and one at its end.

      Lookahead Assertions
        These serve as a means of vetting out a string before running a .match on it. For example,
        "Match this sequence only if it ends with a period."

          ?=  # Positive assertion, that the given is present
          ?!  # Negative assertion, that the given is NOT present

        So here's an example of how this might work. Note the included positive assertion specifying
        to only match a sequence that ends in a period

          str = "123 456. 789"
          m = /\d+(?=\.)/.match(str)
          m[0] => "456"

      Modifiers
        p 328

  }}}

  singleton methods {{{
    "A singleton method defined in a class is commonly referred to as a class method of the class on
    which it is defined. The idea of a class method is that you send a message to the object that is
    the class rather than to one of the class's instances. The message most_expensive (as in
    Ticket.most_expensive) goes to the class Ticket, not to a particular ticket instance." - Well
    Grounded Rubyist, p 113 of 519

  }}}

  structs {{{
    https://ruby-doc.org/core-2.7.0/Struct.html

    These are basically Class-lite objects that store/organize data. Typically used as a way to
    in-line a simple Class structure.

  }}}

  syntax cheat sheet {{{

    ::CONSTANT
      Double colon, followed by all caps. Some people just cap the first letter, but I prefer the
      all caps, it pops more. The double colon acts as a sort of file directory call to start at the
      "root". Kind of like tilde ~ in linux, it's telling Ruby to search for the varible but to
      start at the highest level rather than down inside the program, where the name may be
      duplicated as a class. Because remember, since class names also start with a capital letter,
      there needs to be a way to tell Ruby you're really trying to locate a Constant, which is where
      the :: comes into play.

    class Name
      class names start  with a capital letter

    @instance_variable
      visible strictly per object. Object, of course, meaning per class, per instance.
      UPDATE: This means that they are visible/accessible to "inner" methods within the class. Which
      is very different from how standard variables operate.

    @@class_variable
      "Provides a storage mechanism that's shared between a class and instances of that class,
      that's not visible to any other objects"

    "123abc".to_i
      Becomes 123. The alphabet characters are ignored.
      You can also add an argument to covert the string into a desired base value.
        "1000".to_i(2)
        => 8 # Since, in base two, 8 is expressed as 1000

    Integer("123abc")
      Using the stricter version of to_integer, the constant
      Integer, will only translate number strings to integers, not number letter
      mixes.

    The same thing holds true for to_f and Float()."

    n..m
      TWO DOTS Range of values from n to m, including m.

    n...m
      THREE DOTS Range of values from n to m, excluding m."

    On combining strings together…
      The use of '+' always creates a new string object with the pieces together.

      The use of '<<' permanently appends the two together.

      each_with_index and *.with_index are two very powerful ways to cycle through hashes.

      for arrays, flatten and compact are pretty interesting.

      each
        returns its receiver, the value returned by the block is discarded every cycle

      map
        on the other hand, map creates an array of the accumulated values returned by each block
        cycle

      It's important to note here that just because the code executes doesn't mean the right thing
      was done with the data itself. for example...

          array = [1, 2, 3, 4]
          result = array.map { |n| puts n * 100 }

      The puts code will execute each time, and you'll see output of the math.
      HOWEVER, because the return value of puts is always nil, the result array map will contain
      [nil, nil, nil, nil]

    See Well Grounded Rubyist 10.6.1

    "on hashes, you can use the command: Hash["key", "value", "key", "value",...] and it will pair
    the two up appropriately

    Update is destructive, merge is nondestructive

    A set is a collection of objects that enforces uniqueness in the content (ie there can't be two
    strings "New York"). Part of the standard library, not core Ruby, which means in order to access
    sets you have to use "require 'set'"

    === (threequals) is a test on the type of argument objects, not a direct test on content. So the
      following are all examples of threequals:

        String === 'foo'
        Array === %w(one two three)
        Range === (1..10)
        /car/ === 'carpool'

      Where it gets kind of interesting though is that this is also a proper use of threequals. It's
      specifically applying a range

        test to determine the age
          def teenager?
            (13..19) === age
          end

      The question threequals asks is not one of equality, but someting more along the lines of, "If
      I have a drawer labelled a, would it make sense to put b in that drawer?" Or, "If a described
      a set, would b be a member of that set?"

    grep
      can be a powerful "searching" tool to search for === things.

    comparison statements
      When it comes to comparables in Ruby, you need to fully list out the statements on each side
      of the operator. So for example, you can't do

        if a == b || c

      That translates to (a == b) || c

      What you're probably after, in that instance, is something like this

        if a == b || a == c

    <=> # The aka the spaceship, enables you to use all forms of comparison operators

      include Comparable
      ...
      def <=>(variable_name)"

    //
      contains a regular expression, all the regex gobbledegook goes between the slashes. Responds
      to 'match' as in

        /abc/.match("The alphabet starts with abc.")

        and to =~ which serves as a means of matching literal patterns as in

          "The alphabet starts with abc" =~ /abc/
            => 25

      NOTE: match stops at the first complete hit whereas scan continues searching through the
      string. The StringScanner class is quite powerful, gives a lot of additional tools to mix in

    p <var>
      p is shorthand for <var>.inspect

    puts always returns nil. This is why when you do `puts a`, the string will be shown, then on the
    next line => nil.

    Another cool thing is Regexp.escape()
      The use of escape makes sure periods any other potentially nasty characters that may be part
      of strings are first properly escaped into the regexp before putting it through the ringer.
      For example, say you have  a string "a.c" that you pass into a regexp. Since in the regexp
      world, the period counts as a wild-card, it's inclusion in the string could really fubar your
      results. By using Regexp.escape , the string will automatically be formatted into "a\\.c".
      Very cool!"

    You REQUIRE file names (extensions are optional) and INCLUDE module names (aka mix-ins)

    On comparing text strings, upper case letters come before lower case letters, so "Xerox" comes
    before "baseball". To avoid this confusion, best to either upcase or downcase the characters in
    the strings before the comparison is made.

    REGEX!!! The match operator is =~
      Don't use ==, that implies a literal match and obviously that's not what you're after with
      regex"

    On modules… The convention is to name a module as an adjective since it represents additional
    behavior.

    include = all of the module's instance methods become instance methods in the class

    extend = all of the module's instance methods become class methods in the class

    prepend = adds module methods to a class, but inserts them BEFORE the class's methods. This is
    a way to ensure that if there are two methods with the same name, prepend will ensure the
    module's method is the one to be executed. In this way, it can be used to override method
    definitions"

    !! _xyz_
      leading with double-bang converts and object to its corresponding boolean value example:

        !!user.authenticate("foobar") will return => true if the password matches.

    <<
      I just applied this successfully for the first time, so I'm sure I have more learning to do,
      but I want to write about it here just to get it on paper... On a many-to-many association
      with a join table... Let's say you have unique values in Table A that you've associated with
      values in Table C, which naturally creates the fk stuff in the joining Table B. Now... If you
      then want to create new associations with existing values in Table A, because you're enforcing
      uniqueness with that table, if you were to do tablec.tablea.create(blah blah blah) you'll more
      than likely get a validation error because it'd be trying to create new values in BOTH tables.
      To simply USE existing values from Table A, and still create the full association, that's
      where << comes into play. Here's how it works...

      section1 = Section.create(number:  1)
        creation straight into the sections table so the value already exists in the database.
      county24 = County.find(24)
        this is the county we want to "add" the existing section to.
      county24.sections << section1
        using << allows us to make the appropriate association without inadvertently trying to
        create "another" section number. Cool huh!?

    <<-HEREDOC
      ...
    HEREDOC

      This is a sort of multiline string designation. The HEREDOC word can be anything you want it
      to be, the only rule is that the two tags need to match since they serve as a sort of
      start/end indicator. Another little quirk about tit is that it explicitly adheres to whatever
      whitespace and newlines you put in it. Which means, unless you want it to have tab/spaces in
      the text, do not indent the string content when you're writing out the HEREDOC. For example,
      here's how it might look:

        class Bottles
          def verse(number)
            <<-VERSE
        Verse text here.

        Notice how the text is not inline with the normal indentation of the code, but is instead
        pushed all the way to the left side so it won't contain any leading whitespace.

            VERSE
          end
        end

  }}}

}}}

RVM {{{
  NOTE: Some advocate using RVM just for ruby versions and leaving gem files and their updates, etc
  to be handled by Bundler. Using an app's gemfile gives you fine control over the gem version and
  it also helps make it more portable in the sense that it's easy to pack up and give to someone
  else to "recreate" your gem setup.

  Installing rvm  {{{
    UPDATE: Use your installrvm1 installrvm2 aliases

    https://rvm.io
      for latest install files and process, including the initial key setup command you need to run.
      You basically run that initial gpg2 command, if you receive notice that you don't have gpg2
      installed, get'er here

      $ sudo apt install gpgv2 gnupg gnupg2

    $ sudo apt install curl
      This should already be installed if you followed the Initial Install steps above
    $ \curl -sSL https://get.rvm.io | bash -s stable
      After this is done, close and reopen the terminal window
    $ source ~/.rvm/scripts/rvm
    NOTE: If you're doing this as part of an rvm reinstall process, run the below before continuing
    further...
      $ rvm reset
    $ rvm requirements
    $ rvm install ruby
      defaults to latest ruby version in `$ rvm list known`
    $ rvm use ruby --default
      NOTE: If this throws an error, you likely need to first run `$ /bin/bash --login`
    $ rvm rubygems current
    $ gem install bundler --no-document
    $ gem install rails --no-document
      I tried leaving this one off, to instead allow the gemfiles to handle it, but ran into some
      issues.

    And after that, you're good to go with your base rvm setup. The next step after this is to tap
    into rvm's real bread-and-butter which is creating and maintaining separate gemsets. Continue on
    down to 'Using rvm' for that :)

    If you ever want to get the ri and rdoc information, you can use the following commands:

      $ gem rdoc --all --overwite
        regen ALL docs, warning this can take a looooong time, the files can be large
      $ gem rdoc <gem_name> # generate docs for named gem

    NOTE: The shortcut edit immediately following should no longer be necessary now that your
    dotfiles edits the bash file appropriately:

        On the menu, navigate to the shortcut for LXTerminal. Right-click, and select properties. In
        the window that pops up, find the "Command" textbox, and change the text to lxterminal -e
        "bash -il" # This allows rvm to work with the LXTerminal. Do this AFTER rvm has been
        installed per the above steps

}}}

  Using rvm{{{
    Now, a cool thing with rvm is that it can work off of predefined gemsets. Practically speaking,
    this allows for multiple sets of gems (either the same or different versions) to be installed on
    a system, ready and available to switch between at will.

    To set this up, here's what you do:

      $ rvm all do ruby -v
        This will show you the ruby version/s you have installed
      $ rvm use <ruby_version_number> --default
        This sets up the version of ruby you'd like rvm to use by default
      # Navigate to your project's root directory
      $ rvm gemset list_all
        list all available gemsets
      $ rvm gemset use <gemset_name>
        NOTE: If you're coming back from a reinstall of rvm and/or your application already has the
        ruby-version and ruby-gemset files that rvm requires, you don't need to create the gemset
        manually. From my experience thusfar, if these files are present, rvm will do the setup for
        you to accomodate that configuration. Very cool!

        NOTE: As of 20180208, whenever I navigated into the directory, the rvm files were
        automatically created and this was noted in the terminal. Very cool! If for some reason it
        does not do that, then continue as follows:

          $ rvm --create --ruby-version use ruby-<desired ruby version here>@<gemset name here>
          Example: $ rvm --create --ruby-version use ruby-2.2.1@rails4inaction_ticketee

          This will create a few things, including two files, ruby-version and ruby-gemset. This
          ruby-gemset file is what tells rvm to associate installed ruby gems with a particular
          project. This way rvm installs only one copy of gems and then pulls those into the project
          based on this gemset.  Pretty coolio!

          Now, if your project already has those files in there and you see some sort of notice
          saying that ruby version blah blah blah is not installed, what that probably means is that
          your installed ruby version differs from the ruby version that's named in the projects
          ruby-version file. To let the app know you want to use a different ruby version, simply
          run through the "create" actions named above, naming your currently installed version. It
          will then "backup" the existing rvm files in the project and create two new ones.

    To add and remove gems into the active gemset, you basically have two approaches available to
    you at this point. If you're maintaining an application specific Gemfile.lock file and you want
    to install those gems, you can navigate to the application's root directory then use 'bundle'
    commands:

      $ bundle install
      $ bundle update
      etc

    If, however, you'd like to install gems onto the system irrespective of any Gemfile.lock file,
    you can install them directly with rvm by using the following:

      $ gem install <gem_name>
      $ gem uninstall <gem_name>
      $ gem list

    One reason you may want to do this is to "test" gem upgrades. So let's say you're rocking Rails
    5.6 and want to test 6.0 before you transition the project fully. You could create a copy of the
    gemset, name it something else, install rails
    6.0, then tell the app to 'use' that new gemset. If things go well, you can make the change to
      the application's gemfile. If not, then switching back to the original gemset is easy as pie
      and you're right back where you started.

    If there is a gem you'd like to install to a certain gemset (ie into @global), you can do
    something like this

      $ rvm <gemset> do gem install <gem name>
        $ rvm @global do gem install asciidoctor

      Another thing you can do is go into ~/.rvm/gemsets/global.gems and add whatever global gems
      you'd like to have in there.

  }}}

  Upgrading RVM, Ruby, and Gemsets  {{{
    To see the latest version of Ruby, check out this page: http://www.ruby-lang.org/en/downloads/

    NOTE: Before you kick this off, be sure you're out of anything that's using rvm and/or ruby (ex,
    land_app)

    There are TWO ways of doing this. One uses the built-in 'update' stuff, whereas the other is
    manually done.

    Built-in {{{
      $ rvm all do ruby -v
        This will show you the ruby version/s you have installed

      $ rvm list known
        Displays a list of known Ruby and related versions that are "supported" by rvm. If you do
        not see the desired ruby version listed here, then you have two options:

          1) Wait a bit longer until it is supported, usually this happens once it's confirmed to be
             stable and reliable, or
          2) force an update to the beta-esque version of rvm with the following command, which
             should then list the latest ruby:

              $ rvm get head && rvm reload && rvm list known

      $ rvm get stable --auto-dotfiles
        Run this to update rvm to the current STABLE version, the auto-dotfiles part is to make it
        compatible with how you structure your dotfiles

      $ rvm install ruby --latest
        This installs the latest version of Ruby. If you want a specific version, do

        $ rvm install <desired Ruby version number>
          example: rvm install 2.3.1

      $ rvm all-gemsets do gem cleanup
        This will get rid of any old/outdated gems first, which should help make the next step
        complete a little faster

      $ rvm upgrade <curr ruby version> <desired ruby version>
        example: rvm upgrade 2.3.0 2.3.1   This will then walk you through automatically migrating
        things over from 2.3.0 to 2.3.1. Note that it also migrates over all of the corresponding
        gemsets as well. Very cool!

      UPDATE 20200407: I just completed the built-in upgrade from 2.6.3 to 2.7.0 and here's one more
      step I noticed. Whenever I went back into the land_app project and checked the ruby-version
      and ruby-gemset files, the ruby version still pointed to 2.6.3. In order to bring it up to
      date to 2.7.0, I ran the following:

        $ rvm --create --ruby-version use ruby-2.7.0@land_app

      This updated those files accordingly and also created timestamped copies of the previously
      used files.

      Then I went back to the terminal for land_app and ran

        $ gem install bundler --no-document
        $ bundle install

      And while everything then completed okay, I'm confused at this point... I was under the
      impression the upgrade process was supposed to migrate those gems over seamlessly. If that's
      the case, then why did this bundle command appear to do a full reacquire and install for all
      of the gems?

        I think it may have been because I switched over to some sort of "use land_app", when
        really, I should have told it to keep using 'global' because that's likely where all of
        those migrated gems were moved over to

        NOTE: Now, when upgrading from ruby 2.4.1 to 2.5.0, I ran into several issues with rvm. {{{
        First, it was not yet listed in the `$ rvm list known`. And secondly, I didn't know yet
        to try the `$ rvm get head` command. Nonetheless, here's how I eventually went about fixing
        the situation. RVM gave a warning about needing to reinstall libyaml and ruby.

            $ rvm requirements # This made it go through several things, including "cleaning" all the gemsets with pristine
            $ gem update

        # UPDATE: See troubleshooting if you run into other issues, you have some more notes there

        }}}

    }}}

    Manual {{{
      If for some reason you want to go through the upgrade process yourself, you can do it manually
      with the following:

        $ rvm gemset copy <source> <destination>
          This does what it sounds like
          $ rvm gemset copy ruby-2.4.1@land_app ruby-2.5.0@land_app
            to copy a specific gemset
          $ rvm gemset copy 2.4.1 2.5.0
            to copy all related gemsets
        $ rvm migrate <source> <destination>
          Similar to copy, but it also deletes the source directory after the files are moved over

    }}}

    After all that's done, there's a good chance your bundler will be out of date, so then run
      $ gem install bundler --no-document
      $ bundle install

    There may be some old cobwebs hanging around in RVM, clean'em up with:
      $ rvm cleanup all
      $ gem cleanup --dryrun
      $ gem cleanup

    I read also that running this can help clean up some stuff related to the "Deprecated with no
    replacement" warnings
      $ gem pristine --all --no-extensions

      If that one doesn't work, give this'n a try
        $ rvm rubygems current

  }}}

  Updating gem files {{{
    NOTE: Okay so I've since learned a bit more about how rvm and bundler related to applications.
    Whenever you use a 'gem' command like 'gem update', what you're doing is telling rvm to update
    gems installed on the system IRRESPECTIVE of how their versions may pertain to your development
    applications. This is why, even though you had Rails 5.6 specified in land_app, running 'gem
    update' downloaded Rails 6.0. Because rvm by itself does not "know" about the Rails version you
    have in your application's gemfile. This is why it may be a better practice to stick with
    "bundle update". HOWEVER, that said, there are reasons why you may want to update things outside
    of your app's setting. For example, let's say you want to try out Rails 6.0 on your app, but you
    don't want to "risk" the current setup you have on Rails 5.6. Well this is where downloaded
    Rails 6 with "gem update" will get the files, and then from there, you can create a new rvm
    gemset that uses Rails 6.0. This allows you to effortlessly switch between the two versions
    without any fear of fubaring your app in the process. Does that make sense?

      https://stackoverflow.com/a/20224856
      The main difference is that Rubygems (invoked with the command gem) manages all the gems
      (gemsets if you are using RVM) for a single machine, whereas Bundler (bundle) manages a gem
      set for a single application (its purpose being to deploy on multiple machines).

      The Ruby Version Manager (rvm) only eases the task of managing different gem versions on the
      same machine, but it's not application-related unless you want to name a gemset for
      a particular application (see named gemsets).

      Both will ignore any previously installed gems and resolve all dependencies again based on the
      latest versions of all gems available in the sources. However Bundler, unlike Rubygems, will
      only update gems and dependencies specified in the application's Gemfile, complying with its
      restrictions (version numbers and spermises).

      That said, you should use bundle install instead of bundle update, to be sure you are
      installing the same exact gems and versions across machines.

    First, switch to the appropriate gemset
      `$ rvm gemset list_all`

    Recommended that you use global:
      $ rvm gemset use global
      $ gem outdated
        lists all the outdated gems
      $ gem update --no-document
        updates all the listed gems to their latest versions

      $ gem update --system

    If you want to target a particular gem, use '$ gem update <gem_name>'

      $ bundle update
        updates all gems listed in the gemfile
      $ bundle update <gem_name>
        updates only the listed gem found in the gemfile

    From there, if you've cloned a repo or have a gemfile already set to go, you can run your usual
    commands and rvm will do its thang

    $ bundle install
      If you get notice that bundler is not installed, DO NOT run the recommended "sudo apt"
      command, use "gem install bundler" instead. This is a **VERY** important point. With rvm in
      general, one of the big things it brings to the table is a workaround so that users do NOT
      need to use sudo commands, which may expose the system to unwanted effects/attacks/etc.  So as
      a rule of thumb, once you're using rvm, unless you are specifically wanting something to be
      installed system wide (like curl for example), use the non-sudo versions

    Some other handly gem related commands I've come across
      http://guides.rubygems.org/command-reference/#gem-check

      $ gem list
      $ gem check
        This has a lot of useful options you can run too. I don't know what to do if problems are
        found...
      $ gem cleanup

      NOTE: I've ran into a couple times where rvm wasn't able to connect to the https version of
      rubygems.org. If you ever need to change to the http version, here's what ya do:

          $ sudo gem sources -r https://rubygems.org
          $ sudo gem sources -a http://rubygems.org

        Then, highly recommend setting it back to https so it will be used when available next time
        with:

          $ sudo gem sources -r http://rubygems.org
          $ sudo gem sources -a https://rubygems.org

  }}}

  Removing old gem versions {{{
    $ gem cleanup --dryrun
      displays a list for what will be removed
    $ gem cleanup
      executes the deletion process

    If there's a specific gem you want to cleanup, specify it like'a so'a
      $ gem cleanup <gem_name>

    $ rvm all-gemsets do gem cleanup
      This is another approach that may be more thorough than the above

    Now, there are times where, even after a gem cleanup process, you still see what appears to be
    older gem versions still hanging around. Chances are, however, these old versions are being kept
    due to some dependency that may exist in another gem/s. To check for this, use

      $ gem dependency <gem_name> --reverse-dependencies

    If that doesn't return anything, then it's safe to proceed with manually removing the gem with
    the following

      $ gem uninstall <gem_name> --version <123>

}}}

  Uninstalling gemsets or rvm  {{{
    $ rvm gemset empty <gemset_name>
      Empties out all gems in the gemset, but retains the gemset itself
    $ rvm gemset delete <gemset_name>
      Deletes gemset
    $ rvm remove <ruby_version>
      removes ruby version and all related gemsets
      `$ rvm remove 2.4.1` removes ruby-2.4.1 and all related gemsets

    To completely remove rvm, use the following commands:
      rvm implode
      gem uninstall rvm

  }}}

  Troubleshooting  {{{
    Error running '__rvm_make-j2' {{{

      Received this error while trying to install ruby 2.3.1, it popped up during the compiling
      stage. Here's what got me back on track:

        $ rvm get master
          NOTE: If this command doesn't go through for some reason, first run '$ rvm cleanup all``
        $ rvm reload
        $ rvm pgk install openssl
        < Then proceed with your normal install procedure, `$ rvm install ruby 2.3.1` or whatever>

    }}}

    Upgrade woes {{{
      Upgrading from ruby 2.4.1 to 2.6.3 threw me for quite a loop. The standard

        $ rvm upgrade 2.4.1 2.6.3

      command failed whenever it tried to migrate the gemsets. At the end of the day, here's what
      I had to do:

        $ sudo apt install libtool
        $ rvm pkg install libyaml
        $ rvm reinstall all --force

          NOTE: During this process, it took a LONG time to go through the 2.4.1 reinstall (we're
          talking like, 30+ minutes), and I noticed it was stuck on "importing gemset..." so
          I pulled up the latest .rvm/log file (gemsets.import.gobal.log), scrolled to the bottom,
          and noticed that the following user prompt was listed:

            bundler's executable "bundle" conflicts with
            /home/linux/.rvm/rubies/ruby-2.4.1/bin/bundle
            Overwrite the executable? [yN]

          On a whim, I went back to the "frozen" reintsall terminal, typed `y <cr>` and BOOM! It
          finished it right up!

      Once that finished, I went back into the root land_app directory and ran the following:

        $ rvm use 2.6.3
        $ rvm gemset use land_app
        $ rvm remove 2.4.1
          get rid of the old ruby and related gemsets
        $ rvm gemset delete ruby-2.4.1@land_app
          for some reason this little guy was still hanging around, this command got rid of'em

        $ gem install bundler --no-document
        $ bundle install
        $ bundle update

        $ rvm cleanup all
        $ gem cleanup --dryrun
        $ gem cleanup

      Also, later down the line, when it came time to `bundle install/update` everything, I had an
      issue with the capybara-webkit gem not going through. Turns out there are a few more qt
      related packages you need to install on the system in order for it to go through, as detailed
      here:

        https://github.com/thoughtbot/capybara-webkit/wiki/Installing-Qt-and-compiling-capybara-webkit#debian--ubuntu

        $ sudo apt-get install g++ qt5-default libqt5webkit5-dev gstreamer1.0-plugins-base
          gstreamer1.0-tools gstreamer1.0-x

    }}}

    can't check signature: public key not found {{{
      When this happens, you may need to try out a different keyserver, see the below

      https://github.com/rvm/rvm/issues/4573
      gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB

    }}}

  }}}

}}}

Static site generators{{{
  eleventy javascript {{{
    https://11ty.dev

  }}}

  jekyll ruby {{{
    https://jekyllrb.com/

  }}}

}}}

Testing {{{
  Mocks
    Used to create "stand-in" classes for testing purposes
    In rspec, mocks are called "doubles"
      https://www.tutorialspoint.com/rspec/rspec_test_doubles.htm

  Stubs
    Used to create "stand-in" methods for testing purposes
      https://www.tutorialspoint.com/rspec/rspec_stubs.htm

  Test
    Validates that a bit of code is working properly

  Spec
    Describes the desired behavior for a bit of code

  Example
    Shows how a particular API is intended to be used

  Unit tests or Model tests {{{
    Test the models/tables and their interaction with the database

    Common areas of testing include:
      validations
      associations
      scopes
      business logic

  }}}

  Controller tests {{{
    Test the controllers, which serve as a middleman between the models and the views. The goal here
    is to test what happens after a request is made by the user, not necessarily the business logic
    behind it

    Common areas of testing include:
      status codes
      content types
      layouts/templates rendered
      flash messages
      inserts/updates/deletes
      redirects

  }}}

  Integration tests {{{
    UPDATE 20190928: Watched a youtube video that touched briefly on this. So an integration test is
    not characterized by it testing a view layer (which is what you were thinking below), but rather
    one that incorporates one or more dependencies. So whereas a unit test looks at a fully isolated
    function or piece of code, an integration test comes into play whenever what you're testing
    relies on a result or output that comes from another function or piece of code.

    UPDATE: Yeah, so get this! An integration test looks at the resulting views that are shown to
    the user, BUT! it is unable to take javascript into account. For that, you need to turn to
    a system test :)

    Test the views and how they are displayed to the user by way of the browser. Appropriate to
    think of as front-end tests?

    What I've described above may technically be classified more as a system test...

    In other books and stuff integration tests are described more as tests that look at how the
    various moving parts of other parts of the application (model, controller, etc) work together to
    produce a desired outcome. So it's kind of like testing the final end-result, though still from
    a "code" perspective rather than a front-end browser interaction perspective. Perhaps that
    description is more accurate after all?

  }}}

  System tests {{{
    General info and explanation {{{
      https://blog.appsignal.com/2020/02/12/getting-started-with-system-tests-in-ruby-with-minitest.html

      UPDATE 20200714: When it comes to running system tests in a Rails app, before Rails 6, this
      was typically handled with a combination of selenium webdriver and capybara. The good news
      here is that selenium is a tried-and-true been-around-forever veteran. The bad news? It may be
      a bit outdated given modern browser capabilities. For this reason, you may consider replacing
      selenium with something purpose-built specifically for Ruby, a two-punch combo called Ferrum
      and Cuprite.

        https://github.com/rubycdp/ferrum
        https://github.com/rubycdp/cuprite
        The main advantage? SPEED! Ferrum takes full advantage of Chromium's async capabilities


      UPDATE 20190928: This may be described as "end-to-end" testing or "full flow" testing. The key
      piece that's brought in here is that DOM manipulations are typically included here. In order
      to pull this off, you'll likely to to incorporate some sort of web emulator or driver. My
      understanding is this is where capybara and selenium comes into play if we're talking about
      a Rails environment. In order to save processing power and stuff, a "headless" web driver can
      be used so that the browser interactions are done "behind the scenes" rather than having to
      literally open up a browser and visibly walk through the motions.

      The main draw here is that system tests do include javascript related interactions

      This style of testing is often referred to as "happy path" testing because it tries to cover
      the ideal use case from the user to make sure that things come together properly from the
      end-result browser perspective.

      Rails has a test/system directory, this seems to be where the selenium/capybara-esque test
      code is supposed to go.

      This system test environment is configured and customized in the
      `test/application_system_test_case.rb` file

    }}}

    Selenium {{{
      https://www.selenium.dev/documentation/en/

    }}}

    Capybara {{{
      It's my understanding thusfar that capybara relies heavily on css-selectors which is why
      I included the link here

      https://www.rubydoc.info/github/jnicklas/capybara/master/Capybara
      https://www.rubydoc.info/github/jnicklas/capybara/Capybara/Minitest/Assertions
      https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors
        :nth-of-type() is pretty friggin cool

      Cheatsheets
      https://devhints.io/capybara
      https://kapeli.com/cheat_sheets/Capybara.docset/Contents/Resources/Documents/index
      https://gist.github.com/zhengjia/428105

      You can access the Selenium API from within Capybara.

        page.driver
          this "returns" the Selenium driver, and then from there, you can run its methods

        page.driver.browser.action.pointer_down(:left)
          for example

      If you try to "mix" a Capy finder with a Selenium method, you'll first need to "convert" the
      Capy result into an object that Selenium is expecting by calling `.native` on the object.

        example = find("#example")
        example = example.native

    }}}

  }}}

  Tons of other test types and focuses out there

  rspec {{{
    At first, I really resisted moving over to rspec since I felt that MiniTest took care of
    everything I was needing. Which, for personal use, may be true. But when it comes to software
    dev, there's no getting around the fact that a LOT of dev teams use rspec.

    Install {{{
      gem 'rspec-rails'
      $ bundle install
      $ rails g rspec:install

    }}}

    Additional tools to consider {{{
      Most of these are taken from Obie Fernandez's The Rails 5 Way

      https://github.com/guard/guard-rspec
      https://github.com/briandunn/flatware
      https://github.com/travisjeffery/timecop
      https://github.com/bblimke/webmock
      https://github.com/vcr/vcr

    }}}

    Setup {{{
      .rspec # add the following
        --color
        --format documentation

      NOTE: Another way to set the --format documentation as the default is to edit the configure
      block in the spec_helper.rb

        Rspec.configure do |config|
          ...
          config.formatter = :documentation
        end

      spec_helper.rb # uncomment the following block
        if config.files_to_run.one?
          # Use the documentation formatter for detailed output,
          # unless a formatter has already been configured
          # (e.g. via a command-line flag).
          config.default_formatter = "doc"
        end

    }}}

    Terminology {{{
      an expectation is like an assertion in Minitest and other testing frameworks

    }}}

  }}}

  On testing private methods {{{
    NOTE: Are you sure you want all of these as private methods? My initial reasoning is that 1)
    they do not act directly upon an @image record and 2) they're used more as calculators. And
    since these calculations rely upon Ruby's Math modules, what you're essentially testing is the
    accuracy of their results. Now, the other part of me says I'd like to have a double-check for
    the formulas themselves, but that seems redundant. Because formulating an accurate test value
    depends on using a proper formula. From what I've read online, some users feel testing private
    methods is a 'must have' for rooting out potential bugs while other users feel the testing
    domains should be largely limited to only public methods. That tests should only cover the
    public behavior and NOT the internal implementations.

    The best argument I've heard for this is that, if you start tying your tests too closely with
    your implementation, if/when you want to try a different approach, you'll be forced to rewrite
    any/all related tests. When really, at the end of the day, what you're after is BEHAVIOR! That
    was DHH's argument and I think by the end of it, even Fowler came around to it a bit. Beck had
    a hard time making that leap, though he did seem to concede that it's possible it comes down to
    a difference in their own personalities and make-ups, and the level of ambiguity each can live
    with.

    DHH's other point on this front was that chasing "five nines" everywhere is a complete waste of
    time (ie have 99.999% coverage, virtually every line of code) because the sheer amount of work
    and dedication to go from, say, 90 to 95 may be extensive, and from 95 to 99.999 may be even
    more herculean. And for what use? To what end?

    DHH's main motivation seemed to be that the purpose of tests was to aid in getting code into
    production. Not to become a driving force, an end-all-be-all in and of itself. Because in his
    mind, to do so, was to largely lose sight of the goal your'e ultimately after.

    Now, that said, I got the impression that Beck and DHH come from different programming
    "realms". Where Beck may be more of a maintainer and streamliner and educator, etc DHH is
    first-and-foremost product driven. DHH must get things out to market. Whereas Beck? Maybe not
    so much. Or at least, if he does, he's not the one "close to the bone" trying to develop new and
    useful software as his primary means of income.

  }}}

}}}

Transcript services {{{
  https://app.getwelder.com/transcriptions
  https://simon.kde.org/
  https://kaldi-asr.org/
  https://commonvoice.mozilla.org/en
  http://julius.osdn.jp/en_index.php
  https://dictationbridge.com/
  https://mycroft.ai/
  https://www.nuance.com/dragon.html

}}}

typing {{{
  keybr
    https://keybr.com

  klavaro
    $ sudo apt get install klavaro

}}}

Vim {{{
  Initial installs   {{{
    UPDATE: You've since organized this whole chirade into the bash alias `installvim`

    $ sudo apt install vim vim-nox vim-gtk
      Though I'm not 100% sure this is the same thing as what you'll install if you search for "vim"
      in the package manager... Whatever it's worth, the one named in the package manager is the one
      you're after. vim-nox adds lua compatability, which is required for certain plugins like
      neocomplete. vim-gtk adds support for xterm_clipboard so you'll be able to copy to the paste
      from the system-wide reigster for linux. To double check, run the following in terminal

        $ vim --version # and then look for "+lua" and "+xterm_clipboard".

    ~/.vimrc and other dotfiles now backed up and accessible from bitbucket!
    See .dotfiles_readme for instructions on how to get this set up

    ack {{{
      an "extra" program that can come in handy for searching and stuff with vim, also allows the ag
      and fzf plugins do their thing:

        $ sudo apt install silversearcher-ag

        $ pacman -S the_silver_searcher
          NOTE: For arch linux distros like manjaro

    }}}

    ripgrep {{{
      Supposedly even faster than ack and silversearcher. Can be used in place common `grep`
      commands in Linux too. I tried it for a bit after setting it up with FZF and quite frankly,
      I couldn't tell that much of a difference for how I currently use FZF and, further, it was
      a bit more rough around the edges with how it interacted with the FZF window. So I presently
      moved back to ack, though this may be worth exploring further if FZF fleshes out its support
      for ripgrep, etc etc

        https://github.com/BurntSushi/ripgrep

      UPDATE 20180809: There is another way to install it now. I included these notes within the
      .vimrc file
      NOTE 201804: Unfortunately, at the time of this writing ripgrep is not part of the official
      Ubuntu repositories. So in order to install it, you must take one of two routes.

        1) Use another package management tool that comes with Ubuntu called snap.

          $ sudo snap install --classic rg

        2) Use curl to pull in the files then install the package manually

          $ curl -LO https://github.com/BurntSushi/ripgrep/releases/download/0.8.1/ripgrep_0.8.1_amd64.deb
          $ sudo dpkg -i ripgrep_0.8.1_amd64.deb

    }}}

    Capslock key {{{
      UPDATE 2: I've since abandoned the jk/kj stuff and started using the built in <c-[> to go back
      to normal mode
      UPDATE: I've since learned of a setup within vim that does not require any changes to the CAPS
      key. It instead maps the dual press of jk/kj to <Esc> within vim.

      $ cd /usr/share/X11/xkb/symbols
      $ sudo vim pc
      # cursor down (with HJKL) to change the following line to read:
      key <CAPS> { [ Escape ] };
        default is Caps_Lock
      :wq
        this writes the file (ie saves it) then closes it
      Log out and log back in and viola!

    }}}

    Javascript {{{
      Enhanced functionality for things like "go to function definition" and stuff like that
      UPDATE: From what I've read, it doesn't seem that ctags or cscope supports Ruby/Rails or
      Javascript. If you want that functionality, you might have some luck using something like
      eclim or the ruby gem startag

      UPDATE 2: Actually, on further research tern_for_vim is likely what you've got to use
      muchacho!

      tern_for_vim {{{
        https://github.com/ternjs/tern_for_vim
        This may very well be one of the only working solutions to getting some type of goTo
        function definition ability for javascript files within vim

      }}}

      eclim {{{
        UPDATE: 20181023 Couldn't ever seem to get it up and running. So I'm going to forget about
        it for right now, half a day is enough trying to get a feature setup that I don't really
        NEED need at this point in time.

        Installation
          First off, eclim definitely requires Java and I think (??) the Eclipse IDE to be installed prior:

            $ sudo apt install openjdk-11-jre openjdk-11-jre-headless openjdk-11-jdk openjdk-11-jdk-headless
            $ sudo apt install eclipse

              UPDATE: This didn't seem to do the trick. Perhaps it needs to be installed through one
              of the installers at the website? https://www.eclipse.org/downloads/packages/ The
              problem was that when I went to install eclim, it asked me where the root directory
              was for Eclipse. I entered in /usr/share/eclipse but it reported back that it could
              not find the eclipse executable in that location. Further, whenever I tried to launch
              eclipse, it would crash on start and output and error log. I'm guessing because it was
              never really fully installed through the apt package...

          Get the latest .bin version of eclim from https://github.com/ervandew/eclim/releases/
          Download it where you like, command-line into that directory, then run:

            $ chmod +x <eclim bin filename>
            $ ./eclim_2.8.0.bin

      }}}

}}}

    Troubleshooting {{{
      Something interesting happened with my last reinstall of Lubuntu... Whenever I got vim setup
      and opened it for the first time, that's when I'd expect the curl script stuff to run from the
      top of .vimrc file in order to get plug.vim all ready to go. Except this time, it didn't
      work... Messing around with it a bit more and I noticed I was receiving a bunch of "You do not
      have permission" type messages in response to trying to create new directories and/or access
      certain directories. Which is weird considering I should have full admin rights...
      Nonetheless, here's a command set I found on the good ole web that seemed to bring the user
      account back into order. Just note that the user cautions that this is for a "personal"
      computer environment, if you ever are in a network type situation where more than one user may
      be using the computer, you'll likely need to do something else. Hopefully, though, I don't run
      into this issue going forward...

      NOTE: Whenever I tried to run these recently --long since this note was made-- the system
      seemed to hang... Even tried prefacing with sudo but they still seemed to make the terminal
      unresponsive

        $ chown -R <username> ~<username>
        $ chgrp -R $(id -gn <username>) ~<username>

    }}}

  }}}

  Notes  {{{
    Plugins and dotfiles stuff {{{
      www.vimawesome.com # great resources for plugins
      https://romainl.github.io/the-patient-vimmer/0.html
      https://github.com/mhinz/vim-galore
      https://dotfiles.github.io
      http://dotshare.it

      https://vimways.org/2018/from-vimrc-to-vim/
        excellent rundown on how to organize and structure your vimrc among separate files

    }}}

    Learning vim {{{
      $ vimtutor
        a bomb.com little training session for a lot of VIM's common stuff
      :h
        reference manual
      :h user-manual
        a more reader-friendly version of the :help manual
      :h quickref
      :h myvimrc

      https://vimhelp.org/?
        an online version of vim's :help file

      https://danielmiessler.com/study/vim/
      https://zzapper.co.uk/vimtips.html
      https://moolenaar.net/habits.html
      https://github.com/mhinz/vim-galore
      https://vim.zeef.com/patrick.schanen
      https://www.vi-improved.org/recommendations/
      https://learnvimscriptthehardway.stevelosh.com

    }}}

    Grab bag, a bunch of unsorted useful stuff. Move to separate categories as able {{{
      If you've made some changes to your vimrc file and want it to be incorporated into your
      currently open vim session, use

        :source ~/.vimrc

      <line_number>gg
        goes to that line number

      =<motion>
        autoindent along a given motion. So for example...
      =G
        autoindent to the end of file

      ==
        autoindent current line

      * search for word under cursor, forward
      # search for word under cursor, backward

      ^6
        go back to previously opened file. This is a HUGE lifesaver! I can't tell you how many times
        I accidentally open up a new file over my currently active window!

        NOTE: Sometimes you need to hold down shift as well

        :h alternate-file

      // vim: set ft=javascript :
        put this line toward the top of a file and vim will use the stated filetype for the purposes
        of syntax highlighting, formatting, etc. Known as a file modeline

      :pwd
        returns the present working directory for the current vim instance
      :lcd %:p:h
        changes the present working directory to the parent directory of the currently active file.
        This is useful if you're running multiple vim instances and you need to establish different
        working directories for each vim session

      If you ever try to write a file and vim complains with E212, saying you don't have the proper
      permissions, use this:

        :w !sudo tee %

      :!xdg-open %:p &
        opens current file in default browser
        UPDATE: Nope, this command doesn't seem to work, it fubars the vim session like no other!

      gx
        opens cursored weblink in browser
        NOTE: I typically use <leader>b , not sure if that's a built-in thing or something afforded
        by a plugin

      gO
        displays a sort of "table of contents" for the current buffer with all navigable tags
        displayed

      When opening files, if you notice any greyed out ^M's on screen where there should be spaces,
      chances are the file was created on a system that incorporated a different version of line
      breaks. To bring the formatting into unix-safe standards, use the following commands:

        :e ++ff=dos
        :set ff=unix
        :wq and reopen

      .
        repeat last command

      ^w<S-t>
        Opens currently selected pane in a new window. Hmm... The downside to this is that it closes
        the pane instance so whenever you close the new tab window in an effort to go back to it in
        the previous pane, it's not there, so you have to reopen it. I wonder if there's a way to
        keep the pane open?

      u
        undo
      ^r
        redo

      ^z
        temporarily background vim instance
        fg<cr>
          bring vim instance back to foreground
        fg %<num>
          in the event there are multiple background jobs listed and you want to target a specific
          job

        UPDATE: I've since added a bashrc alias that allows you to always use `vim`. If there's
        a vim instance in the background, it calls `fg`, otherwise it launches a new vim session.
        Very cool!

      :echo wordcount().words # get the wordcount in active buffer

      :h digraphs
        ^K # while in insert mode, followed by the "code" for whatever digraph you want

      %s/\s\+$//e
        Removes trailing white space throughout the buffer

      :%!xxd
      :%!xxd -r
        Enter and exit hex-editing mode
        :h hex-editing for an augroup command you can use to automatically go in/out of this mode
          depending on the filetype currently in the buffer

    }}}

    The "language" of doing things in vim {{{
      A generic way to start thinking about using Vim is its use of verb + modifier + noun.

      There's another interaction style that comes into play with other commands and motions and
      that is count + operator + text-object + motion

      Technically, the second "pattern" seems to also take care of the first, but it's all about
      building a familiarity with how to actively translate your thoughts and intents into proper
      "vim speak"

      verb + modifier + noun {{{
        Verbs
          see operators

        Modifiers
          i     # inside
          a     # "a", though some find it more helpful to think of as being "around"
          <num> # x number of times
          f     # forward search onto
          t     # forward search to before
          /     # find string onward
          ?     # find string backward

        Nouns
          see text-objects

      }}}

      operator + motion = action {{{
        :h operator
        :h motion
        :h motion.txt
        :h navigation
        :h left-right-motions
        :h up-down-motions
        :h word-motions
        :h object-motions
        :h object-select
        :h mark-motions
        :h jump-motions
        :h various-motions

        Or, to put a different way...
        [count] [operator] [text object / motion]  Here are some examples
          6                                 +      6+   : 6x go down to line start
                    gU           aW                gUaW : capitalize a word
          3          c                      e       3ce : 3x change to word end
          4                                 $        4$ : 4x go to end of line
                     d           ]m                 d]m : delete to start of next method
                                            %         % : jump to matching paren/bracket/etc
                    gq           ip                gqip : format inner paragraph
                     d      /<word>            d/<word> : delete to start of <word>

        This is the workflow of vim while in NORMAL mode. And it allows for some VERY powerful
        combinations. For example, to capitalize all the characters in a word, you can use `gUaw`.
        gU is the uppercase operator, and aw is the motion. To select the cursored sentence, `vis`.
        Cool huh? When an operator command is invoked twice in a row (like dd), it acts upon the
        current line

          W       # go to next human readable word
          B       # go to previous human readable word
          ^       # move to first non-whitespace character of line
          _       # also move to first non-whitespace character of line
          $       # move to last non-whitespace character of line
          ()      # move by sentence
          {}      # move by paragraph
          t<char> # jump forward to character
          T<char> # jump backward to character
          f<char> # jump forward onto character
          F<char> # jump backward onto character

        Search results can also be used as text objects! This is what is demonstrated with d/<word>

          ;
            repeats any of the above char searches
          ,
            goes 'back' one search, useful in case you 'overshoot' your desired location with too
            many ;'s

        i<object> # inside object
        a<object> # around object

        text-objects:
          :h text-objects

          b   # () block
          B   # {} block
          l   # single character
          w/W # word
          s/S # sentence
          p/P # paragraph
          '   # single quote
          "   # double quote
          `   # backtick
          ()  # parentheses
          []  # brackets
          {}  # braces
          t   # tags

        Which means this inside/around stuff can pull off some really neat things
          c "change" i "inside" <object> "whatever object you want"
            cis # change sentence, remain in insert mdoe
            ci" # change everything within double quotes, remain in insert mode
            cit # change everything inside tags, remain in insert mode
            ct_ # change everything up to next _, remain in insert mode. For this particular
              movemement pattern, there are some plugins out there that claim to make it a bit
              easier. Also, you'll find some advocate removing the underscore from vim's iskeyword
              stuff, but evidently that can cause problems with other plugins, including FZF and
              some word completion stuff. So for right now, given how infrequently I come across
              this issue, I'll stick with the built in t/f movements.

          And this structure can lead to some REALLY cool selection tricks like...
            vit = "visually select everything inside the tag I'm currently cursored in"
            vat = "visually select everything around the tag I'm currently cursored in", which kind
              of acts like vit + the tags

          And this same functionality exists with practically all of the commands! So above you were
          using change. What if, instead, you wanted to to visual mode selection? Great! Just use
          something like vi( and you'll select everything within parentheses. Want to copy and paste
          an entire paragraph? vipyjjp Visual mode character select, inside paragraph, yank content,
          down two lines, then paste. Or let's say you wanted to insert a colon in front of every
          line of your huge 6,000 line program? Let vim help you. Oi:jOvG. Move to the beginning of
          the current line, insert a :, then move down, move to the beginning of the line, then
          enter visual line mode, select everything to the end of the file, and . to repeat your
          last command (ie your insert command). Wicked!

            UPDATE: Okay, cool, so get this! This also works if you cut straight to the yank
            operator without even messing with the visual selection stuff. So you can do things like
            yip and you'll yank the entire paragraph under cursor.

        Inclusive vs Exclusive {{{
          :h inclusive
          :h exclusive

          Another interesting thing about motions is that some are inclusive (meaning they INCLUDE
          the last char) while others are exclusive (meaning they EXCLUDE the last char).

          For example:
            dw is INCLUSIVE
              deletes the current word and the following space, goes right up to the 'start' of the
              next word
            de is EXCLUSIVE
              deletes to the end of the current word and EXCLUDES the following space
            db is EXCLUSIVE
              deletes backward to the beginning of the cursored word but PRESERVES the cursored
              character you can use this line as a test bed to see how these various commands word

          The good news here is that an exclusive motion can be made to be inclusive be preceeding
          the motion with `v`. So, for example, if you want the db command to be inclusive, you can
          use it as `dvb` and it will INCLUDE the cursored letter

        }}}

      }}}

    }}}

    :help {{{
      This is actually a very robust and handy tool built into vim. Here are some tricks to make it
      even more powerful, and it involves tapping into vim's keycodes

      :h               # get a quick rundown of what I'm talking about
      :h help-summary  # deeper!
      :h help          # even deeper deeper!
      :h reference_toc # the motherload, anything and everything about vim
      K                # pull up help docs for cursored word

      :helpc
        close any open help windows, this prevents any sort of quick command thing from popping up

      NOTE: You can also use <TAB> completion while using search. For example `:h keyc<TAB>` will
      show a quicklist of results

      :h keycodes      # breakdown of all of vim's internal keycodes, so you can speak in vim's
                         language when searching
      :h <command>     # find help for this command in normal mode
      :h i_<command>   # find help for this command in insert mode
      :h v_<command>   # find help for this command in visual mode
      :h c_<command>   # find help for this command in command mode
      :h :<command>    # find help for this ex-command
      :h 'option'      # find help for this option
      :h function()    # find help for this function
      :helpgrep <word> # search through all help documents for anything and everything relating to
                         the supplied word/etc
      :cnext/:cn       # go to next match
      :cwindow/:cw     # show list of matches in quickfix window
      :lwindow/:lw     # show list of matches in location list

    }}}

    abbreviations {{{
      Similar to mappings, but meant to be used in insert, replace, and command modes
      :h abbreviations

      :iabbrev adn and

    }}}

    buffers {{{
      NOTE: You have some buffer related plugins made available in your .vimrc that may expand the
      functionality written about here
      :b <text><TAB>
        :b jou<TAB>
        will pull up the buffers that match a filename starting with jou. This can be a very
        efficient way to access buffers if you know what it's called, maybe even faster than the
        buffer explorer plugin will allow
      :bd
        closes the active buffer, useful for things like netrw.
      :buffers
        see a list of all buffers
        :<num>bd
          closes the buffer that matches that number
      :tab split
        This will open the current file in a new tab without closing out the buffer from the
        currently active tab you're working from. There is likely more to it than that, but for now,
        this has allowed me to essentially have the same buffer open in more than one tab. Without
        this, accessing a buffer from the buffer list will also take me to that buffer's related tab
        rather than opening it "again" in the current window

    }}}

    checking commands, mappings, and settings {{{
      :help index # This will give you a full rundown
      :help map-which-key # kind of a vim recommendation of sorts

      To check the value of a vim setting while in vim, use one of these:
        :verbose set <command>? # The added question mark is the kicker :)
        :verbose map <command>?

        :h set
        :h map

      If you're not sure what the proper keypress reference is, use this...
        :map ^q <then whatever keypress you're interested in>
          ^q allows vim to read-in your next keypress
            --OR--
          <command>, then press ^d
          ie `:Hex ^d`

      If you need to see what might be setting a command's value, try this!
        $ vim -V<num><filename>
          NOTE: There is NO SPACE, ie `\nvim -V10verbose.log`
          sets verbose level to <num> value upon startup, outputs results to file

        set verbose=<verbose_number_level>
        :help :verbose is your friend if you want some more info on how you can use it :)
        :help 'verbose'

        NOTE: Verbose comes in various flavors, ranging from 0-15. As you increase the value, vim
        logs more and more details for what's going on behind the scenes

          1  : When the viminfo file is read or written.
          2  : When a file is \": source"'ed.
          5  : Every searched tags file and include file.
          8  : Files for which a group of autocommands is executed.
          9  : Every executed autocommand.
          12 : Every executed function.
          13 : When an exception is thrown, caught, finished, or discarded.
          14 : Anything pending in a \": finally" clause.
          15 : Every executed Ex command (truncated at 200 characters).

      If you want to see ALL the mappings use one of these bad boys

        :verbose map
        :verbose cmap
        :verbose imap
        :verbose nmap
        :verbose vmap

    }}}

    colorschemes {{{
      http://colorswat.ch
      https://themer.dev
      https://vimcolors.org
      https://mswift42.github.io/themecreator/#
      https://github.com/romainl/vim-rnb
      https://github.com/jacoborus/estilo

      :h xterm-color
        :runtime syntax/colortest.vim
      :h xterm-true-color
      :h termguicolors

      https://github.com/dylanaraps/pywal
        $ sudo pip3 install pywal
          One that caught my eye is the built-in dkeg-harbing
          $ sudo pip3 install wpgtk
            a gui interface for pywal, requires the following
              $ sudo apt install python-gobject feh imagemagick
            run with `$ wgp`

      If you ever start getting a bunch of weird accent colors and underlines with a colorscheme,
      there's a chance you may have spellchecker enabled. Use `:set nospell` to turn it off and see
      if it clears up the issue.

      Making your own colorscheme {{{
        NOTE: This is admittedly rather sparse and unorganized at the moment, but I'll keep adding
        to it as I go along

        https://coolors.co
        (See other sites mentioned earlier too)

        :h syntax
        :h highlight-groups

        :h hitest.vim
          This shows you how to quickly display ALL colorscheme colors at once. Very cool!

        You also added a mapping to display the syntax info for a cursored word
          <leader>c

        https://medium.com/@gillicarmon/create-color-scheme-for-vim-335e842e29ea
          recommends saving yourself some typing by using the following function throughout

          function! Coloring(group,guibg,guifg,gui,ctermbg,ctermfg)
            let highlightstr =  'highlight ' . a:group . ' '
            let highlightstr .= 'guibg' . a:guibg . ' '
            let highlightstr .= 'guifg' . a:guifg . ' '
            let highlightstr .= 'gui' . a:gui . ' '
            let highlightstr .= 'ctermbg' . a:ctermbg . ' '
            let highlightstr .= 'ctermfg' . a:ctermfg . ' '
            let highlightstr .= 'cterm' . a:gui . ' '

            execute histring
          endfunction

          and then it says you use it like this

            call Coloring("Comment","NONE",#ff0000,"NONE","NONE","9")

          Though the reference to `call` seems off... If I'm using it within a file, could I not
          just use it like Coloring(blah blah blah) ?

      }}}

    }}}

    copying and pasting {{{
      UPDATE: If you're ever doing stuff involving remote terminals, you may want to check out
      something called 'OSC 52'. There's a vim plugin similarly named that allows you to seamless
      cut/copy from a remote terminal into your local terminal on the fly. It's like a syncronized
      clipboard that should be available in just about any linux-ish terminal you're using

        https://github.com/fcpg/vim-osc52

      What makes vim a little confusing when it comes to copying and pasting is that it has the
      ability to use different registers. Some are isolated and apply ONLY within the vim
      environment, while others are able to hook into the system's register. Further still, the user
      is able to define his own registers. Holy schamolly!

        My understanding thusfar is that "<key> is what sets the register. So for example... In
        visual mode "1y yanks the selected to the 1 register. To paste from that same register,
        you'd use "1p If you use letters, a capital letter tells vim to append your selection to the
        register, to add to what's already there. So "ay yanks to a and "Ay would yank additional
        stuff to a without overwriting it To paste while you're in insert mode, use CTRL+R
        <register>

      Now, where things get interesting is that supposedly if you use the + register, it's
      accessible systemwide in Linux So if you copy something in visual mode with "+y you can then
      paste it in another program with the usual CTRL+V And it works in reverse too. So if you've
      copied something from a browser, you can paste it into vim by accessing the register with "+p

      In practice, you'll find that if you copy something that you want to use later, then go and
      delete a line to replace with the copied text, the delete action essentially overwrote your
      earlier copy. This is because, by default, vim uses a <blank> unnamed register. So in this
      instance, if there's something you want to repeatedly be able to refer back to and paste from,
      you'd throw that bit into a named register and then paste from that same register when needed.
      Make sense?

      :reg # Lets you see a list of all actively defined registers

      :h register will give you the full enchilada

      If you're having trouble getting some of this to work, specifically being able to copy and
      paste with the system-wide register, make sure your vim has +xterm_clipboard enabled ($ vim
      --version). If not, install the following package:

        $ sudo apt install vim-gtk

      If you ever want to paste test into the vim command mode, yank the text you're wanting, go
      into command mode, then

        ^r"

    }}}

    cursor position {{{
      H  # jump to top of screen
      M  # jump to middle of screen
      L  # jump to bottom of screen

      zz # scroll window so cursor's current position is at the middle of the screen
      zt # scroll window so cursor's current position is at the top of the screen
      zb # scroll window so cursor's current position is at the bottom of the screen

    }}}

    find and replace {{{
      /<string> # search for a regex string
      If you're wanting to find something with more than one word, use a . in place of the space

      :s/<find>/<replace>  # find and replace on current line only
      :%s/<find>/<replace> # find and replace on all lines
        Additional options that can be added at the end by way of /<chars>
          g # replace each occurence on every line, not just the first occurence on every line
          c # ask for confirmation
          i # case insensitive, this is the default setting
          I # case sensitive
            NOTE: You have it set up in your .vimrc that lowercase strings are case insensitive
            whereas uppercase strings are case sensitive

      The above also works after a visual selection, as well. Just visually select the lines you
      want to search through, followed by the above commands. You will likely see something like
      this at the beginning of the command

        `:'<,'>`

      that just means it's limited the visual selection.
        NOTE: The visual selection will prevent any of the 'highlight' stuff you have going on. Just
        keep typing though and you'll see the words replace as you go

      https://vonheikemen.github.io/devlog/tools/vim-and-the-quickfix-list/
      If you want to do a find and replace across all files within your present working directory,
      you can do this by interacting with the search results that are shown within the quickfix list
      after, say, a <Ctrl-s> search

        <Ctrl-s> <search_term>
        Then, while still in the quickfix window of search results, type the following
          :cdo %s/<search_term>/<replace_term>/gc
        This will then walk you through a confirmation prompt
        Once it's done it's thang, you can either `:cdo update` or `:wa` to save the edits

    }}}

    folds {{{
      NOTE: There are times where you find your fold markers not working at all, like they've been
      corrupted somehow. If that's the case, open up vim and run this command

        :verbose set foldmarker?

      In the past, this has shown the default fold marker was overwritten somehow in the Session.vim
      file.

      -----

      Okay, so here's something interesting about vim folds... This whole time, I've been using
      foldmethod=marker, which adds characters into the current file that vim then knows to fold.
      Which is nice, because this means that from session to session, vim can 'remember' where you
      placed the folds. BUT! And I think this is a big but... Vim only recognizes them if the vim
      session likewise has its fold settings set to marker.

      Now get this, this is where it gets interesting... There's another way to have vim "remember"
      fold locations without polluting your files with fold marker characters! If you use any of the
      other foldmethods, vim can "save" the folds it created---or that you created, if you have it
      set to manual---through a command called :mkview. Then, after the file is loaded, if you run
      :loadview, it "rebuilds" everything as it was before. Cool, eh!? The drawback to this
      approach, however, is that the folds only persist through the related :mkview file. Which
      means if that gets lost or misplaced, say bye-bye to your foldings and stuff.

      :h folds # there are a few more I don't mention here
      :h fold-commands
      :h fold-method
      :h fold-expr

      zj/k # move to next/previous fold
      [z   # move to start of open fold
      ]z   # move to end of open fold

      zo   # open one fold under cursor
      zO   # open all folds under cursor recursively
      zc   # close one fold under cursor
      zC   # close all folds under cursor recursively
      za   # toggle fold under cursor
      zA   # toggle folds under cursor recursively
      zm   # fold more
      zM   # close all folds
      zr   # reduce folding
      zR   # open all folds
      zv   # expand folds to reveal cursor

      NOTE: Use these with caution, as they are not "undo-able"
      zd   # delete current fold
      zE   # delete all folds

    }}}

    :g {{{
      :help :g

      https://vim.fandom.com/wiki/Power_of_g

    }}}

    gq<motion> {{{
      Interesting thing to reformat text and stuff. See :h gq for info and :h textwidth
      gqG # Format all lines below cursor position
      gq} # Format current paragraph
      gqq # Format current line

      There's also some interesting stuff you can do to ensure that vim moves entire words, rather
      than split them at the letters.

        :set formatoptions=1
        :set lbr # assume that stands for linebreak
        :set autoindent
        :set smartindent
        :set breakindent # :help breakindent, :help breakindentopt

    }}}

    grepping {{{
      https://codeyarns.com/2017/09/15/how-to-grep-in-vim

      :vim /<regex>/ %
        The key here is that you also need to specify the file you want to grep. In this example,
        that's what `%` accomplished

      On this topic, this is where other tools like ack come into play. Because while vimgrep (vim's
      built-in grep engine) is good enough for smaller file sets, if you're ever working with huge
      files, that is when the speed of vimgrep may start to show it's rough spots. And that's where
      pluggin into something like ack comes into play as it performs much faster

    }}}

    jumps {{{
      :h jump-motions
      :ju # see list of jumps available for the current file, type corresponding number followed by
        ^O # move back in jumplist
        ^I # move forward in jumplist

      You can also use these two commands while in normal mode to iterate through the jumplist one
      by one, or with a preceeding number as just described.

      %   # jump to matching paren, bracket, etc etc, even works for keywords like do/end
      ()  # sentence
      {}  # paragraph
      HML # top/middle/bottom of screen
      gf  # to file name under cursor

      g; # cursor previous edit position
      g, # cursor next edit position

    }}}

    locationlist window {{{
      :lcl[ose] # close location list
      :lop[en] # open location list

    }}}

    macros {{{
      q<letter><commands>q # record named macro
      <number>@<letter> # execute named macro X number of times
      q<capital letter of existing macro>q # append to existing macro of same lowercase letter
      :let @<letter>=' # open the given register letter for editing
        NOTE: From what I've read, this one may be a little tricky to pull off...

    }}}

    marks in buffers {{{
      ` # enter mark at same cursor position
      ' # enter mark at beginning of the line
      ]' [' # move between lowercase marks
      ]` [` # move between uppercase marks
      for more info, see :h E20

      :delm a-z # delete all lower-case marks in current buffer
        NOTE: You used to use `:delm!` but this deletes ALL marks, including the change history
        marks!

      Vim also creates it's own special marks in the background
        for a full listing, see :h '[

        `. # jump to position of last change in current buffer
        `" # jump to position where last exited current buffer
        `0 # jump to position in last file edited
        `' # jump back to line where jumped from
        `` # jump back to position where jumped from
        `[ `] # jump to beginning/end of previously changed or yanked text
        `< `> # jump to beginning/end of last visual selection

    }}}

    math {{{
      n^A / n^X
        Whenever the cursor is on a number, you can use this to add/subtract n to the number. Note
        that if you're using <^a> as your tmux prefix, pressing <^a> twice will instead send the
        command through to vim. You can also use this command if you're not currently on a number.
        What it will do instead is jump to the next number and apply the count there.

        And evidently it gets even cooler! It turns out you don't even need to have the number
        cursored. If there's a number as part of a line, ^A will search forward for the first number
        and apply the math there. Neato!

    }}}

    message output {{{
      :messages
        displays messages output by vim, usually displayed in the lower fly-out.

        If you ever want to output this to a register, use
          :redir @<register>
          :messages
          :redir END

        See `:h redir` for more info

      https://stackoverflow.com/questions/3025615/is-there-a-vim-runtime-log
      https://superuser.com/questions/201090/write-vim-log-to-a-file

    }}}

    modes (includes omnicompletion) {{{
      i  # insert mode
      v  # visual mode character
      V  # visual mode line
      ^V # visual block mode. This is meant to serve as an up/down columnar type editing tool
        <S-i> # Enter into Insert mode after making a visual block mode selection
      gv # reselect last visual selection
      r  # replace mode, single character
      gr # virtual replace mode, single character
      R  # replace mode, toggle
      gR # virtual replace mode, toggle. Replaces actual screenspace rather than character space,
           which makes it much more consistent with what you'd expect to happen, especially where
           <tabs> are involved

      While in NORMAL mode {{{
        I
          move to beginning of line
        A
          move to the end of the line

      }}}

      While in INSERT mode {{{
        ^d / ^t
          decrease/increase indent
        ^h
          delete back one character, similar to <backspace>
        ^j
          new line
        ^k <code>
          insert digraph
        ^o
          go into normal mode to allow for a single command, then immediately back into insert mode.
          One particular way this can be useful is to scroll the window so that the cursor position
          is in the middle of the screen with zz, then go right back to typing. Very cool!
        ^r<register>
          paste yanked text from <register> at current cursor position. Typically used as <^r>0 for
          the default register (I think? Will go into more detail here later on in Neil's text and
          update)
        ^u
          delete back to start of line
        ^w
          delete back one word

        Code completion
          NOTE: Use of this can be combined with * to get a sort of wild/fuzzy search capability (ie
          *.rb for all ruby files)

          ^x
            :h ins-completion, <^p> and <^n> are used while in this mode to go backward/forward
            through results

            Here are some of the things you can do with it:
              ^x ...
                ^f filename
                ^i keywords in current and included files
                ^l line
                ^o omnicompletion
                ^s spelling suggestions
          ^p/^n
            this is a variation of vim's built-in ins-completion stuff, cuts right to a word
            completion list without first having to press the usual ^X prefix. The difference
            between n and p are the order in which the matches are displayed

          ^y
            "choose" currently selected item from the popupmenu
          ^e
            close the popupmenu without making any selection

      }}}

      While in VISUAL mode {{{
        c
          change highlighted text
        o
          go to other end of highlighted text. Motion commands can be used on either end to adjust
          the selection range
        U/u
          upcase/downcase selection
        i/a
          form the first part of a text-object call

        Notes on Visual mode
          In Normal mode, the general workflow is to supply an operator command then a motion
          command in order to do something. However, in Visual mode, this is turned on its head. By
          selecting text, the user is essentially setting the motion, or the extent, and THEN
          supplying the operator command

          When a visual mode command is repeated, it affects the same range of text that was
          previously selected. This means if you've highlighted the word 'two' and the next word you
          repeat the command on is the word 'three', only the first three letters (thr) will be
          affected. For this reason, any word-wise type commands are better ran in NORMAL mode, that
          way they can better adapt when repeated.

      }}}

      While in VISUAL BLOCK mode {{{
        When using ^v to edit a block of stacked text, the additions to the lower lines will not
        be diplayed until the user escapes out of his action

        If you're wanting to do a replace while in visual block mode, either `r` or `c` should do
        the trick

        ^v can also be used to adjust the "ends" of lines of differents lengths using the $ (end
        of line) command after the initial vertical selection has been made. The takeaway here is
        that while it's called a visual "block" mode, it doesn't have to be confined to a rigid
        square section, etc

      }}}

      While in COMMAND mode (ie `:`) {{{
        !<command>
          preceeding a command with a bang will pass that command, not through vim, but through the
          terminal. During this time, the current instance of vim will be moved to the background.

          So let's say you'd like to create a new directory from within vim without going through
          a :netrw

            :!mkdir "<directory>"

        ^f
          opens an editable list of your command history

        ^r<register>
          pastes content from referenced register
          example: ^r" to paste what's in the " register

      }}}

    }}}

    plugins, how to write'em {{{
      :h plugin
      :h write-plugin
      :h plugin-details

      https://vim.fandom.com/wiki/How_to_write_a_plugin
      https://vimways.org/2019/writing-vim-plugin/
      https://blog.semanticart.com/2017/01/05/lets-write-a-basic-vim-plugin/
      https://learnvimscriptthehardway.stevelosh.com/
        https://stevelosh.com/blog/2011/09/writing-vim-plugins/

    }}}

    quickfix window {{{
      :ccl[ose]
      :copen

    }}}

    regex {{{
      Regex in vim in a special duck, it is its own set of rules, unlike anything else. Which is
      annoying, but it's also one of the most powerful regex tools out there.

      It contains something called magic and very magic modes, which allows it to act more like
      traditional regex's without the need to escape so many characters and symbols. In practice,
      magic mode is virtually worthless, meaning very magic mode should be used when you want that
      functionality. It is activated with \v

      Look ahead / look behind This is a pretty cool thing where you can match a pattern, but only
      if it comes after and before whatever you set. So for example... Say you're trying to find all
      instances of String, but only whenever it's within a set of brackets like this: [String]

          This is where you can use look ahead/behind with your regex.
            /\v\[zsString\ze\]
          zs is start, ze is end. So you're saying to find instances of String that
          start with [ and end with ]

    }}}

    registers {{{
      :h registers

      "     # filled up with d, c, s, x, and y commands
      0     # filled up with most recent yank command
      <1-9> # filled up with most recent "big delete" of more than one line, starting with 0 then front-filling
      -     # filled with the most recent "small delete" of less than one line

    }}}

    resolving merge conflicts {{{
      UPDATE: I've since started using an alternate viewmode diffconflicts which defaults to present
      things as a two-way. See section in git notes above for the rundown on this view. Otherwise,
      back to your regularly scheduled program...

      First off, you need to open the file git reports has a merge conflict
        $ vim <filename>
      Then activate the fugitive vim diff environment
        :Gdiff
      This opens three windows side by side:
        Left = Target, Middle= Working, Right: Merge
      What you do now is go through each of the conflicts and tell it which version you want to keep
      in the center working copy:

        ]c
          next conflict
        [c
          previous conflict
        :diffget <buffer> | diffupdate
          You identify the buffer either by its number or by using some unique string in the
          buffer's name. To make things easy, the left window can always be identified with //2 and
          the right with //3. So if you'd like to pull the code from the left window into the
          working branch, with your cursor in the middle window, you'd type :diffget //2 and boom!
          The "| diffupdate" part causes the highlights to be appropriately updated

      If you'd rather just cut to the chase and accept one version in it's entirety, then cursor
      that file and type

        :Gwrite
          This will throw up a confirmation warning to make sure you want to do this.
        :Gwrite!
          If you've previously cherry-picked some changes, use bang to push it through

      Whenever you're done, cursor the middle window and type
        :only
          This will remove the two side windows
      write that badboy and you're ready to go!

    }}}

    tabs {{{
      :h tab-page-commands

      :tabnew       # create new tab
      :tabc[lose]   # close current tab
      :tabs         # view all open tabs and their contents
      <number>gt    # go to tab of that number
      [t ]t         # go to previous/next tab
      :tabm[ove]<n> # move current tab the n'th zero indexed position

    }}}

    tags aka links {{{
      ^]
        follow a link, like with what's found in :help. It also will jump to a method definition in
        your code if you have c-tags enabled
      ^t
        go back where you were previously

      Syntax
        |word-in-normal-usage|
        *definition-of-that-word*
        :helptags ./

      To get started, check out:
        :h help-writing
        :h tags
        https://superuser.com/a/1321979

        The Vimwiki plugin also offers this functionality

      c-scope {{{
        Similar to ctags but a bit more powerful, already packaged with vim

        :h cscope
        :cs # shows various cscope commands

      }}}

      ctags {{{
        NOTE: Just a heads-up. Evidently ctags is a third-party aftermarket kind of thing to
        manage tags in vim. But vim also has its own built-in tag manager that is evidently very
        capable and sometimes even more robust and feature-rich than any of these other "add-ons"

        https://github.com/universal-ctags/ctags
        https://thoughtbot.com/upcase/videos/intelligent-navigation-with-ctags

          $ sudo apt install universal-ctags

        Confirm everything intsalled poroperly with

          $ which -a ctags
          $ ctags --version

        To setup the tags file for a project, navigate to its root directory then type

          $ ctags -R .

        NOTE: This could take some time depending on how many files are associated with the
        directory Once this is done, you can confirm the tags file was created with the following

          $ ls -l | grep tags

        Allows you to jumpto definitions in your code with ^] run these bad boyz:

          $ ctags

        Navigate to the root project directory then...

          $ ctags -R --exclude=node_modules

        This creates a new file called `tags` which will house what is essentially a manifest of
        all the definitions and stuff that ctags will reference

        Troubleshooting {{{
        If you receive an error message saying there is a format error within the tags file, here
        are a few ways you might resolve it:

          Open the tags file and gg to the byte number that was listed in the error. Most of the
          time, simply deleting out the line above (at?) the one you were taken to will solve the
          issue

          /^[^\t]{50,}
            this looks for function names longer than 50 chars, evidently that can cause
            issues. Removing the offending lines or renaming the offending functions to be shorter
            could do the trick here

          If there are any extra lines before !_TAG_FILE_FORMAT, delete'em and resave the tags file

        }}}

      }}}

    }}}

    viewing differences between open windows {{{
      :windo difft
        If you have a several versions of the same file opened up, this command will highlight the
        differences

    }}}

    viminfo  {{{
      This file keeps record of pretty much everything you do in vim, allowing you to "jump through"
      this rolling history of jumps and edits and marks.

      By default, this file is saved when vim closes. If you want to save the file without leaving
      your vim session, use

        :wv

      Customize what gets saved to the viminfo file with something like
        set viminfo='0,:0,<0,@0,f0
          '0 marks will not be saved
          :0 command-line history will not be saved
          <0 registers will not be saved
          @0 input-line history will not be saved
          f0 marks will not be saved
          no % buffer list will not be saved
          no / search history will be saved

    }}}

    window resizing, closing, etc {{{
      ^w N- # decrease height
      ^w N+ # increase height
        NOTE: <^w>99+ # restores view area to full height
      ^w N< # decrease width
      ^w N> # increase width
      ^w_   # expand window to full height
      ^w|   # expand window to full width

      ^w 0  # toggle display of ONLY current window, hide all others
      ^w o  # close all other windows

    }}}

  }}}

    Troubleshooting {{{
      slow cursor movement? {{{
        UPDATE: Okay, so oddly enough, I fixed it. Or perhaps it fixed itself? Either way, here's
        what I did right before it went back to normal:

          Comment out all active Plugins
          :PlugClean

          Reinstate all of the previously active Plugins

          :PlugInstall

          Now this didn't do anything... So I then turned my attention to the various miscellaneous
          settings I had in my .vimrc file

          Comment out all of the miscellaneous commands

          Noticed that, oh wow, the cursor speed is back up to snuff! So then I went in and renabled
          each of the miscellaneous settings one by one to isolate what was causing the slowdown.
          Turns out? Well, nothing! I reneabled all of the settings and it never slowed down again.
          So who knows? For what it's worth, none of the above seemed to fix anything

        Still looking for answers on this one. In the meantime here are some things that I've come
        across:

          .vimrc settings that might help:
            set cul!
              this disables character highlighting on the current line

            let loaded_matchparen = 1
              disables parenthases highlighting, which may improve responsiveness

            some say that commenting out 'set cursorline' can improve responsiveness

            set re=1
              if you're getting desperate, this will force Vim to use an older verison of its regex
              engine

          benchmark Vim's start-times like'a this'a:
            $ vim --startuptime timeCost.txt timeCost.txt
            :h --startuptime # This should output some information for you to view
            :h startup

      }}}

      vim feeling sluggish? {{{
        https://codeinthehole.com/tips/debugging-vim-by-example/
        https://sanctum.geek.nz/arabesque/debugging-vim-setup/
        https://vimways.org/2018/debugging-your-vim-config/

        :scr[iptnames] # shows everything loaded into vim during this session

        :h option-list

        $ vim --clean # minimal vim startup, loads only defaults.vim
        $ vim --noplugin # loads vim with .vimrc, but without plugins enabled

        Start vim barebones, then add in plugins one by one until you start experiencing
        sluggishness again

        :h --noplugin
          argument	load vimrc files	load plugins
          (nothing)		    yes		          yes
          -u NONE			    no		          no
          -u NORC			    no		          yes
          --noplugin		  yes		          no

        $ vim -u NONE
        :so plugin/<plugin_name>.vim

        Launch vim, then try this:
          :profile start ~/profile.log | :profile func * | :profile file *
          <Do the things that make vim feel sluggish>
          :profile pause
          :noautocmd qall!

          This creates the named file (profile.log), outlining everything that was going on behind
          the scenes while you were using vim. Very helpful!

          :h profile

        Another thing to try is to disable syntax coloring with `:syntax off`. Does vim immediately
        become more responsive? If so, here's how you can profile your syntax file.

          :syntime on
          <scroll through the file for a bit>
          :syntime report

        :verbose au[tocmd]
          This will show you all of the autocommands that are going on behind the scenes. Looking
          through this list may give an indication for what could be causing some of the
          bottlenecks.

          This can be focused to certain actions too. For example, since you experience a slowdown
          when you change lines, try something like this

            :verbose au CursorMoved

        Okay, so get a load of this! After having vim become VERY unresponsive while typing (to the
        point where I couldn't even see what I was typing until I paused and waited for vim to catch
        up). Doing some more researching and it lead me to a few posts talking about how ruby files
        can be problematic for vim's syntax related performance. The configuration that seems to fix
        many of the issues I was having is to tell vim to use an older syntax engine and to set
        a the path for ruby's location

          set re=1
          let g:ruby_path = ''

        Another thing I discovered is that, when running :syntime, a command related to matching
        paren pairs came up quite often. This seems to be related to a plugin that's built-in to
        vim's defaults. Added some stuff to .vimrc to disable it and that has helped quite a bit!

      }}}

      folds seem corrupted? {{{
        :verbose set foldmarker
          see what this returns in vim If it looks out of whack, that means it was modified
          somewhere. Check yer Session.vim file, search if anything fold related turns up there

      }}}

    }}}

    gVim notes for windows {{{
      Install through exe downloadable, at default locations

      C:\Users\<name>\_vimrc

      if has('gui_running')
        set t_Co=8
        set t_md=
        set guifont=Lucida_Console:h10
        set guioptions-=m
        set guioptions-=T
        set guioptions-=r
      endif

    }}}

}}}

Windows related {{{
  NOTE: New section, may be a bit disorganized at first as I decide how I want to use it
  chocolately
    https://docs.chocolatey.org/en-us/choco/commands/

    > choco search
    > choco outdated
    > choco upgrade <program>

  scoop

  winget

  winget-cli

  screenkey {{{
    Carnac
      > choco install carnac

  }}}

}}}

==== On dollars and cents     {{{
  Setting up the field as float can actually lead to some upredictable rounding issues. For this
  reason, any time you're dealing with dolla billz y'all, it's best to set the fields up as integers
  that you then convert into dollars and cents for how it's presented in view.

  When it comes to Postgres, the use of numeric is touted every now and then. But some are quick to
  point out that even with using numeric set to a 2 unit precision, it may still introduce many of
  the same hang-ups that come with float.

}}}

==== Remember the memoization, it'll save your db from unnecessary hits, Hartl  {{{
  p 354
  Memoization is an optimization technique in which a program stores the result of a calculation to
  avoid repeating the same calculation multiple times

}}}

==== UML Schtuff  {{{
	--------|>       IS-A (ie inheritance or implements)
	-------->        HAS-A (ie composition)

}}}

==== Puma 'address already in use'  {{{
  $ lsof -wni tcp:3000
    # Added as alias in .bashrc, `$ servers`
  $ kill -9 <PID_number>

}}}
